{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Azure Synapse Analytics Documentation","text":"<p>Welcome to the comprehensive documentation for Azure Synapse Analytics, focusing on Spark Delta Lakehouse and Serverless SQL features.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Azure Synapse Analytics is an enterprise analytics service that accelerates time to insight across data warehouses and big data systems. This documentation provides guidance, best practices, and technical references for implementing and optimizing Azure Synapse Analytics solutions.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Delta Lakehouse Architecture: Learn about implementing the Delta Lakehouse pattern with Azure Synapse Analytics</li> <li>Serverless SQL: Understand how to leverage serverless SQL pools for cost-effective data exploration</li> <li>Shared Metadata: Explore the capabilities of sharing metadata between different services in Synapse</li> </ul>"},{"location":"#documentation-sections","title":"Documentation Sections","text":"Section Description Architecture Detailed architectural patterns and implementation guides Best Practices Recommendations for performance, security, cost optimization, and governance Code Examples Sample code and implementation templates Reference API references and technical specifications FAQ Frequently asked questions Troubleshooting Common issues and solutions"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Delta Lakehouse Overview</li> <li>Serverless SQL Guide</li> <li>Shared Metadata Documentation</li> <li>Performance Best Practices</li> <li>Security Guidelines</li> </ul>"},{"location":"#latest-updates","title":"Latest Updates","text":"<ul> <li>Added comprehensive FAQ section</li> <li>Improved navigation with breadcrumbs and related content</li> <li>Added versioned documentation support</li> <li>Enhanced search functionality</li> </ul>"},{"location":"faq/","title":"Frequently Asked Questions","text":""},{"location":"faq/#general-questions","title":"General Questions","text":""},{"location":"faq/#what-is-azure-synapse-analytics","title":"What is Azure Synapse Analytics?","text":"<p>Azure Synapse Analytics is an integrated analytics service that brings together data integration, enterprise data warehousing, and big data analytics. It gives you the freedom to query data on your terms, using either serverless or dedicated resources at scale.</p>"},{"location":"faq/#how-does-synapse-analytics-differ-from-azure-sql-data-warehouse","title":"How does Synapse Analytics differ from Azure SQL Data Warehouse?","text":"<p>Azure Synapse Analytics evolved from Azure SQL Data Warehouse, offering all its capabilities plus additional features including integrated Apache Spark, serverless SQL pools, data integration pipelines, and unified management.</p>"},{"location":"faq/#what-are-the-main-components-of-azure-synapse-analytics","title":"What are the main components of Azure Synapse Analytics?","text":"<ul> <li>SQL pools (dedicated and serverless)</li> <li>Apache Spark pools</li> <li>Data integration pipelines</li> <li>Studio (web-based interface)</li> <li>Synapse Link for near real-time analytics</li> </ul>"},{"location":"faq/#delta-lakehouse-questions","title":"Delta Lakehouse Questions","text":""},{"location":"faq/#what-is-a-delta-lakehouse","title":"What is a Delta Lakehouse?","text":"<p>A Delta Lakehouse combines the best features of data lakes and data warehouses, using Delta Lake format to provide ACID transactions, schema enforcement, and time travel capabilities on top of your data lake storage.</p>"},{"location":"faq/#what-are-the-advantages-of-using-delta-lake-format","title":"What are the advantages of using Delta Lake format?","text":"<ul> <li>ACID transactions</li> <li>Schema enforcement and evolution</li> <li>Time travel (data versioning)</li> <li>Support for batch and streaming data</li> <li>Improved query performance with optimized file layout and indexing</li> </ul>"},{"location":"faq/#how-do-i-optimize-performance-with-delta-lake-in-synapse","title":"How do I optimize performance with Delta Lake in Synapse?","text":"<ul> <li>Use Z-ordering for frequently queried columns</li> <li>Implement regular OPTIMIZE commands to compact small files</li> <li>Configure auto-optimize settings for tables with frequent updates</li> <li>Use partitioning for large tables that are queried by partition columns</li> </ul>"},{"location":"faq/#serverless-sql-questions","title":"Serverless SQL Questions","text":""},{"location":"faq/#what-is-a-serverless-sql-pool","title":"What is a Serverless SQL pool?","text":"<p>A Serverless SQL pool is an on-demand, scalable compute service that enables you to run SQL queries on data stored in your data lake without the need to provision or manage infrastructure.</p>"},{"location":"faq/#what-are-the-cost-benefits-of-serverless-sql","title":"What are the cost benefits of Serverless SQL?","text":"<p>Serverless SQL pools use a pay-per-query model where you're charged based on the data processed rather than provisioned compute resources, making it cost-effective for intermittent or unpredictable workloads.</p>"},{"location":"faq/#what-file-formats-are-supported-by-serverless-sql","title":"What file formats are supported by Serverless SQL?","text":"<ul> <li>Parquet</li> <li>CSV</li> <li>JSON</li> <li>Delta Lake (using OPENROWSET with Parquet)</li> </ul>"},{"location":"faq/#shared-metadata-questions","title":"Shared Metadata Questions","text":""},{"location":"faq/#how-does-shared-metadata-work-between-spark-and-sql-in-synapse","title":"How does shared metadata work between Spark and SQL in Synapse?","text":"<p>Azure Synapse Analytics uses a shared metadata model where tables created in Spark can be directly accessed from SQL pools without moving or copying data, using a common metadata store.</p>"},{"location":"faq/#what-are-the-limitations-of-shared-metadata","title":"What are the limitations of shared metadata?","text":"<ul> <li>Some data types may have compatibility issues between Spark and SQL</li> <li>Three-part naming has specific limitations in both environments</li> <li>Some advanced Spark DataFrame operations may not translate directly to SQL</li> </ul>"},{"location":"faq/#can-i-create-views-that-work-across-both-spark-and-sql","title":"Can I create views that work across both Spark and SQL?","text":"<p>Yes, you can create views that can be accessed from both environments, but you need to ensure compatible data types and follow naming conventions that work in both contexts.</p>"},{"location":"troubleshooting/","title":"Troubleshooting Guide","text":"<p>This guide covers common issues and solutions when working with Azure Synapse Analytics.</p>"},{"location":"troubleshooting/#connection-issues","title":"Connection Issues","text":""},{"location":"troubleshooting/#cannot-connect-to-synapse-workspace","title":"Cannot connect to Synapse workspace","text":"<p>Symptoms: Unable to access Synapse Studio or connect to the workspace.</p> <p>Possible Causes and Solutions:</p> <ol> <li>Network Connectivity:</li> <li>Ensure your network allows connections to the Azure Synapse service endpoints</li> <li>Check if firewall rules are properly configured in the Azure portal</li> <li> <p>Verify you're using the correct workspace URL</p> </li> <li> <p>Authentication Issues:</p> </li> <li>Confirm you have the appropriate permissions to access the workspace</li> <li>Check if your Azure Active Directory credentials are valid</li> <li> <p>Try signing out and signing back in to refresh authentication tokens</p> </li> <li> <p>Service Outage:</p> </li> <li>Check the Azure Status page for any ongoing service issues</li> </ol>"},{"location":"troubleshooting/#performance-problems","title":"Performance Problems","text":""},{"location":"troubleshooting/#slow-query-performance-in-dedicated-sql-pools","title":"Slow Query Performance in Dedicated SQL Pools","text":"<p>Symptoms: Queries take longer than expected to complete.</p> <p>Possible Causes and Solutions:</p> <ol> <li>Suboptimal Distribution:</li> <li>Verify table distribution keys are appropriate for common query patterns</li> <li>Check for data skew using <code>DBCC PDW_SHOWSPACEUSED</code></li> <li> <p>Consider redistributing tables with high skew</p> </li> <li> <p>Statistics Issues:</p> </li> <li>Ensure statistics are up-to-date using <code>UPDATE STATISTICS</code></li> <li> <p>Check for missing statistics in execution plans</p> </li> <li> <p>Resource Constraints:</p> </li> <li>Monitor resource utilization during query execution</li> <li>Consider scaling up the SQL pool during peak workloads</li> <li>Implement query concurrency management</li> </ol>"},{"location":"troubleshooting/#spark-jobs-failing-or-running-slowly","title":"Spark Jobs Failing or Running Slowly","text":"<p>Symptoms: Spark notebooks or jobs time out, fail, or perform poorly.</p> <p>Possible Causes and Solutions:</p> <ol> <li>Resource Allocation:</li> <li>Check if the Spark pool has sufficient memory and cores</li> <li>Monitor executor memory usage and adjust configurations</li> <li> <p>Consider increasing the executor count for large datasets</p> </li> <li> <p>Data Skew:</p> </li> <li>Inspect the Spark UI for task skew</li> <li>Use appropriate partitioning strategies to balance data</li> <li> <p>Implement salting for join operations on skewed keys</p> </li> <li> <p>Code Optimization:</p> </li> <li>Review and optimize transformations (prefer narrow over wide)</li> <li>Tune caching strategies for frequently accessed DataFrames</li> <li>Use appropriate file formats (Parquet, Delta) and compression</li> </ol>"},{"location":"troubleshooting/#delta-lake-issues","title":"Delta Lake Issues","text":""},{"location":"troubleshooting/#delta-lake-transaction-conflicts","title":"Delta Lake Transaction Conflicts","text":"<p>Symptoms: Transaction conflicts when multiple writers update the same Delta table.</p> <p>Possible Causes and Solutions:</p> <ol> <li>Concurrent Writers:</li> <li>Implement optimistic concurrency control with retry logic</li> <li>Consider using Delta Lake's optimistic concurrency control features</li> <li> <p>Schedule jobs to avoid concurrent writes to the same table</p> </li> <li> <p>Long-Running Transactions:</p> </li> <li>Break down large operations into smaller batches</li> <li>Avoid holding open transactions for extended periods</li> </ol>"},{"location":"troubleshooting/#missing-delta-table-history","title":"Missing Delta Table History","text":"<p>Symptoms: Unable to access previous versions of Delta tables.</p> <p>Possible Causes and Solutions:</p> <ol> <li>Vacuum Operations:</li> <li>Check if aggressive VACUUM commands have removed needed history</li> <li>Adjust retention period in VACUUM commands</li> <li>Use time travel only within the configured retention window</li> </ol>"},{"location":"troubleshooting/#serverless-sql-pool-issues","title":"Serverless SQL Pool Issues","text":""},{"location":"troubleshooting/#cannot-query-delta-tables-directly","title":"Cannot Query Delta Tables Directly","text":"<p>Symptoms: Error when trying to query Delta tables directly from Serverless SQL.</p> <p>Solution: Use the OPENROWSET function with the Parquet format, pointing to the Delta table's location, and include the latest file version:</p> <pre><code>SELECT TOP 100 *\nFROM OPENROWSET(\n    BULK 'https://youraccount.dfs.core.windows.net/container/table/_delta_log/*.json',\n    FORMAT = 'CSV',\n    FIELDTERMINATOR = '|',\n    FIELDQUOTE = '',\n    ROWTERMINATOR = '0x0b'\n) WITH (json_content VARCHAR(8000)) AS [rows]\nCROSS APPLY OPENJSON(json_content)\nWITH (\n    add BIT '$.add',\n    path VARCHAR(400) '$.path'\n)\nWHERE add = 1;\n</code></pre>"},{"location":"troubleshooting/#metadata-and-catalog-issues","title":"Metadata and Catalog Issues","text":""},{"location":"troubleshooting/#tables-created-in-spark-not-visible-in-sql","title":"Tables Created in Spark Not Visible in SQL","text":"<p>Symptoms: Tables created in Spark notebooks don't appear in Serverless SQL.</p> <p>Possible Causes and Solutions:</p> <ol> <li>Metadata Sync Issues:</li> <li>Ensure tables are created in the proper database/schema</li> <li>Verify the table is registered in the metastore</li> <li> <p>Check for name conflicts or case sensitivity issues</p> </li> <li> <p>Permissions Problems:</p> </li> <li>Confirm the SQL user has proper permissions to the storage location</li> <li>Verify storage access using SAS token or managed identity</li> </ol>"},{"location":"troubleshooting/#pipeline-and-integration-issues","title":"Pipeline and Integration Issues","text":""},{"location":"troubleshooting/#pipeline-failures-with-integration-datasets","title":"Pipeline Failures with Integration Datasets","text":"<p>Symptoms: Integration pipelines fail when connecting to external sources.</p> <p>Possible Causes and Solutions:</p> <ol> <li>Connectivity Issues:</li> <li>Check if the linked service configuration is correct</li> <li>Verify network connectivity to external sources</li> <li> <p>Confirm firewall rules allow connections from Azure</p> </li> <li> <p>Authentication Problems:</p> </li> <li>Verify credentials in linked services</li> <li>Check for expired secrets or certificates</li> <li>Test connections independently of the pipeline</li> </ol>"},{"location":"troubleshooting/#getting-further-help","title":"Getting Further Help","text":"<p>If you continue experiencing issues after trying these troubleshooting steps, consider the following resources:</p> <ol> <li>Azure Synapse Analytics Documentation</li> <li>Microsoft Q&amp;A for Synapse</li> <li>Azure Support</li> <li>Stack Overflow - Azure Synapse</li> </ol>"},{"location":"architecture/","title":"Azure Synapse Analytics Architecture","text":"<p>This section provides comprehensive architectural guidance for implementing Azure Synapse Analytics solutions in enterprise environments.</p>"},{"location":"architecture/#overview","title":"Overview","text":"<p>Azure Synapse Analytics is Microsoft's unified analytics service that brings together enterprise data warehousing, big data processing, data integration, and AI capabilities. This architecture documentation covers proven patterns, implementation approaches, and best practices for building robust, scalable, and secure analytics solutions.</p>"},{"location":"architecture/#key-architecture-principles","title":"Key Architecture Principles","text":"<ol> <li>Unified Data Platform: Integrate all your data assets into a cohesive ecosystem</li> <li>Polyglot Processing: Choose the right compute engine for different workloads (SQL, Spark, Data Explorer)</li> <li>Decoupled Storage &amp; Compute: Scale resources independently and optimize costs</li> <li>Security-First Design: Implement comprehensive security at all layers</li> <li>Performance Optimization: Apply techniques for maximum throughput and query performance</li> </ol>"},{"location":"architecture/#reference-architectures","title":"Reference Architectures","text":"<ul> <li> <p>Delta Lakehouse Architecture: Enterprise-scale lakehouse implementation using Delta Lake and Synapse Spark pools. This modern approach provides ACID transactions, schema enforcement, and time travel capabilities while maintaining the flexibility of a data lake.</p> </li> <li> <p>Serverless SQL Architecture: Pay-per-query patterns for ad-hoc analytics over data lake storage. Learn how to implement a cost-effective query layer for your data lake without provisioning infrastructure.</p> </li> <li> <p>Shared Metadata Architecture: Implementation approaches for unified semantic layers that work across Synapse engines. Create consistent metadata that can be leveraged by SQL pools, Spark pools, and external tools.</p> </li> </ul>"},{"location":"architecture/#integration-patterns","title":"Integration Patterns","text":"<ul> <li>Data Lake Integration: Patterns for connecting Azure Synapse with Azure Data Lake Storage Gen2</li> <li>Power BI Integration: Architectural approaches for real-time and scheduled analytics visualizations</li> <li>Azure ML Integration: Methods for incorporating machine learning workflows into your analytics pipeline</li> <li>CI/CD Pipeline Integration: DevOps practices for Synapse workspace artifacts</li> </ul>"},{"location":"architecture/#architecture-decision-framework","title":"Architecture Decision Framework","text":"<p>Use this decision tree to determine the optimal Synapse architecture for your specific requirements:</p> <ol> <li>Primary Workload Type:</li> <li>Enterprise Data Warehouse \u2192 Dedicated SQL Pool</li> <li>Data Lake Analytics \u2192 Serverless SQL + Spark</li> <li>Real-time Analytics \u2192 Synapse Data Explorer</li> <li> <p>Mixed Workloads \u2192 Unified approach with multiple engines</p> </li> <li> <p>Data Volume and Velocity:</p> </li> <li>TB-scale structured data \u2192 Dedicated SQL Pool</li> <li>PB-scale mixed data \u2192 Spark + Delta Lake</li> <li> <p>Streaming data \u2192 Data Explorer or Spark Structured Streaming</p> </li> <li> <p>Query Patterns:</p> </li> <li>Complex joins and aggregations \u2192 Dedicated SQL Pool</li> <li>AI/ML and data science \u2192 Spark</li> <li>Ad-hoc exploration \u2192 Serverless SQL Pool</li> <li>Time-series and log analytics \u2192 Data Explorer</li> </ol>"},{"location":"architecture/delta-lakehouse-overview/","title":"Azure Synapse Analytics Delta Lakehouse Architecture","text":"<p>Home &gt; Architecture &gt; Delta Lakehouse Overview</p>"},{"location":"architecture/delta-lakehouse-overview/#overview","title":"Overview","text":"<p>Azure Synapse Analytics Delta Lakehouse is a unified analytics platform that combines the best of data warehousing and big data processing. This architecture enables organizations to build a modern data architecture that supports both analytics and operational workloads.</p>"},{"location":"architecture/delta-lakehouse-overview/#key-components","title":"Key Components","text":""},{"location":"architecture/delta-lakehouse-overview/#1-delta-lake","title":"1. Delta Lake","text":"<ul> <li> <p>Open-source storage layer that brings ACID transactions to Apache Spark and big data workloads</p> </li> <li> <p>Built on top of Apache Parquet</p> </li> <li> <p>Supports schema evolution and enforcement</p> </li> <li> <p>Provides time travel capabilities</p> </li> <li> <p>Optimized for both streaming and batch processing</p> </li> </ul>"},{"location":"architecture/delta-lakehouse-overview/#2-spark-processing","title":"2. Spark Processing","text":"<ul> <li> <p>Apache Spark as the compute engine</p> </li> <li> <p>Runs on Synapse Spark pools</p> </li> <li> <p>Supports both batch and real-time processing</p> </li> <li> <p>Native integration with Delta Lake</p> </li> </ul>"},{"location":"architecture/delta-lakehouse-overview/#3-storage-layer","title":"3. Storage Layer","text":"<ul> <li> <p>Azure Data Lake Storage Gen2</p> </li> <li> <p>ADLS Gen2 provides:</p> </li> <li> <p>High scalability</p> </li> <li>Secure access control</li> <li>Cost-effective storage</li> <li>Integration with Azure services</li> </ul>"},{"location":"architecture/delta-lakehouse-overview/#architecture-diagram","title":"Architecture Diagram","text":"<pre><code>graph TD\n    ADLS[ADLS Gen2 Storage] --&gt; Delta[Delta Lake]\n    Delta --&gt; Spark[Apache Spark]\n    Spark --&gt; Synapse[Synapse Analytics]\n    Synapse --&gt; Serverless[Serverless SQL]\n    Synapse --&gt; Dedicated[Dedicated SQL]\n\n    subgraph \"Data Ingestion\"\n        Ingest[Data Sources]\n        Ingest --&gt; ADLS\n    end\n\n    subgraph \"Processing Layer\"\n        Spark --&gt; Delta\n    end\n\n    subgraph \"Analytics Layer\"\n        Synapse --&gt; Serverless\n        Synapse --&gt; Dedicated\n    end\n</code></pre>"},{"location":"architecture/delta-lakehouse-overview/#key-features","title":"Key Features","text":""},{"location":"architecture/delta-lakehouse-overview/#1-schema-management","title":"1. Schema Management","text":"<ul> <li> <p>Schema enforcement</p> </li> <li> <p>Schema evolution</p> </li> <li> <p>Version control</p> </li> <li> <p>Time travel capabilities</p> </li> </ul>"},{"location":"architecture/delta-lakehouse-overview/#2-performance-optimization","title":"2. Performance Optimization","text":"<ul> <li> <p>Data skipping</p> </li> <li> <p>Z-ordering</p> </li> <li> <p>Clustering</p> </li> <li> <p>Statistics collection</p> </li> </ul>"},{"location":"architecture/delta-lakehouse-overview/#3-security","title":"3. Security","text":"<ul> <li> <p>Role-based access control</p> </li> <li> <p>Row-level security</p> </li> <li> <p>Data masking</p> </li> <li> <p>Audit logging</p> </li> </ul>"},{"location":"architecture/delta-lakehouse-overview/#best-practices","title":"Best Practices","text":""},{"location":"architecture/delta-lakehouse-overview/#storage-organization","title":"Storage Organization","text":"<ul> <li> <p>Use hierarchical folder structure</p> </li> <li> <p>Implement proper partitioning</p> </li> <li> <p>Regularly optimize files</p> </li> <li> <p>Use appropriate file sizes</p> </li> </ul>"},{"location":"architecture/delta-lakehouse-overview/#schema-design","title":"Schema Design","text":"<ul> <li> <p>Start with a flexible schema</p> </li> <li> <p>Plan for schema evolution</p> </li> <li> <p>Use appropriate data types</p> </li> <li> <p>Implement proper indexing</p> </li> </ul>"},{"location":"architecture/delta-lakehouse-overview/#performance","title":"Performance","text":"<ul> <li> <p>Use appropriate partitioning</p> </li> <li> <p>Implement proper bucketing</p> </li> <li> <p>Use Z-ordering for queries</p> </li> <li> <p>Regularly optimize files</p> </li> </ul>"},{"location":"architecture/delta-lakehouse-overview/#next-steps","title":"Next Steps","text":"<ol> <li>Serverless SQL Architecture</li> <li>Shared Metadata Architecture</li> <li>Best Practices</li> <li>Code Examples</li> </ol>"},{"location":"architecture/delta-lakehouse/","title":"Delta Lakehouse Architecture with Azure Synapse","text":""},{"location":"architecture/delta-lakehouse/#overview","title":"Overview","text":"<p>The Delta Lakehouse architecture combines the flexibility and cost-efficiency of a data lake with the data management and ACID transaction capabilities of a data warehouse. Azure Synapse Analytics provides native integration with Delta Lake format, enabling a modern and efficient lakehouse implementation.</p>"},{"location":"architecture/delta-lakehouse/#architecture-components","title":"Architecture Components","text":""},{"location":"architecture/delta-lakehouse/#core-components","title":"Core Components","text":"<ol> <li>Azure Data Lake Storage Gen2</li> <li>Foundation for storing all data in raw, refined, and curated zones</li> <li>Hierarchical namespace for efficient file organization</li> <li> <p>Fine-grained ACLs for security at folder and file levels</p> </li> <li> <p>Delta Lake</p> </li> <li>Open-source storage layer that brings ACID transactions to data lakes</li> <li>Schema enforcement and evolution capabilities</li> <li>Time travel (data versioning) for auditing and rollbacks</li> <li> <p>Support for optimized Parquet format for performance</p> </li> <li> <p>Azure Synapse Spark Pools</p> </li> <li>Distributed processing engine for data transformation</li> <li>Native support for Delta Lake format</li> <li>Scalable compute for batch and stream processing</li> <li> <p>Integration with Azure Machine Learning for advanced analytics</p> </li> <li> <p>Azure Synapse SQL</p> </li> <li>SQL interface for querying Delta tables</li> <li>Serverless pool for ad-hoc analytics</li> <li>Dedicated pool for enterprise data warehousing</li> </ol>"},{"location":"architecture/delta-lakehouse/#implementation-patterns","title":"Implementation Patterns","text":""},{"location":"architecture/delta-lakehouse/#multi-zone-data-organization","title":"Multi-Zone Data Organization","text":"<pre><code>adls://data/\n\u251c\u2500\u2500 raw/                  # Raw ingested data\n\u251c\u2500\u2500 refined/              # Cleansed and conformed data\n\u2514\u2500\u2500 curated/              # Business-ready data products\n</code></pre>"},{"location":"architecture/delta-lakehouse/#medallion-architecture","title":"Medallion Architecture","text":"<p>The medallion architecture organizes your Delta Lake data into layers with increasing data quality and refinement:</p> <ol> <li>Bronze Layer (Raw Data)</li> <li>Ingestion sink for all source data</li> <li>Preserves original data format and content</li> <li>Minimal transformation, primarily ELT</li> <li> <p>Schema-on-read approach</p> </li> <li> <p>Silver Layer (Refined Data)</p> </li> <li>Cleansed and conformed data</li> <li>Standardized formats and resolved duplicates</li> <li>Common data quality rules applied</li> <li> <p>Typically organized by domain or source system</p> </li> <li> <p>Gold Layer (Curated Data)</p> </li> <li>Business-level aggregates and metrics</li> <li>Dimensional models for reporting</li> <li>Feature tables for machine learning</li> <li>Optimized for specific analytical use cases</li> </ol>"},{"location":"architecture/delta-lakehouse/#performance-optimization","title":"Performance Optimization","text":""},{"location":"architecture/delta-lakehouse/#delta-optimizations","title":"Delta Optimizations","text":"<ul> <li>Data Skipping: Delta maintains statistics to skip irrelevant files during queries</li> <li>Z-Ordering: Multi-dimensional clustering for improved filtering performance</li> <li>Compaction: Small file consolidation to optimize read performance</li> <li>Caching: Metadata and data caching for frequently accessed tables</li> </ul>"},{"location":"architecture/delta-lakehouse/#spark-tuning","title":"Spark Tuning","text":"<ul> <li>Autoscaling: Configure Spark pools to scale based on workload</li> <li>Partition Management: Right-size partitions to optimize parallelism</li> <li>Memory Configuration: Allocate appropriate memory for shuffle and execution</li> <li>Query Plan Optimization: Analyze and tune Spark execution plans</li> </ul>"},{"location":"architecture/delta-lakehouse/#governance-and-security","title":"Governance and Security","text":"<ul> <li>Azure Purview Integration: Data cataloging and lineage tracking</li> <li>Column-Level Security: Fine-grained access control within tables</li> <li>Row-Level Security: Filter data based on user context</li> <li>Transparent Data Encryption: Data encryption at rest</li> </ul>"},{"location":"architecture/delta-lakehouse/#deployment-and-devops","title":"Deployment and DevOps","text":"<ul> <li>Infrastructure as Code: Deploy lakehouse components using ARM templates or Terraform</li> <li>CI/CD Pipelines: Automated testing and deployment of Spark notebooks and SQL scripts</li> <li>Monitoring: Azure Monitor integration for performance tracking and alerts</li> <li>Delta Live Tables: Declarative ETL framework for reliable pipeline development</li> </ul>"},{"location":"architecture/delta-lakehouse/#best-practices","title":"Best Practices","text":"<ol> <li>Implement a systematic approach to schema evolution</li> <li>Use appropriate partitioning strategies based on data access patterns</li> <li>Apply retention policies to manage data lifecycle efficiently</li> <li>Leverage checkpoint files for streaming workloads</li> <li>Implement Slowly Changing Dimension patterns for tracking historical changes</li> <li>Use Z-Ordering on frequently filtered columns</li> <li>Maintain separate compute clusters for ETL and query workloads</li> <li>Implement CI/CD practices for Delta table schema changes</li> </ol>"},{"location":"architecture/delta-lakehouse/detailed-architecture/","title":"Azure Synapse Analytics Delta Lakehouse Detailed Architecture","text":"<p>Home &gt; Architecture &gt; Delta Lakehouse &gt; Detailed Architecture</p>"},{"location":"architecture/delta-lakehouse/detailed-architecture/#overview","title":"Overview","text":"<p>The Delta Lakehouse architecture combines the best of data lakes and data warehouses, providing ACID transactions, schema enforcement, and time travel capabilities while maintaining the flexibility and scalability of a data lake. This document details the implementation of a Delta Lakehouse using Azure Synapse Analytics.</p>"},{"location":"architecture/delta-lakehouse/detailed-architecture/#core-components","title":"Core Components","text":""},{"location":"architecture/delta-lakehouse/detailed-architecture/#storage-layer","title":"Storage Layer","text":""},{"location":"architecture/delta-lakehouse/detailed-architecture/#azure-data-lake-storage-gen2-adls-gen2","title":"Azure Data Lake Storage Gen2 (ADLS Gen2)","text":"<ul> <li>Hierarchical namespace for efficient directory/file operations</li> <li>Built-in security with Azure Active Directory integration</li> <li>Cost-effective storage with tiering capabilities (hot, cool, archive)</li> <li>Designed for high throughput and parallelism</li> </ul>"},{"location":"architecture/delta-lakehouse/detailed-architecture/#storage-organization","title":"Storage Organization","text":"<pre><code>datalake/\n\u251c\u2500\u2500 bronze/             # Raw ingested data\n\u2502   \u251c\u2500\u2500 source1/\n\u2502   \u2514\u2500\u2500 source2/\n\u251c\u2500\u2500 silver/             # Cleaned and transformed data\n\u2502   \u251c\u2500\u2500 dimension1/\n\u2502   \u2514\u2500\u2500 fact1/\n\u2514\u2500\u2500 gold/               # Business-level aggregated data\n    \u251c\u2500\u2500 reports/\n    \u2514\u2500\u2500 analytics/\n</code></pre>"},{"location":"architecture/delta-lakehouse/detailed-architecture/#compute-layer","title":"Compute Layer","text":""},{"location":"architecture/delta-lakehouse/detailed-architecture/#azure-synapse-spark-pools","title":"Azure Synapse Spark Pools","text":"<ul> <li>Fully managed Apache Spark service</li> <li>Autoscaling capabilities based on workload</li> <li>Native integration with Delta Lake</li> <li>Configurable for memory-optimized or compute-optimized workloads</li> </ul>"},{"location":"architecture/delta-lakehouse/detailed-architecture/#pool-configurations","title":"Pool Configurations","text":"Pool Type Node Size Autoscale Use Case Small Medium (8 vCores) 3-10 nodes Development, testing Medium Large (16 vCores) 5-20 nodes Production ETL Large XLarge (32 vCores) 10-40 nodes Data science workloads"},{"location":"architecture/delta-lakehouse/detailed-architecture/#delta-lake-integration","title":"Delta Lake Integration","text":""},{"location":"architecture/delta-lakehouse/detailed-architecture/#key-components","title":"Key Components","text":"<ul> <li>Transaction log for ACID compliance</li> <li>Optimistic concurrency control</li> <li>Schema enforcement and evolution</li> <li>Data skipping and Z-ordering for query optimization</li> <li>Time travel capabilities</li> </ul>"},{"location":"architecture/delta-lakehouse/detailed-architecture/#implementation","title":"Implementation","text":"<pre><code># Example of configuring Spark with Delta Lake\nspark = SparkSession.builder \\\n    .appName(\"Delta Lake Configuration\") \\\n    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n    .getOrCreate()\n\n# Setting Delta specific configurations\nspark.conf.set(\"spark.databricks.delta.properties.defaults.enableChangeDataFeed\", \"true\")\nspark.conf.set(\"spark.databricks.delta.optimize.maxFileSize\", 1024 * 1024 * 256)  # 256MB\nspark.conf.set(\"spark.databricks.delta.autoCompact.enabled\", \"true\")\n</code></pre>"},{"location":"architecture/delta-lakehouse/detailed-architecture/#architecture-patterns","title":"Architecture Patterns","text":""},{"location":"architecture/delta-lakehouse/detailed-architecture/#bronze-silver-gold-pattern","title":"Bronze-Silver-Gold Pattern","text":""},{"location":"architecture/delta-lakehouse/detailed-architecture/#bronze-layer-raw-data","title":"Bronze Layer (Raw Data)","text":"<ul> <li>Ingests data in raw format with minimal transformation</li> <li>Preserves original data for auditing and reprocessing</li> <li>Implemented as Delta tables with schema inference</li> <li>Retention policies based on compliance requirements</li> </ul>"},{"location":"architecture/delta-lakehouse/detailed-architecture/#silver-layer-processed-data","title":"Silver Layer (Processed Data)","text":"<ul> <li>Cleaned and conformed data</li> <li>Standardized formats and data types</li> <li>Data quality checks and validation</li> <li>Implemented as Delta tables with strict schemas</li> </ul>"},{"location":"architecture/delta-lakehouse/detailed-architecture/#gold-layer-business-data","title":"Gold Layer (Business Data)","text":"<ul> <li>Aggregated, enriched data ready for consumption</li> <li>Optimized for specific business domains or use cases</li> <li>Often dimensional models or denormalized structures</li> <li>Implemented as Delta tables optimized for query performance</li> </ul>"},{"location":"architecture/delta-lakehouse/detailed-architecture/#data-ingestion-patterns","title":"Data Ingestion Patterns","text":""},{"location":"architecture/delta-lakehouse/detailed-architecture/#batch-ingestion","title":"Batch Ingestion","text":"<ul> <li>Using Azure Synapse pipelines for orchestration</li> <li>Scheduled or event-triggered processing</li> <li>Support for various source formats (CSV, JSON, Parquet, etc.)</li> <li>Parallel loading for high-volume data</li> </ul>"},{"location":"architecture/delta-lakehouse/detailed-architecture/#stream-ingestion","title":"Stream Ingestion","text":"<ul> <li>Integration with Azure Event Hubs or Kafka</li> <li>Real-time processing with Structured Streaming</li> <li>Delta Lake's support for streaming writes</li> <li>Auto-compaction for optimizing small files</li> </ul>"},{"location":"architecture/delta-lakehouse/detailed-architecture/#data-processing-patterns","title":"Data Processing Patterns","text":""},{"location":"architecture/delta-lakehouse/detailed-architecture/#elt-extract-load-transform","title":"ELT (Extract, Load, Transform)","text":"<ul> <li>Load raw data into Bronze layer</li> <li>Transform in-place using Spark SQL or DataFrame APIs</li> <li>Move processed data to Silver and Gold layers</li> <li>Leverages Synapse's distributed processing capabilities</li> <li>Optimize and manage metadata with VACUUM and ANALYZE</li> </ul>"},{"location":"architecture/delta-lakehouse/detailed-architecture/#advanced-features","title":"Advanced Features","text":""},{"location":"architecture/delta-lakehouse/detailed-architecture/#time-travel-and-versioning","title":"Time Travel and Versioning","text":"<p>Delta Lake provides time travel capabilities, allowing queries against previous versions of the data. This is particularly useful for:</p> <ul> <li>Auditing and compliance</li> <li>Debugging and rollback scenarios</li> <li>Point-in-time analysis</li> <li>Reproducible reporting</li> </ul> <pre><code>-- Query data as of a specific timestamp\nSELECT * FROM delta.`/path/to/table` TIMESTAMP AS OF '2025-08-01 00:00:00'\n\n-- Query data as of a specific version\nSELECT * FROM delta.`/path/to/table` VERSION AS OF 123\n</code></pre>"},{"location":"architecture/delta-lakehouse/detailed-architecture/#schema-evolution","title":"Schema Evolution","text":"<p>Delta Lake supports schema evolution, allowing tables to adapt as data structures change over time:</p> <ul> <li>Add new columns</li> <li>Change data types (with compatible conversions)</li> <li>Rename columns using column mapping</li> </ul> <pre><code>-- Add a new column with a default value\nALTER TABLE delta_table ADD COLUMN new_column STRING DEFAULT 'default_value'\n</code></pre>"},{"location":"architecture/delta-lakehouse/detailed-architecture/#change-data-capture-cdc","title":"Change Data Capture (CDC)","text":"<p>Delta Lake supports Change Data Feed, enabling downstream systems to consume only changed data:</p> <pre><code># Enable CDC on a Delta table\nspark.sql(\"ALTER TABLE delta_table SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\")\n\n# Read changes between versions\nchanges = spark.read.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .option(\"startingVersion\", 5) \\\n    .option(\"endingVersion\", 10) \\\n    .table(\"delta_table\")\n</code></pre>"},{"location":"architecture/delta-lakehouse/detailed-architecture/#optimizations","title":"Optimizations","text":""},{"location":"architecture/delta-lakehouse/detailed-architecture/#data-skipping","title":"Data Skipping","text":"<ul> <li>Delta Lake maintains statistics on data files</li> <li>Query predicates use these statistics to skip irrelevant files</li> <li>Significantly improves query performance</li> </ul>"},{"location":"architecture/delta-lakehouse/detailed-architecture/#z-ordering","title":"Z-Ordering","text":"<ul> <li>Multi-dimensional clustering technique</li> <li>Colocates related data together</li> <li>Improves query performance when filtering on Z-ordered columns</li> </ul> <pre><code>-- Z-order by multiple columns\nOPTIMIZE delta_table ZORDER BY (date_column, region_column)\n</code></pre>"},{"location":"architecture/delta-lakehouse/detailed-architecture/#file-compaction","title":"File Compaction","text":"<ul> <li>Combines small files into larger ones</li> <li>Reduces metadata overhead</li> <li>Improves scan performance</li> </ul> <pre><code>-- Compact files without Z-ordering\nOPTIMIZE delta_table\n</code></pre>"},{"location":"architecture/delta-lakehouse/detailed-architecture/#security-and-governance","title":"Security and Governance","text":""},{"location":"architecture/delta-lakehouse/detailed-architecture/#authentication-and-authorization","title":"Authentication and Authorization","text":""},{"location":"architecture/delta-lakehouse/detailed-architecture/#azure-active-directory-integration","title":"Azure Active Directory Integration","text":"<ul> <li>Single sign-on with Azure AD</li> <li>Role-based access control (RBAC)</li> <li>Integration with existing identity systems</li> <li>Support for managed identities</li> </ul>"},{"location":"architecture/delta-lakehouse/detailed-architecture/#fine-grained-access-control","title":"Fine-grained Access Control","text":"<ul> <li>Table-level and column-level security</li> <li>Row-level security through Delta Lake filters</li> <li>Dynamic data masking for sensitive fields</li> </ul>"},{"location":"architecture/delta-lakehouse/detailed-architecture/#data-governance","title":"Data Governance","text":""},{"location":"architecture/delta-lakehouse/detailed-architecture/#azure-purview-integration","title":"Azure Purview Integration","text":"<ul> <li>Automated data discovery and classification</li> <li>Data lineage tracking</li> <li>Sensitive data identification</li> <li>Centralized metadata management</li> </ul>"},{"location":"architecture/delta-lakehouse/detailed-architecture/#metadata-management","title":"Metadata Management","text":"<ul> <li>Schema history tracking</li> <li>Transaction history logging</li> <li>Origin tracking with detailed provenance</li> <li>Integration with external metadata systems</li> </ul>"},{"location":"architecture/delta-lakehouse/detailed-architecture/#monitoring-and-optimization","title":"Monitoring and Optimization","text":""},{"location":"architecture/delta-lakehouse/detailed-architecture/#performance-monitoring","title":"Performance Monitoring","text":""},{"location":"architecture/delta-lakehouse/detailed-architecture/#azure-monitor-integration","title":"Azure Monitor Integration","text":"<ul> <li>Resource utilization tracking</li> <li>Query performance metrics</li> <li>Cost analysis</li> <li>Alerting on performance degradation</li> </ul>"},{"location":"architecture/delta-lakehouse/detailed-architecture/#delta-specific-metrics","title":"Delta-specific Metrics","text":"<ul> <li>Transaction log size and growth rate</li> <li>Data skipping effectiveness</li> <li>Compaction efficiency</li> <li>Read/write throughput</li> </ul>"},{"location":"architecture/delta-lakehouse/detailed-architecture/#cost-optimization-strategies","title":"Cost Optimization Strategies","text":""},{"location":"architecture/delta-lakehouse/detailed-architecture/#storage-optimization","title":"Storage Optimization","text":"<ul> <li>Tiered storage policies</li> <li>Data lifecycle management</li> <li>Vacuum operations to remove stale files</li> <li>Compression settings optimization</li> </ul>"},{"location":"architecture/delta-lakehouse/detailed-architecture/#compute-optimization","title":"Compute Optimization","text":"<ul> <li>Right-sizing Spark pools</li> <li>Autoscaling configurations</li> <li>Workload isolation for predictable performance</li> <li>Caching strategies for frequently accessed data</li> </ul>"},{"location":"architecture/delta-lakehouse/detailed-architecture/#integration-points","title":"Integration Points","text":""},{"location":"architecture/delta-lakehouse/detailed-architecture/#synapse-sql-integration","title":"Synapse SQL Integration","text":"<ul> <li>Query Delta tables directly from Serverless SQL pools</li> <li>Create external tables over Delta format</li> <li> <p>Join between Delta Lake and other data sources</p> </li> <li> <p>Cross-engine queries (Spark and SQL)</p> </li> </ul>"},{"location":"architecture/delta-lakehouse/detailed-architecture/#power-bi-integration","title":"Power BI Integration","text":"<ul> <li>Direct Query support for Delta tables</li> <li>Composite models combining Delta Lake with other sources</li> <li> <p>Incremental refresh based on Delta Lake partitioning</p> </li> <li> <p>Enterprise-scale semantic models</p> </li> </ul>"},{"location":"architecture/delta-lakehouse/detailed-architecture/#azure-machine-learning","title":"Azure Machine Learning","text":"<ul> <li>Feature store implementation using Delta Lake</li> <li>Model training on Delta tables</li> <li> <p>Model deployment with feature versioning</p> </li> <li> <p>MLOps workflows with data and model versioning</p> </li> </ul>"},{"location":"architecture/delta-lakehouse/detailed-architecture/#reference-implementation","title":"Reference Implementation","text":"<p>For a detailed reference implementation of Delta Lakehouse in Azure Synapse Analytics, refer to the code examples section of this documentation.</p>"},{"location":"architecture/serverless-sql/","title":"Index","text":"<p>Home &gt; Architecture &gt; Serverless SQL</p>"},{"location":"architecture/serverless-sql/#serverless-sql-architecture","title":"Serverless SQL Architecture","text":"<p>Serverless SQL architecture in Azure Synapse Analytics allows you to query data directly in your data lake without moving or copying data, using familiar T-SQL syntax.</p>"},{"location":"architecture/serverless-sql/#documentation","title":"Documentation","text":"<ul> <li>Serverless SQL Overview - Introduction to Serverless SQL capabilities</li> <li>Detailed Architecture - Comprehensive technical architecture of Serverless SQL implementation</li> </ul>"},{"location":"architecture/serverless-sql/#key-features","title":"Key Features","text":"<ul> <li>On-demand querying with no infrastructure to manage</li> <li>Pay-per-query cost model</li> <li>T-SQL compatibility</li> <li>Native integration with Azure Data Lake Storage</li> <li>Built-in data virtualization</li> <li>Seamless integration with visualization tools</li> </ul>"},{"location":"architecture/serverless-sql/#related-resources","title":"Related Resources","text":"<ul> <li>Best Practices for Serverless SQL</li> <li>Code Examples</li> <li>Reference Documentation</li> </ul>"},{"location":"architecture/serverless-sql/detailed-architecture/","title":"Detailed architecture","text":"<p>Home &gt; Architecture &gt; Serverless SQL &gt; Detailed Architecture</p>"},{"location":"architecture/serverless-sql/detailed-architecture/#azure-synapse-analytics-serverless-sql-detailed-architecture","title":"Azure Synapse Analytics Serverless SQL: Detailed Architecture","text":""},{"location":"architecture/serverless-sql/detailed-architecture/#overview","title":"Overview","text":"<p>Azure Synapse Serverless SQL Pool provides on-demand, auto-scaling SQL query capabilities without the need to provision or manage infrastructure. This document provides a detailed technical overview of the serverless SQL architecture in Azure Synapse Analytics, focusing on querying data lakes, integrating with Delta Lake format, and optimizing for performance and cost.</p>"},{"location":"architecture/serverless-sql/detailed-architecture/#core-architecture","title":"Core Architecture","text":""},{"location":"architecture/serverless-sql/detailed-architecture/#distributed-query-processing","title":"Distributed Query Processing","text":"<p>Serverless SQL in Azure Synapse Analytics utilizes a distributed query processing architecture:</p> <ol> <li>Query Parsing and Planning</li> <li>SQL query parsing and syntax validation</li> <li>Query plan optimization based on statistics and metadata</li> <li> <p>Distributed execution plan generation</p> </li> <li> <p>Compute Layer</p> </li> <li>Dynamically allocated compute resources based on query complexity</li> <li>Automatic scaling during query execution</li> <li> <p>Pay-per-query billing model (TB processed)</p> </li> <li> <p>Data Access Layer</p> </li> <li>Parallel data access to storage systems</li> <li>Native support for multiple file formats</li> <li>Data virtualization capabilities</li> </ol>"},{"location":"architecture/serverless-sql/detailed-architecture/#logical-architecture","title":"Logical Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                 Client Applications               \u2502\n\u2502  (SSMS, Azure Data Studio, Power BI, Custom Apps) \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                    \u2502\n                    \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502               Synapse SQL Endpoint                \u2502\n\u2502           (TDS Protocol over TCP/IP)              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                    \u2502\n                    \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              Query Processing Engine              \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Query Parser  \u2502 Query Planner \u2502 Query Optimizer   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                            \u2502\n                            \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502            Distributed Query Execution            \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Data Access  \u2502  Processing   \u2502  Result Assembly  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                            \u2502\n                            \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                  Storage Layer                    \u2502\n\u2502       (ADLS Gen2, Azure Blob, Delta Lake)         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/serverless-sql/detailed-architecture/#key-components","title":"Key Components","text":""},{"location":"architecture/serverless-sql/detailed-architecture/#endpoint-management","title":"Endpoint Management","text":"<p>Serverless SQL Pool provides a dedicated SQL endpoint with:</p> <ul> <li>Standard TDS (Tabular Data Stream) protocol support</li> <li>Compatibility with standard SQL clients and tools</li> <li>Always-on connectivity for applications</li> <li>Connection pooling and management</li> </ul>"},{"location":"architecture/serverless-sql/detailed-architecture/#resource-management","title":"Resource Management","text":"<p>Dynamic Resource Allocation - Resources automatically scale based on query complexity - Parallel processing adapts to data volume and query patterns - CPU and memory allocation optimized for each query phase - Isolation between multiple concurrent queries</p> <p>Billing Model - Pay only for data processed during query execution - Billed per TB of data scanned - No charges when idle - Predictable cost model for data exploration and analytics</p>"},{"location":"architecture/serverless-sql/detailed-architecture/#query-processing","title":"Query Processing","text":"<p>Query Compilation - SQL query parsing and validation - Syntax compatibility with T-SQL - Query plan optimization for distributed execution - Statistics-based cardinality estimation</p> <p>Execution Engine - Massively parallel processing (MPP) architecture - Distributed query execution across multiple nodes - Dynamic node allocation based on workload - Fault-tolerant execution with node failover</p>"},{"location":"architecture/serverless-sql/detailed-architecture/#data-access-capabilities","title":"Data Access Capabilities","text":""},{"location":"architecture/serverless-sql/detailed-architecture/#file-format-support","title":"File Format Support","text":"<p>Serverless SQL Pool provides native support for multiple file formats:</p> Format Key Features Best For Parquet Columnar storage, compression, predicate pushdown Analytics workloads, high-performance queries Delta ACID transactions, time travel, schema evolution Data lakes with transactional requirements CSV Human-readable, widely supported, variable delimiters Data exchange, simple datasets JSON Semi-structured data, nested objects, arrays Application logs, API data, flexible schemas"},{"location":"architecture/serverless-sql/detailed-architecture/#external-tables","title":"External Tables","text":"<p>Serverless SQL enables creating metadata-driven external tables that provide:</p> <ul> <li>Schema-on-read capabilities with schema enforcement</li> <li>Statistics collection for better query optimization</li> <li>Persistent metadata for consistent data access</li> <li>Security and access control integration</li> </ul> <pre><code>-- Example of creating an external table over Delta format\nCREATE EXTERNAL TABLE ExternalDeltaTable\n(\n    CustomerID INT,\n    Name NVARCHAR(100),\n    OrderDate DATE,\n    Amount DECIMAL(18,2)\n)\nWITH\n(\n    LOCATION = 'orders/delta/',\n    DATA_SOURCE = ExternalDataSource,\n    FILE_FORMAT = DeltaFormat\n)\n</code></pre>"},{"location":"architecture/serverless-sql/detailed-architecture/#query-syntax-extensions","title":"Query Syntax Extensions","text":"<p>Serverless SQL Pool extends T-SQL with specialized syntax for external data access:</p> <p>OPENROWSET - Ad-hoc queries against file storage - Schema inference capabilities - Format-specific options for optimal access</p> <pre><code>-- Example of querying Delta format with OPENROWSET\nSELECT TOP 100 *\nFROM OPENROWSET(\n    BULK 'https://account.dfs.core.windows.net/container/path/to/delta/',\n    FORMAT = 'DELTA'\n) AS [result]\n</code></pre> <p>Specialized Functions - <code>FILEPATH()</code> - Access file path information - <code>FILENAME()</code> - Extract filename from path - <code>FORMAT_TYPE()</code> - Determine file format details</p>"},{"location":"architecture/serverless-sql/detailed-architecture/#integration-with-delta-lake","title":"Integration with Delta Lake","text":""},{"location":"architecture/serverless-sql/detailed-architecture/#reading-delta-tables","title":"Reading Delta Tables","text":"<p>Serverless SQL Pool provides native support for reading Delta Lake tables:</p> <ul> <li>Reads Delta transaction log to find latest snapshot</li> <li>Honors partition pruning for efficient data access</li> <li>Supports time travel queries using timestamp or version</li> </ul> <pre><code>-- Query latest version of Delta table\nSELECT * FROM OPENROWSET(\n    BULK 'https://account.dfs.core.windows.net/container/path/to/delta/',\n    FORMAT = 'DELTA'\n) AS [data]\n\n-- Query specific version of Delta table\nSELECT * FROM OPENROWSET(\n    BULK 'https://account.dfs.core.windows.net/container/path/to/delta/',\n    FORMAT = 'DELTA',\n    DELTA_VERSION = 5\n) AS [data]\n</code></pre>"},{"location":"architecture/serverless-sql/detailed-architecture/#metadata-integration","title":"Metadata Integration","text":"<p>Schema Discovery - Automatic schema inference from Delta metadata - Data type mapping between Spark and SQL types - Support for nested structures and arrays</p> <p>Statistics Utilization - Leverages Delta statistics for query optimization - Data skipping based on min/max values - Partition elimination for efficient data access</p>"},{"location":"architecture/serverless-sql/detailed-architecture/#security-and-access-control","title":"Security and Access Control","text":""},{"location":"architecture/serverless-sql/detailed-architecture/#authentication-methods","title":"Authentication Methods","text":"<p>Serverless SQL Pool supports multiple authentication methods:</p> <ul> <li>Azure Active Directory integration</li> <li>SQL authentication for legacy applications</li> <li>Managed identities for service-to-service authentication</li> <li>Azure AD Pass-through for end-user identity flow</li> </ul>"},{"location":"architecture/serverless-sql/detailed-architecture/#authorization-and-access-control","title":"Authorization and Access Control","text":"<p>Resource-level Security - Role-based access control (RBAC) on Synapse workspace - SQL role-based security for database objects - Managed private endpoints for network isolation</p> <p>Data-level Security - Row-level security (RLS) policies - Column-level security and data masking - Azure storage access control with SAS or AAD</p> <pre><code>-- Example of row-level security implementation\nCREATE SECURITY POLICY SalesDataFilter\nADD FILTER PREDICATE dbo.fn_securitypredicate(RegionID) ON dbo.SalesData\nWITH (STATE = ON);\n</code></pre>"},{"location":"architecture/serverless-sql/detailed-architecture/#performance-optimization","title":"Performance Optimization","text":""},{"location":"architecture/serverless-sql/detailed-architecture/#query-performance-techniques","title":"Query Performance Techniques","text":"<p>Data Layout Optimization - Partitioning strategies for efficient filtering - File size optimization (recommended: 100MB-1GB) - Data organization for common access patterns</p> <p>Predicate Pushdown - Filter pushdown to storage layer - Column pruning for reading only required fields - Partition elimination for scanned data reduction</p> <p>Statistics Management - Creating statistics on key columns - AUTO_CREATE_STATISTICS option - Regular statistics updates for changing data</p> <pre><code>-- Create statistics for better query plans\nCREATE STATISTICS Stats_OrderDate ON ExternalTable(OrderDate);\n</code></pre>"},{"location":"architecture/serverless-sql/detailed-architecture/#caching-mechanisms","title":"Caching Mechanisms","text":"<p>Result Set Cache - Automatic caching of query results - Cache invalidation on data changes - Configurable TTL for cached results</p> <p>Metadata Caching - Storage of file listings and statistics - Schema caching for faster queries - Partition metadata for efficient access</p>"},{"location":"architecture/serverless-sql/detailed-architecture/#scaling-and-limits","title":"Scaling and Limits","text":""},{"location":"architecture/serverless-sql/detailed-architecture/#concurrency-management","title":"Concurrency Management","text":"<p>Serverless SQL Pool provides built-in concurrency control:</p> <ul> <li>Dynamic resource management for concurrent queries</li> <li>Workload classification and importance</li> <li>Query queuing during high concurrency periods</li> <li>Configurable concurrency limits by resource class</li> </ul>"},{"location":"architecture/serverless-sql/detailed-architecture/#resource-limits","title":"Resource Limits","text":"<p>Key resource limitations to consider:</p> Resource Limit Maximum query memory 1 GB per DW100 Maximum query execution time 60 minutes Maximum result set size 10 GB Maximum columns per table 1,024 Maximum SQL statement size 1 MB Maximum concurrent queries Varies by resource class"},{"location":"architecture/serverless-sql/detailed-architecture/#integration-scenarios","title":"Integration Scenarios","text":""},{"location":"architecture/serverless-sql/detailed-architecture/#cross-engine-queries","title":"Cross-Engine Queries","text":"<p>Serverless SQL Pool can participate in cross-engine queries:</p> <ul> <li>Query Spark tables from SQL</li> <li>Join data between SQL and Spark</li> <li>Create views combining multiple sources</li> <li>Cross-service parameterized queries</li> </ul>"},{"location":"architecture/serverless-sql/detailed-architecture/#synapse-link-integration","title":"Synapse Link Integration","text":"<p>Native integration with Synapse Link for:</p> <ul> <li>Azure Cosmos DB analytical store</li> <li>Azure SQL Database change feed</li> <li>Near real-time analytics on operational data</li> <li>Hybrid HTAP (Hybrid Transactional/Analytical Processing) workloads</li> </ul>"},{"location":"architecture/serverless-sql/detailed-architecture/#powerbi-integration","title":"PowerBI Integration","text":"<p>Optimized connection patterns for PowerBI:</p> <ul> <li>DirectQuery for real-time data access</li> <li>Import mode for pre-aggregated datasets</li> <li>Composite models combining multiple sources</li> <li>Row-level security pass-through</li> </ul>"},{"location":"architecture/serverless-sql/detailed-architecture/#monitoring-and-management","title":"Monitoring and Management","text":""},{"location":"architecture/serverless-sql/detailed-architecture/#query-monitoring","title":"Query Monitoring","text":"<p>Comprehensive monitoring capabilities:</p> <ul> <li>Dynamic management views (DMVs) for query insights</li> <li>Query execution plans and statistics</li> <li>Resource utilization metrics</li> <li>Query performance troubleshooting</li> </ul> <pre><code>-- Example of monitoring active queries\nSELECT\n    request_id,\n    session_id,\n    status,\n    submit_time,\n    total_elapsed_time,\n    command\nFROM\n    sys.dm_pdw_exec_requests\nWHERE\n    status NOT IN ('Completed', 'Failed', 'Cancelled')\nORDER BY\n    submit_time DESC;\n</code></pre>"},{"location":"architecture/serverless-sql/detailed-architecture/#cost-management","title":"Cost Management","text":"<p>Tools and practices for cost optimization:</p> <ul> <li>Query data volume estimation</li> <li>Cost tracking by query and user</li> <li>Alerting on excessive data processing</li> <li>Query optimization for reduced data scanning</li> </ul>"},{"location":"architecture/serverless-sql/detailed-architecture/#best-practices","title":"Best Practices","text":""},{"location":"architecture/serverless-sql/detailed-architecture/#query-optimization","title":"Query Optimization","text":"<ul> <li>Filter data as early as possible in the query</li> <li>Limit columns selected to only those needed</li> <li>Use appropriate data types for joins and comparisons</li> <li>Optimize file formats and compression settings</li> <li>Use partitioning aligned with common query patterns</li> </ul>"},{"location":"architecture/serverless-sql/detailed-architecture/#data-organization","title":"Data Organization","text":"<ul> <li>Implement logical partitioning by date/business unit</li> <li>Target optimal file sizes (100MB-1GB)</li> <li>Use Delta Lake for frequently updated data</li> <li>Implement a medallion architecture (bronze/silver/gold)</li> <li>Consider data lifecycle management for cost optimization</li> </ul>"},{"location":"architecture/serverless-sql/detailed-architecture/#reference-architecture-patterns","title":"Reference Architecture Patterns","text":""},{"location":"architecture/serverless-sql/detailed-architecture/#data-lake-query-layer","title":"Data Lake Query Layer","text":"<p>Using Serverless SQL as a query layer over a data lake:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Power BI    \u251c\u2500\u2500\u2500\u25ba\u2502 Serverless  \u251c\u2500\u2500\u2500\u25ba\u2502 Data Lake   \u2502\n\u2502 Excel       \u2502    \u2502 SQL Pool    \u2502    \u2502 (ADLS Gen2) \u2502\n\u2502 SSMS        \u2502    \u2502             \u2502    \u2502             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/serverless-sql/detailed-architecture/#hybrid-query-architecture","title":"Hybrid Query Architecture","text":"<p>Combining dedicated and serverless pools for different workloads:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Mission-    \u251c\u2500\u2500\u2500\u25ba\u2502 Dedicated   \u251c\u2500\u2500\u2500\u25ba\u2502 Curated     \u2502\n\u2502 Critical    \u2502    \u2502 SQL Pool    \u2502    \u2502 Data Mart   \u2502\n\u2502 Reports     \u2502    \u2502             \u2502    \u2502             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n      \u2502                                      \u25b2\n      \u2502                                      \u2502\n      \u2502            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\u2502 Serverless  \u251c\u2500\u2500\u2500\u25ba\u2502 Data Lake   \u2502\n                   \u2502 SQL Pool    \u2502    \u2502 (Raw Data)  \u2502\n                   \u2502             \u2502    \u2502             \u2502\n                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/serverless-sql/detailed-architecture/#data-virtualization-hub","title":"Data Virtualization Hub","text":"<p>Using Serverless SQL as a data virtualization layer:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 BI Tools    \u2502    \u2502 Serverless  \u2502    \u2502 Azure SQL   \u2502\n\u2502 Custom Apps \u251c\u2500\u2500\u2500\u25ba\u2502 SQL Pool    \u251c\u2500\u2500\u2500\u25ba\u2502 Cosmos DB   \u2502\n\u2502 Reporting   \u2502    \u2502 (Polybase)  \u2502    \u2502 Data Lake   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/serverless-sql/detailed-architecture/#reference-implementation","title":"Reference Implementation","text":"<p>For detailed code samples and implementation patterns for Serverless SQL in Azure Synapse Analytics, refer to the code examples section of this documentation.</p>"},{"location":"architecture/serverless-sql/serverless-overview/","title":"Serverless overview","text":"<p>Home &gt; Architecture &gt; Serverless SQL &gt; Overview</p>"},{"location":"architecture/serverless-sql/serverless-overview/#azure-synapse-serverless-sql-architecture","title":"Azure Synapse Serverless SQL Architecture","text":""},{"location":"architecture/serverless-sql/serverless-overview/#overview","title":"Overview","text":"<p>Azure Synapse Serverless SQL is a serverless query engine that enables you to query data in your data lake using SQL without managing any infrastructure. It works seamlessly with Delta Lakehouse and provides several advantages over traditional SQL databases.</p>"},{"location":"architecture/serverless-sql/serverless-overview/#key-features","title":"Key Features","text":""},{"location":"architecture/serverless-sql/serverless-overview/#1-serverless-architecture","title":"1. Serverless Architecture","text":"<ul> <li>No infrastructure management</li> <li>Pay-per-query pricing</li> <li>Automatic scaling</li> <li>High availability</li> </ul>"},{"location":"architecture/serverless-sql/serverless-overview/#2-data-access","title":"2. Data Access","text":"<ul> <li>Query data directly from ADLS Gen2</li> <li>Support for multiple file formats:</li> <li>Parquet</li> <li>Delta</li> <li>CSV</li> <li>JSON</li> <li>Avro</li> </ul>"},{"location":"architecture/serverless-sql/serverless-overview/#3-performance-optimizations","title":"3. Performance Optimizations","text":"<ul> <li>Pushdown predicates</li> <li>Columnar processing</li> <li>Caching</li> <li>Query optimization</li> </ul>"},{"location":"architecture/serverless-sql/serverless-overview/#architecture-components","title":"Architecture Components","text":""},{"location":"architecture/serverless-sql/serverless-overview/#1-external-tables","title":"1. External Tables","text":"<ul> <li>Define schema over existing data</li> <li>Support for partitioned data</li> <li>Statistics collection</li> <li>Row-level security</li> </ul>"},{"location":"architecture/serverless-sql/serverless-overview/#2-views","title":"2. Views","text":"<ul> <li>Materialized views</li> <li>Regular views</li> <li>Security views</li> </ul>"},{"location":"architecture/serverless-sql/serverless-overview/#3-security","title":"3. Security","text":"<ul> <li>Role-based access control</li> <li>Row-level security</li> <li>Column-level security</li> <li>Secure data access</li> </ul>"},{"location":"architecture/serverless-sql/serverless-overview/#shared-metadata-architecture","title":"Shared Metadata Architecture","text":"<pre><code>graph TD\n    ADLS[ADLS Gen2 Storage] --&gt; Delta[Delta Lake]\n    Delta --&gt; Serverless[Serverless SQL]\n    Delta --&gt; Spark[Spark Pool]\n\n    subgraph \"Metadata Layer\"\n        Metadata --&gt; Delta\n        Metadata --&gt; Serverless\n        Metadata --&gt; Spark\n    end\n\n    subgraph \"Security\"\n        Security --&gt; Metadata\n        Security --&gt; ADLS\n    end\n</code></pre>"},{"location":"architecture/serverless-sql/serverless-overview/#best-practices","title":"Best Practices","text":""},{"location":"architecture/serverless-sql/serverless-overview/#schema-design","title":"Schema Design","text":"<ul> <li>Use appropriate data types</li> <li>Implement proper statistics</li> <li>Use meaningful column names</li> <li>Plan for schema evolution</li> </ul>"},{"location":"architecture/serverless-sql/serverless-overview/#performance","title":"Performance","text":"<ul> <li>Use appropriate partitioning</li> <li>Implement proper indexing</li> <li>Use query hints when needed</li> <li>Regularly update statistics</li> </ul>"},{"location":"architecture/serverless-sql/serverless-overview/#security","title":"Security","text":"<ul> <li>Implement proper RBAC</li> <li>Use row-level security</li> <li>Regularly audit access</li> <li>Use secure connection strings</li> </ul>"},{"location":"architecture/serverless-sql/serverless-overview/#code-examples","title":"Code Examples","text":""},{"location":"architecture/serverless-sql/serverless-overview/#creating-external-tables","title":"Creating External Tables","text":"<pre><code>CREATE EXTERNAL TABLE my_table\nWITH (\n    LOCATION = 'abfss://container@storageaccount.dfs.core.windows.net/path',\n    DATA_SOURCE = my_datasource,\n    FILE_FORMAT = parquet_format\n)\nAS SELECT * FROM source_table\n</code></pre>"},{"location":"architecture/serverless-sql/serverless-overview/#creating-views","title":"Creating Views","text":"<pre><code>CREATE VIEW secure_view\nWITH (NOEXPAND)\nAS\nSELECT * FROM my_table\nWHERE sensitive_column = 'public'\n</code></pre>"},{"location":"architecture/serverless-sql/serverless-overview/#query-optimization","title":"Query Optimization","text":"<pre><code>SELECT /*+ PUSHDOWN */\n    customer_id,\n    COUNT(*) as order_count\nFROM orders\nWHERE order_date &gt;= '2024-01-01'\nGROUP BY customer_id\n</code></pre>"},{"location":"architecture/serverless-sql/serverless-overview/#next-steps","title":"Next Steps","text":"<ol> <li>Shared Metadata Architecture</li> <li>Best Practices</li> <li>Code Examples</li> <li>Security Guide</li> </ol>"},{"location":"architecture/shared-metadata/","title":"Index","text":"<p>Home &gt; Architecture &gt; Shared Metadata</p>"},{"location":"architecture/shared-metadata/#azure-synapse-analytics-shared-metadata","title":"Azure Synapse Analytics Shared Metadata","text":"<p>Azure Synapse Analytics provides a powerful shared metadata architecture that enables seamless integration between different compute engines, including Apache Spark pools and serverless SQL pools. This section provides in-depth documentation on the shared metadata capabilities, architecture, and best practices.</p>"},{"location":"architecture/shared-metadata/#documentation","title":"Documentation","text":"<ul> <li>Shared Metadata Architecture Overview - Comprehensive guide to the shared metadata architecture, including key components, security model, and best practices.</li> <li>Visual Guides and Diagrams - Visual representations of serverless replicated databases, three-part naming concepts, and layered data architecture.</li> <li>Code Examples - Detailed code samples for implementing shared metadata patterns in Azure Synapse Analytics.</li> </ul>"},{"location":"architecture/shared-metadata/#key-features","title":"Key Features","text":"<ul> <li>Single metadata store for multiple compute engines</li> <li>Consistent schema definition across Spark and SQL</li> <li>Unified data governance and lineage</li> <li>Streamlined cross-engine workloads</li> <li>Simplified DevOps management</li> </ul>"},{"location":"architecture/shared-metadata/#related-resources","title":"Related Resources","text":"<ul> <li>Best Practices for Metadata Management</li> <li>Code Examples</li> <li>Reference Documentation</li> </ul>"},{"location":"architecture/shared-metadata/shared-metadata-examples/","title":"Azure Synapse Shared Metadata - Code Examples","text":"<p>Home &gt; Architecture &gt; Shared Metadata &gt; Code Examples</p> <p>This document provides practical code examples for implementing the shared metadata architecture in Azure Synapse Analytics, focusing on serverless replicated databases, three-part naming solutions, and best practices for implementing layered architectures.</p>"},{"location":"architecture/shared-metadata/shared-metadata-examples/#creating-and-synchronizing-databases-between-spark-and-serverless-sql","title":"Creating and Synchronizing Databases Between Spark and Serverless SQL","text":""},{"location":"architecture/shared-metadata/shared-metadata-examples/#creating-a-database-and-table-in-spark","title":"Creating a Database and Table in Spark","text":"<pre><code># In a Spark notebook\n# Create a new database\nspark.sql(\"CREATE DATABASE IF NOT EXISTS sales_db\")\n\n# Create a table using Parquet format (will be synchronized to SQL)\nspark.sql(\"\"\"\nCREATE TABLE IF NOT EXISTS sales_db.transactions (\n    transaction_id STRING,\n    customer_id STRING,\n    product_id STRING,\n    quantity INT,\n    price DECIMAL(10,2),\n    transaction_date DATE,\n    store_id STRING\n) USING PARQUET\n\"\"\")\n\n# Insert sample data\nspark.sql(\"\"\"\nINSERT INTO sales_db.transactions VALUES \n('T1001', 'C100', 'P5001', 2, 120.50, '2023-02-15', 'S001'),\n('T1002', 'C102', 'P5002', 1, 89.99, '2023-02-15', 'S002'),\n('T1003', 'C100', 'P5003', 3, 25.25, '2023-02-16', 'S001'),\n('T1004', 'C103', 'P5001', 1, 120.50, '2023-02-16', 'S003')\n\"\"\")\n</code></pre>"},{"location":"architecture/shared-metadata/shared-metadata-examples/#accessing-the-synchronized-table-from-serverless-sql","title":"Accessing the Synchronized Table from Serverless SQL","text":"<pre><code>-- Connect to Serverless SQL Pool\n-- The database has been automatically created and synchronized\n\n-- View available tables in the synchronized database\nUSE sales_db;\nSELECT * FROM sys.tables;\n\n-- Query the synchronized table\n-- Note the \"dbo\" schema - tables appear in the dbo schema in SQL\nSELECT * \nFROM sales_db.dbo.transactions\nWHERE transaction_date = '2023-02-15';\n\n-- Using three-part naming\nSELECT t.customer_id, SUM(t.price * t.quantity) AS total_spent\nFROM sales_db.dbo.transactions t\nGROUP BY t.customer_id\nORDER BY total_spent DESC;\n</code></pre>"},{"location":"architecture/shared-metadata/shared-metadata-examples/#working-with-delta-tables","title":"Working with Delta Tables","text":"<pre><code># In a Spark notebook\n# Create a Delta table (will be synchronized to SQL)\nspark.sql(\"CREATE DATABASE IF NOT EXISTS inventory_db\")\n\nspark.sql(\"\"\"\nCREATE TABLE IF NOT EXISTS inventory_db.product_inventory (\n    product_id STRING,\n    product_name STRING,\n    category STRING,\n    quantity_on_hand INT,\n    reorder_level INT,\n    last_updated TIMESTAMP\n) USING DELTA\n\"\"\")\n\n# Insert sample data\nspark.sql(\"\"\"\nINSERT INTO inventory_db.product_inventory VALUES \n('P5001', 'Premium Widget', 'Widgets', 350, 100, current_timestamp()),\n('P5002', 'Standard Widget', 'Widgets', 500, 150, current_timestamp()),\n('P5003', 'Economy Gadget', 'Gadgets', 275, 75, current_timestamp()),\n('P5004', 'Premium Gadget', 'Gadgets', 120, 50, current_timestamp())\n\"\"\")\n\n# Update data - these changes will be synchronized to SQL\nspark.sql(\"\"\"\nUPDATE inventory_db.product_inventory \nSET quantity_on_hand = 300, last_updated = current_timestamp()\nWHERE product_id = 'P5001'\n\"\"\")\n</code></pre>"},{"location":"architecture/shared-metadata/shared-metadata-examples/#querying-delta-table-from-serverless-sql","title":"Querying Delta Table from Serverless SQL","text":"<pre><code>-- Delta tables are also synchronized (in preview)\nSELECT * FROM inventory_db.dbo.product_inventory\nWHERE category = 'Widgets';\n\n-- Join with the sales data from another database\nSELECT \n    i.product_id,\n    i.product_name,\n    i.quantity_on_hand,\n    SUM(t.quantity) AS quantity_sold,\n    SUM(t.price * t.quantity) AS total_revenue\nFROM inventory_db.dbo.product_inventory i\nJOIN sales_db.dbo.transactions t ON i.product_id = t.product_id\nGROUP BY \n    i.product_id,\n    i.product_name,\n    i.quantity_on_hand\nORDER BY total_revenue DESC;\n</code></pre>"},{"location":"architecture/shared-metadata/shared-metadata-examples/#managing-schema-evolution","title":"Managing Schema Evolution","text":""},{"location":"architecture/shared-metadata/shared-metadata-examples/#adding-columns-in-spark-synchronized-to-sql","title":"Adding Columns in Spark (Synchronized to SQL)","text":"<pre><code># Add a column to existing table in Spark\nspark.sql(\"\"\"\nALTER TABLE sales_db.transactions \nADD COLUMN payment_method STRING\n\"\"\")\n\n# Update the new column\nspark.sql(\"\"\"\nUPDATE sales_db.transactions\nSET payment_method = \n    CASE \n        WHEN transaction_id = 'T1001' THEN 'Credit Card'\n        WHEN transaction_id = 'T1002' THEN 'Cash'\n        WHEN transaction_id = 'T1003' THEN 'Credit Card'\n        WHEN transaction_id = 'T1004' THEN 'Digital Wallet'\n    END\n\"\"\")\n</code></pre>"},{"location":"architecture/shared-metadata/shared-metadata-examples/#querying-updated-schema-in-sql","title":"Querying Updated Schema in SQL","text":"<pre><code>-- The new column is now available in SQL after synchronization\nSELECT \n    transaction_id, \n    customer_id,\n    payment_method,\n    price * quantity AS total\nFROM sales_db.dbo.transactions\nWHERE payment_method = 'Credit Card';\n</code></pre>"},{"location":"architecture/shared-metadata/shared-metadata-examples/#implementing-layered-architecture-with-shared-metadata","title":"Implementing Layered Architecture with Shared Metadata","text":""},{"location":"architecture/shared-metadata/shared-metadata-examples/#raw-layer-direct-access-minimal-shared-metadata","title":"Raw Layer (Direct Access, Minimal Shared Metadata)","text":"<pre><code># In Spark, create a database for raw data\nspark.sql(\"CREATE DATABASE IF NOT EXISTS raw_data\")\n\n# Create external table pointing to raw data files\nspark.sql(\"\"\"\nCREATE TABLE raw_data.customer_raw (\n    customer_data STRING\n) USING CSV\nLOCATION 'abfss://datalake@storageaccount.dfs.core.windows.net/raw/customers/'\nOPTIONS (header 'true', inferSchema 'true')\n\"\"\")\n</code></pre>"},{"location":"architecture/shared-metadata/shared-metadata-examples/#silver-layer-apply-shared-metadata","title":"Silver Layer (Apply Shared Metadata)","text":"<pre><code># Create curated database for transformed/validated data\nspark.sql(\"CREATE DATABASE IF NOT EXISTS silver_db\")\n\n# Transform raw data into structured format with proper data types\nspark.sql(\"\"\"\nCREATE TABLE silver_db.customers\nUSING PARQUET\nAS\nSELECT \n    from_json(customer_data, 'id STRING, name STRING, email STRING, signup_date DATE') AS customer\nFROM raw_data.customer_raw\n\"\"\")\n\n# Flatten the structure for easier access\nspark.sql(\"\"\"\nCREATE TABLE silver_db.customers_flattened\nUSING PARQUET\nAS\nSELECT \n    customer.id AS customer_id,\n    customer.name AS full_name,\n    customer.email,\n    customer.signup_date\nFROM silver_db.customers\n\"\"\")\n</code></pre>"},{"location":"architecture/shared-metadata/shared-metadata-examples/#gold-layer-business-ready-data-with-full-shared-metadata","title":"Gold Layer (Business-Ready Data with Full Shared Metadata)","text":"<pre><code># Create business-ready database\nspark.sql(\"CREATE DATABASE IF NOT EXISTS gold_db\")\n\n# Create business metrics table that joins data from multiple sources\nspark.sql(\"\"\"\nCREATE TABLE gold_db.customer_sales_summary\nUSING DELTA\nAS\nSELECT \n    c.customer_id,\n    c.full_name,\n    c.email,\n    COUNT(t.transaction_id) AS total_transactions,\n    SUM(t.price * t.quantity) AS total_spent,\n    MAX(t.transaction_date) AS last_purchase_date\nFROM silver_db.customers_flattened c\nLEFT JOIN sales_db.transactions t ON c.customer_id = t.customer_id\nGROUP BY c.customer_id, c.full_name, c.email\n\"\"\")\n</code></pre>"},{"location":"architecture/shared-metadata/shared-metadata-examples/#accessing-the-layered-architecture-from-sql","title":"Accessing the Layered Architecture from SQL","text":"<pre><code>-- Raw layer is typically accessed directly in Spark, not via shared metadata\n\n-- Silver layer via shared metadata\nSELECT * FROM silver_db.dbo.customers_flattened\nWHERE signup_date &gt;= '2023-01-01';\n\n-- Gold layer via shared metadata\nSELECT \n    customer_id,\n    full_name,\n    total_transactions,\n    total_spent,\n    CASE \n        WHEN total_spent &gt; 1000 THEN 'Premium'\n        WHEN total_spent &gt; 500 THEN 'Standard'\n        ELSE 'Basic'\n    END AS customer_tier\nFROM gold_db.dbo.customer_sales_summary\nORDER BY total_spent DESC;\n</code></pre>"},{"location":"architecture/shared-metadata/shared-metadata-examples/#working-with-manual-external-tables-for-non-synchronized-formats","title":"Working with Manual External Tables for Non-Synchronized Formats","text":"<p>If you have data in formats not automatically synchronized from Spark (like JSON, ORC), you can manually create external tables in serverless SQL:</p> <pre><code>-- Create an external data source if you don't have one\nCREATE DATABASE scoped CREDENTIAL [SasCredential]\nWITH IDENTITY='SHARED ACCESS SIGNATURE',\nSECRET = 'sv=2020-02-10&amp;ss=bfqt&amp;srt=sco&amp;sp=rwdlacupx&amp;se=2023-04-15T01:15:28Z&amp;st=2021-03-15T17:15:28Z&amp;spr=https&amp;sig=XXXXX'\n\nCREATE EXTERNAL DATA SOURCE ExternalDataSource\nWITH (\n    LOCATION = 'https://storageaccount.dfs.core.windows.net/datalake',\n    CREDENTIAL = [SasCredential]\n)\n\n-- Create an external file format for JSON\nCREATE EXTERNAL FILE FORMAT JsonFormat\nWITH (\n    FORMAT_TYPE = JSON\n)\n\n-- Create an external table for JSON data\nCREATE EXTERNAL TABLE external_db.customer_preferences (\n    customer_id VARCHAR(50),\n    preferences NVARCHAR(MAX)\n)\nWITH (\n    LOCATION = '/analytics/customer-preferences/',\n    DATA_SOURCE = ExternalDataSource,\n    FILE_FORMAT = JsonFormat\n)\n</code></pre>"},{"location":"architecture/shared-metadata/shared-metadata-examples/#monitoring-metadata-synchronization","title":"Monitoring Metadata Synchronization","text":"<pre><code>-- Check if tables have been synchronized correctly\nUSE sales_db;\nSELECT \n    name, \n    create_date, \n    modify_date,\n    type_desc\nFROM sys.tables;\n\n-- Check external data sources\nSELECT * FROM sys.external_data_sources;\n\n-- Check external file formats\nSELECT * FROM sys.external_file_formats;\n\n-- View column definitions\nSELECT \n    t.name AS table_name,\n    c.name AS column_name,\n    ty.name AS data_type,\n    c.max_length,\n    c.precision,\n    c.scale,\n    c.is_nullable\nFROM sys.tables t\nJOIN sys.columns c ON t.object_id = c.object_id\nJOIN sys.types ty ON c.user_type_id = ty.user_type_id\nWHERE t.name = 'transactions'\nORDER BY c.column_id;\n</code></pre> <p>These code examples demonstrate how to implement the shared metadata architecture in Azure Synapse Analytics using best practices for serverless replicated databases, handling three-part naming, and implementing a layered data architecture.</p>"},{"location":"architecture/shared-metadata/shared-metadata-visuals/","title":"Azure Synapse Shared Metadata Architecture - Visual Guides","text":"<p>Home &gt; Architecture &gt; Shared Metadata &gt; Visual Guides</p>"},{"location":"architecture/shared-metadata/shared-metadata-visuals/#serverless-replicated-database-synchronization","title":"Serverless Replicated Database Synchronization","text":"<pre><code>sequenceDiagram\n    participant Spark as Apache Spark Pool\n    participant MetaStore as Spark Metastore\n    participant SyncService as Metadata Sync Service\n    participant SQLPool as Serverless SQL Pool\n\n    Spark-&gt;&gt;MetaStore: Create database (mydb)\n    MetaStore--&gt;&gt;Spark: Database created\n    Note over MetaStore: Database stored in metastore\n    MetaStore-&gt;&gt;SyncService: Notify new database\n    SyncService-&gt;&gt;SQLPool: Create corresponding database\n    Note over SQLPool: Database 'mydb' created in SQL Pool\n\n    Spark-&gt;&gt;MetaStore: Create table (mydb.mytable) USING PARQUET\n    MetaStore--&gt;&gt;Spark: Table created\n    Note over MetaStore: Table metadata stored in metastore\n    MetaStore-&gt;&gt;SyncService: Notify new table\n    SyncService-&gt;&gt;SQLPool: Create external table\n    Note over SQLPool: External table 'mydb.dbo.mytable' created\n\n    Spark-&gt;&gt;MetaStore: Update table schema\n    MetaStore--&gt;&gt;Spark: Schema updated\n    MetaStore-&gt;&gt;SyncService: Notify schema change\n    SyncService-&gt;&gt;SQLPool: Update external table schema\n    Note over SQLPool: External table schema updated (async)\n</code></pre>"},{"location":"architecture/shared-metadata/shared-metadata-visuals/#three-part-naming-limitations-and-workarounds","title":"Three-Part Naming Limitations and Workarounds","text":"<pre><code>graph TD\n    subgraph \"Spark Environment\"\n        SparkDB1[Spark Database 1]\n        SparkDB2[Spark Database 2]\n        SparkTable1[Table 1 - Parquet]\n        SparkTable2[Table 2 - Delta]\n        SparkTable3[Table 3 - CSV]\n        SparkTable4[Table 4 - JSON]\n        SparkView[Spark View]\n\n        SparkDB1 --&gt; SparkTable1\n        SparkDB1 --&gt; SparkTable2\n        SparkDB2 --&gt; SparkTable3\n        SparkDB2 --&gt; SparkTable4\n        SparkDB1 --&gt; SparkView\n    end\n\n    subgraph \"Serverless SQL Environment\"\n        SQLDB1[SQL Database 1]\n        SQLDB2[SQL Database 2]\n        SQLTable1[External Table 1]\n        SQLTable2[External Table 2]\n        SQLTable3[External Table 3]\n\n        SQLDB1 --&gt; SQLTable1\n        SQLDB1 --&gt; SQLTable2\n        SQLDB2 --&gt; SQLTable3\n    end\n\n    %% Sync relationships\n    SparkTable1 -.Sync.-&gt; SQLTable1\n    SparkTable2 -.Sync.-&gt; SQLTable2\n    SparkTable3 -.Sync.-&gt; SQLTable3\n\n    %% Not synced\n    SparkTable4 -. Not Synced .-&gt; Limitations([JSON format not supported])\n    SparkView -. Not Synced .-&gt; Limitations2([Views require Spark engine])\n\n    %% Three-part naming\n    ThreePart[Three-part naming: database.schema.table]\n    ThreePart --&gt; SQLAccess[SQL: USE database; SELECT * FROM schema.table]\n    ThreePart --&gt; SparkAccess[Spark: SELECT * FROM database.table]\n\n    %% Workaround\n    Workaround[Workaround Pattern]\n    Workaround --&gt; Step1[1. Create tables in Spark with supported formats]\n    Step1 --&gt; Step2[2. Let tables sync to serverless SQL]\n    Step2 --&gt; Step3[3. Use database.dbo.table in SQL queries]\n</code></pre>"},{"location":"architecture/shared-metadata/shared-metadata-visuals/#layered-data-architecture-with-shared-metadata","title":"Layered Data Architecture with Shared Metadata","text":"<pre><code>graph TD\n    Storage[ADLS Gen2 Storage]\n\n    subgraph \"Data Layers\"\n        Raw[Raw Layer]\n        Silver[Silver/Curated Layer]\n        Gold[Gold/Business Layer]\n\n        Storage --&gt; Raw\n        Raw --&gt; Silver\n        Silver --&gt; Gold\n    end\n\n    subgraph \"Engine Access Patterns\"\n        SparkRaw[Direct Spark access]\n        SQLRaw[Direct SQL access]\n\n        SparkSilver[Spark with metadata sync]\n        SQLSilver[SQL with metadata sync]\n\n        SparkGold[Full shared metadata]\n        SQLGold[Full shared metadata]\n\n        Raw --&gt; SparkRaw\n        Raw --&gt; SQLRaw\n\n        Silver --&gt; SparkSilver\n        Silver --&gt; SQLSilver\n\n        Gold --&gt; SparkGold\n        Gold --&gt; SQLGold\n    end\n\n    subgraph \"Recommendations\"\n        RecRaw[Minimal shared metadata]\n        RecSilver[Begin applying shared metadata]\n        RecGold[Fully leverage shared metadata]\n\n        SparkRaw --&gt; RecRaw\n        SQLRaw --&gt; RecRaw\n\n        SparkSilver --&gt; RecSilver\n        SQLSilver --&gt; RecSilver\n\n        SparkGold --&gt; RecGold\n        SQLGold --&gt; RecGold\n    end\n</code></pre>"},{"location":"architecture/shared-metadata/shared-metadata-visuals/#creating-and-accessing-synchronized-tables-process-flow","title":"Creating and Accessing Synchronized Tables - Process Flow","text":"<pre><code>flowchart TD\n    Start([Start]) --&gt; CreateDB[Create Database in Spark]\n    CreateDB --&gt; CreateTable[Create Table in Spark using Parquet/Delta/CSV]\n    CreateTable --&gt; InsertData[Insert Data in Spark]\n    InsertData --&gt; Wait[Wait for Async Sync ~few seconds]\n    Wait --&gt; QuerySQL[Query from Serverless SQL Pool]\n\n    QuerySQL --&gt; UseCase1[Use Case: Analytics]\n    QuerySQL --&gt; UseCase2[Use Case: Reporting]\n    QuerySQL --&gt; UseCase3[Use Case: Data Exploration]\n\n    subgraph \"Code Examples\"\n        SparkCode[\"\n        // Spark SQL\n        CREATE DATABASE mydb;\n\n        CREATE TABLE mydb.sales (\n            id INT,\n            date DATE,\n            amount DOUBLE,\n            customer STRING\n        ) USING PARQUET;\n\n        INSERT INTO mydb.sales \n        VALUES (1, '2023-01-15', 199.99, 'Contoso');\n        \"]\n\n        SQLCode[\"\n        -- Serverless SQL Pool\n        USE mydb;\n\n        -- View available tables\n        SELECT * FROM sys.tables;\n\n        -- Query synchronized table\n        SELECT * FROM mydb.dbo.sales\n        WHERE date &gt;= '2023-01-01';\n        \"]\n    end\n\n    UseCase1 --&gt; SparkCode\n    UseCase2 --&gt; SQLCode\n</code></pre>"},{"location":"architecture/shared-metadata/shared-metadata/","title":"Shared metadata","text":"<p>Home &gt; Architecture &gt; Shared Metadata Architecture</p>"},{"location":"architecture/shared-metadata/shared-metadata/#azure-synapse-shared-metadata-architecture","title":"Azure Synapse Shared Metadata Architecture","text":""},{"location":"architecture/shared-metadata/shared-metadata/#overview","title":"Overview","text":"<p>The shared metadata architecture in Azure Synapse Analytics enables seamless integration between different compute engines while maintaining a single source of truth for your data. This architecture is crucial for maintaining consistency across your analytics environment and supports the modern data warehouse pattern by allowing different processing engines to collaborate efficiently.</p>"},{"location":"architecture/shared-metadata/shared-metadata/#key-components","title":"Key Components","text":""},{"location":"architecture/shared-metadata/shared-metadata/#1-unified-metadata-layer","title":"1. Unified Metadata Layer","text":"<ul> <li>Single metadata store</li> <li>Consistent schema across engines</li> <li>Centralized security</li> <li>Version control</li> </ul>"},{"location":"architecture/shared-metadata/shared-metadata/#2-metadata-synchronization","title":"2. Metadata Synchronization","text":"<ul> <li>Automatic synchronization</li> <li>Schema evolution tracking</li> <li>Data lineage</li> <li>Impact analysis</li> </ul>"},{"location":"architecture/shared-metadata/shared-metadata/#3-security-integration","title":"3. Security Integration","text":"<ul> <li>Unified access control</li> <li>Row-level security</li> <li>Column-level security</li> <li>Audit logging</li> </ul>"},{"location":"architecture/shared-metadata/shared-metadata/#architecture-diagram","title":"Architecture Diagram","text":"<pre><code>graph TD\n    Metadata[Metadata Store] --&gt; Serverless[Serverless SQL]\n    Metadata --&gt; Spark[Spark Pool]\n    Metadata --&gt; Dedicated[Dedicated SQL]\n\n    subgraph \"Metadata Services\"\n        Stats[Statistics]\n        Lineage[Data Lineage]\n        Security[Security]\n    end\n\n    subgraph \"Compute Engines\"\n        Serverless --&gt; Metadata\n        Spark --&gt; Metadata\n        Dedicated --&gt; Metadata\n    end\n\n    subgraph \"Storage\"\n        ADLS[ADLS Gen2]\n        Delta[Delta Lake]\n    end\n</code></pre>"},{"location":"architecture/shared-metadata/shared-metadata/#best-practices","title":"Best Practices","text":""},{"location":"architecture/shared-metadata/shared-metadata/#schema-management","title":"Schema Management","text":"<ul> <li>Use consistent naming conventions</li> <li>Implement proper schema evolution</li> <li>Regularly update statistics</li> <li>Use appropriate data types</li> </ul>"},{"location":"architecture/shared-metadata/shared-metadata/#security","title":"Security","text":"<ul> <li>Implement proper RBAC</li> <li>Use row-level security</li> <li>Regularly audit changes</li> <li>Use secure connection strings</li> </ul>"},{"location":"architecture/shared-metadata/shared-metadata/#performance","title":"Performance","text":"<ul> <li>Use appropriate partitioning</li> <li>Implement proper indexing</li> <li>Use query hints when needed</li> <li>Regularly update statistics</li> </ul>"},{"location":"architecture/shared-metadata/shared-metadata/#code-examples","title":"Code Examples","text":""},{"location":"architecture/shared-metadata/shared-metadata/#creating-a-table-with-shared-metadata","title":"Creating a Table with Shared Metadata","text":"<pre><code>CREATE TABLE my_table\nWITH (\n    LOCATION = 'abfss://container@storageaccount.dfs.core.windows.net/path',\n    DATA_SOURCE = my_datasource,\n    FILE_FORMAT = parquet_format\n)\nAS SELECT * FROM source_table\n</code></pre>"},{"location":"architecture/shared-metadata/shared-metadata/#schema-evolution","title":"Schema Evolution","text":"<pre><code>-- Add column\nALTER TABLE my_table ADD COLUMNS (new_column INT)\n\n-- Rename column\nALTER TABLE my_table RENAME COLUMN old_name TO new_name\n\n-- Drop column\nALTER TABLE my_table DROP COLUMN column_name\n</code></pre>"},{"location":"architecture/shared-metadata/shared-metadata/#security-management","title":"Security Management","text":"<pre><code>-- Grant permissions\nGRANT SELECT ON my_table TO [user]\n\n-- Row-level security\nCREATE SECURITY POLICY my_policy\nADD FILTER PREDICATE my_function(user_id)\nON my_table\n</code></pre>"},{"location":"architecture/shared-metadata/shared-metadata/#serverless-replicated-databases","title":"Serverless Replicated Databases","text":"<p>Serverless replicated databases are a key feature of the shared metadata architecture in Azure Synapse Analytics. These databases are created automatically in the serverless SQL pool when corresponding databases are created in Spark pools.</p>"},{"location":"architecture/shared-metadata/shared-metadata/#how-serverless-replicated-databases-work","title":"How Serverless Replicated Databases Work","text":"<ol> <li>When a database is created in a Spark pool, a corresponding database is automatically created in the serverless SQL pool.</li> <li>Tables created in Spark using Parquet, Delta Lake, or CSV formats are exposed as external tables in the serverless SQL pool.</li> <li>The synchronization happens asynchronously, typically with a delay of a few seconds.</li> <li>Tables appear in the <code>dbo</code> schema of the corresponding database in the serverless SQL pool.</li> <li>The maximum number of databases synchronized from Apache Spark pools is not limited, but serverless SQL pools can have up to 100 additional (non-synchronized) databases.</li> </ol>"},{"location":"architecture/shared-metadata/shared-metadata/#limitations-of-serverless-replicated-databases","title":"Limitations of Serverless Replicated Databases","text":"<ul> <li>Read-Only Access: Replicated databases in serverless SQL are read-only due to the asynchronous nature of metadata synchronization from Spark.</li> <li>Format Restrictions: Only tables using Parquet, Delta Lake (preview), or CSV formats are synchronized; other formats are not automatically available.</li> <li>Asynchronous Updates: Changes in Spark metadata are propagated to SQL with a short delay.</li> <li>Spark Views: Spark views require a Spark engine to process and cannot be accessed from SQL engines.</li> </ul>"},{"location":"architecture/shared-metadata/shared-metadata/#three-part-naming-support-and-limitations","title":"Three-Part Naming Support and Limitations","text":"<p>Three-part naming (database.schema.table) is an important feature for cross-database queries but has specific limitations in both Spark and serverless SQL environments.</p>"},{"location":"architecture/shared-metadata/shared-metadata/#three-part-naming-in-serverless-sql","title":"Three-Part Naming in Serverless SQL","text":"<ul> <li>Serverless SQL pools support three-part name references and cross-database queries, including the <code>USE</code> statement.</li> <li>Queries can reference serverless SQL databases or Lake databases (replicated from Spark) within the same workspace.</li> <li>Cross-workspace queries are not supported.</li> </ul>"},{"location":"architecture/shared-metadata/shared-metadata/#three-part-naming-in-spark","title":"Three-Part Naming in Spark","text":"<ul> <li>Spark has more restrictive three-part naming support compared to traditional SQL environments.</li> <li>In Spark, databases and schemas are treated as the same concept, which limits the traditional three-part naming convention.</li> <li>Using fully qualified names (database.schema.table) may not work as expected in certain Spark operations.</li> </ul>"},{"location":"architecture/shared-metadata/shared-metadata/#workarounds-for-three-part-naming-limitations","title":"Workarounds for Three-Part Naming Limitations","text":""},{"location":"architecture/shared-metadata/shared-metadata/#managed-serverless-databases-with-spark-schema-synchronization","title":"Managed Serverless Databases with Spark Schema Synchronization","text":"<p>A recommended pattern to overcome three-part naming limitations is to create managed serverless databases that leverage Spark database schemas:</p> <ol> <li>Create and design tables in Spark using appropriate formats (Parquet, Delta, CSV).</li> <li>Let these tables automatically synchronize to serverless SQL.</li> <li>Use schema isolation in Spark (which treats schemas the same as databases) for logical separation.</li> <li>Access the synchronized tables in serverless SQL using database.dbo.table naming convention.</li> </ol>"},{"location":"architecture/shared-metadata/shared-metadata/#auto-creation-of-external-tables-in-serverless-sql","title":"Auto-Creation of External Tables in Serverless SQL","text":"<p>To maintain schema synchronization between Spark and serverless SQL:</p> <ol> <li>Define your schema and tables in Spark pools using supported formats.</li> <li>Allow automatic synchronization to create corresponding external tables in serverless SQL.</li> <li>For formats not automatically synchronized, create manual external tables in serverless SQL pointing to the same underlying storage.</li> </ol> <pre><code>-- Example: Creating a table in Spark that will sync to serverless SQL\n-- In Spark:\nCREATE TABLE mydb.mytable (id INT, name STRING, data STRING) USING PARQUET\n\n-- After sync, access in serverless SQL:\nSELECT * FROM mydb.dbo.mytable\n</code></pre>"},{"location":"architecture/shared-metadata/shared-metadata/#best-practices-for-layered-data-architecture","title":"Best Practices for Layered Data Architecture","text":"<ul> <li>Raw Data Layer: Minimal processing, typically accessed directly through specific engines without relying on shared metadata.</li> <li>Silver Layer (Curated): Apply three-part naming and schema synchronization here for clean, transformed data.</li> <li>Gold Layer (Business): Fully leverage synchronized metadata for business-ready data models across engines.</li> </ul> <p>This layered approach ensures that metadata synchronization complexity is applied where it adds the most value, rather than in raw data layers where direct access patterns may be more efficient.</p>"},{"location":"architecture/shared-metadata/shared-metadata/#next-steps","title":"Next Steps","text":"<ol> <li>Delta Lakehouse Architecture</li> <li>Serverless SQL Architecture</li> <li>Best Practices</li> <li>Code Examples</li> </ol>"},{"location":"assets/images/","title":"Image Assets for Azure Synapse Analytics Documentation","text":"<p>This directory contains diagrams and images for the Azure Synapse Analytics documentation.</p>"},{"location":"assets/images/#required-diagrams","title":"Required Diagrams","text":"<p>The following diagrams are referenced in the documentation and should be created:</p> <ol> <li>delta-lakehouse-diagram.png - Architecture diagram for the Delta Lakehouse pattern</li> <li>serverless-sql-architecture.png - Architecture diagram for the Serverless SQL pattern</li> <li>shared-metadata-architecture.png - Architecture diagram for the Shared Metadata pattern</li> <li>microsoft-logo.png - Microsoft logo for the documentation header</li> <li>favicon.ico - Favicon for the documentation site</li> </ol>"},{"location":"assets/images/#design-guidelines","title":"Design Guidelines","text":"<p>When creating these diagrams, please follow these guidelines:</p> <ul> <li>Use the Azure architecture icons from the Azure Architecture Center</li> <li>Use a consistent color palette aligned with Azure branding</li> <li>Include clear labels for all components</li> <li>Add directional arrows to show data flow</li> <li>Include a legend if needed for complex diagrams</li> <li>Export as PNG with transparent background</li> <li>Target resolution: 1280x720 pixels (16:9 aspect ratio)</li> </ul>"},{"location":"assets/images/#tools","title":"Tools","text":"<p>Recommended tools for creating these diagrams:</p> <ul> <li>Draw.io with the Azure icon set</li> <li>PowerPoint with the Azure Architecture templates</li> <li>Figma with Azure UI kits</li> </ul>"},{"location":"assets/images/#examples","title":"Examples","text":"<p>For examples of well-designed Azure architecture diagrams, refer to:</p> <ul> <li>Azure Architecture Center</li> <li>Azure Synapse Analytics documentation</li> </ul>"},{"location":"best-practices/cost-optimization/","title":"Cost Optimization","text":"<p>Home &gt; Best Practices &gt; Cost Optimization</p>"},{"location":"best-practices/cost-optimization/#cost-optimization-best-practices-for-azure-synapse-analytics","title":"Cost Optimization Best Practices for Azure Synapse Analytics","text":""},{"location":"best-practices/cost-optimization/#understanding-synapse-analytics-cost-model","title":"Understanding Synapse Analytics Cost Model","text":""},{"location":"best-practices/cost-optimization/#cost-components","title":"Cost Components","text":""},{"location":"best-practices/cost-optimization/#serverless-sql-pool-costs","title":"Serverless SQL Pool Costs","text":"<ul> <li>Data Scanning: Charged per TB of data processed</li> <li>Result Caching: No charge for subsequent queries using cached results</li> <li>Resource Management: No charge when idle (pay-per-query model)</li> </ul>"},{"location":"best-practices/cost-optimization/#dedicated-sql-pool-costs","title":"Dedicated SQL Pool Costs","text":"<ul> <li>Compute Costs: Based on Data Warehouse Units (DWU) and time running</li> <li>Storage Costs: Based on volume of data stored in the dedicated pool</li> <li>Data Movement: Included in compute costs</li> </ul>"},{"location":"best-practices/cost-optimization/#spark-pool-costs","title":"Spark Pool Costs","text":"<ul> <li>Compute Costs: Based on vCore-hours consumed</li> <li>Autoscale Impact: Costs vary based on actual usage with autoscale</li> <li>Node Types: Different costs for different node types (memory-optimized vs. compute-optimized)</li> </ul>"},{"location":"best-practices/cost-optimization/#storage-costs","title":"Storage Costs","text":"<ul> <li>ADLS Gen2 Storage: Based on volume of data and storage tier (hot/cool/archive)</li> <li>Transaction Costs: Based on number and type of storage operations</li> </ul>"},{"location":"best-practices/cost-optimization/#compute-optimization-strategies","title":"Compute Optimization Strategies","text":""},{"location":"best-practices/cost-optimization/#serverless-sql-pool-optimization","title":"Serverless SQL Pool Optimization","text":""},{"location":"best-practices/cost-optimization/#query-optimization","title":"Query Optimization","text":"<ul> <li> <p>Minimize Data Scanning: Prune data aggressively   <pre><code>-- Good: Scans less data with partition filtering\nSELECT * FROM external_table\nWHERE year_partition = 2025 AND month_partition = 1\n\n-- Avoid: Full scan across all partitions\nSELECT * FROM external_table\nWHERE YEAR(transaction_date) = 2025 AND MONTH(transaction_date) = 1\n</code></pre></p> </li> <li> <p>Use Appropriate File Formats: Prefer columnar formats (Parquet, ORC) over row-based formats (CSV, JSON)   <pre><code>-- Create external file format for Parquet\nCREATE EXTERNAL FILE FORMAT ParquetFormat\nWITH (\n    FORMAT_TYPE = PARQUET,\n    DATA_COMPRESSION = 'SNAPPY'\n);\n</code></pre></p> </li> <li> <p>Statistics: Create statistics on frequently filtered columns   <pre><code>-- Create statistics for better query plans\nCREATE STATISTICS stats_year ON external_table(year_column);\n</code></pre></p> </li> </ul>"},{"location":"best-practices/cost-optimization/#result-set-caching","title":"Result Set Caching","text":"<ul> <li> <p>Enable Result Set Caching: Reuse query results for identical queries   <pre><code>-- Enable result set caching at database level\nALTER DATABASE MyDatabase\nSET RESULT_SET_CACHING ON;\n</code></pre></p> </li> <li> <p>Parameterize Queries: Use parameterized queries to maximize cache hits</p> </li> </ul>"},{"location":"best-practices/cost-optimization/#dedicated-sql-pool-optimization","title":"Dedicated SQL Pool Optimization","text":""},{"location":"best-practices/cost-optimization/#scale-management","title":"Scale Management","text":"<ul> <li> <p>Implement Automated Scaling: Scale up/down based on workload patterns   <pre><code># Scale DW based on schedule\n$startTime = (Get-Date).AddHours(1)\n$timeZone = [System.TimeZoneInfo]::Local.Id\n$schedule = New-AzSynapseWorkspaceManagedSchedule -DayOfWeek Monday, Tuesday, Wednesday, Thursday, Friday -Time \"08:00\" -TimeZone $timeZone\nNew-AzSynapseSqlPoolWorkloadManagement -WorkspaceName $workspaceName -SqlPoolName $sqlPoolName -DwuValue 1000 -Schedule $schedule\n</code></pre></p> </li> <li> <p>Pause During Inactivity: Automatically pause during non-business hours   <pre><code># Pause SQL pool\nSuspend-AzSynapseSqlPool -WorkspaceName $workspaceName -Name $sqlPoolName\n</code></pre></p> </li> </ul>"},{"location":"best-practices/cost-optimization/#resource-classes","title":"Resource Classes","text":"<ul> <li>Optimize Resource Classes: Use smaller resource classes for simple queries   <pre><code>-- Assign smaller resource class for simple queries\nEXEC sp_addrolemember 'smallrc', 'username';\n\n-- Assign larger resource class for complex queries\nEXEC sp_addrolemember 'largerc', 'username';\n</code></pre></li> </ul>"},{"location":"best-practices/cost-optimization/#spark-pool-optimization","title":"Spark Pool Optimization","text":""},{"location":"best-practices/cost-optimization/#autoscale-configuration","title":"Autoscale Configuration","text":"<ul> <li> <p>Right-Size Min/Max Nodes: Configure appropriate autoscale range   <pre><code>{\n  \"name\": \"optimizedSparkPool\",\n  \"properties\": {\n    \"nodeSize\": \"Small\",\n    \"nodeSizeFamily\": \"MemoryOptimized\",\n    \"autoScale\": {\n      \"enabled\": true,\n      \"minNodeCount\": 3,\n      \"maxNodeCount\": 10\n    }\n  }\n}\n</code></pre></p> </li> <li> <p>Session-Level Configuration: Only request resources needed for each job   <pre><code># Configure Spark session with appropriate resources\nspark.conf.set(\"spark.executor.instances\", \"4\")\nspark.conf.set(\"spark.executor.memory\", \"4g\")\nspark.conf.set(\"spark.executor.cores\", \"2\")\n</code></pre></p> </li> </ul>"},{"location":"best-practices/cost-optimization/#node-selection","title":"Node Selection","text":"<ul> <li>Use Appropriate Node Types: Select based on workload characteristics</li> <li>Memory-optimized for ML and large joins</li> <li> <p>Compute-optimized for ETL and data processing</p> </li> <li> <p>Consider Job Requirements: Match node size to job requirements</p> </li> </ul>"},{"location":"best-practices/cost-optimization/#session-management","title":"Session Management","text":"<ul> <li>Session Timeout: Configure appropriate timeout to release resources   <pre><code>{\n  \"name\": \"optimizedSparkPool\",\n  \"properties\": {\n    \"sessionLevelPackages\": [],\n    \"sparkConfigProperties\": {},\n    \"nodeSize\": \"Small\",\n    \"nodeSizeFamily\": \"MemoryOptimized\",\n    \"sessionLevelPackages\": [],\n    \"customLibraries\": [],\n    \"sparkEventsFolder\": \"/events\",\n    \"autoScale\": {\n      \"enabled\": true,\n      \"minNodeCount\": 3,\n      \"maxNodeCount\": 10\n    },\n    \"isComputeIsolationEnabled\": false,\n    \"sessionProperties\": {\n      \"driverSize\": \"Small\",\n      \"executorSize\": \"Small\",\n      \"executorCount\": 2\n    },\n    \"defaultSparkLogFolder\": \"/logs\",\n    \"nodeCount\": 0,\n    \"dynamicExecutorAllocation\": {\n      \"enabled\": true,\n      \"minExecutors\": 1,\n      \"maxExecutors\": 5\n    },\n    \"coordinatorSize\": \"Small\",\n    \"provisioningState\": \"Succeeded\"\n  }\n}\n</code></pre></li> </ul>"},{"location":"best-practices/cost-optimization/#storage-optimization-strategies","title":"Storage Optimization Strategies","text":""},{"location":"best-practices/cost-optimization/#data-lifecycle-management","title":"Data Lifecycle Management","text":""},{"location":"best-practices/cost-optimization/#storage-tiering","title":"Storage Tiering","text":"<ul> <li>Hot Storage: Use for frequently accessed data (last 30-90 days)</li> <li>Cool Storage: Use for infrequently accessed data (older than 90 days)</li> <li>Archive Storage: Use for rarely accessed data (compliance/historical)</li> </ul>"},{"location":"best-practices/cost-optimization/#automated-tiering","title":"Automated Tiering","text":"<ul> <li>Lifecycle Management Policies: Configure to automatically move data between tiers   <pre><code>{\n  \"rules\": [\n    {\n      \"enabled\": true,\n      \"name\": \"MoveToCoolTier\",\n      \"type\": \"Lifecycle\",\n      \"definition\": {\n        \"filters\": {\n          \"blobTypes\": [ \"blockBlob\" ],\n          \"prefixMatch\": [ \"data/historical/\" ]\n        },\n        \"actions\": {\n          \"baseBlob\": {\n            \"tierToCool\": { \"daysAfterModificationGreaterThan\": 90 }\n          }\n        }\n      }\n    }\n  ]\n}\n</code></pre></li> </ul>"},{"location":"best-practices/cost-optimization/#data-storage-optimization","title":"Data Storage Optimization","text":""},{"location":"best-practices/cost-optimization/#compression-and-file-formats","title":"Compression and File Formats","text":"<ul> <li> <p>Use Compression: Prefer columnar formats with compression   <pre><code># Write with compression\ndf.write.format(\"parquet\") \\\n    .option(\"compression\", \"snappy\") \\\n    .save(\"/path/to/data\")\n</code></pre></p> </li> <li> <p>Optimize File Sizes: Target 100MB-1GB per file   <pre><code># Control Parquet file size\nspark.conf.set(\"spark.sql.files.maxPartitionBytes\", 134217728)  # 128 MB\n</code></pre></p> </li> </ul>"},{"location":"best-practices/cost-optimization/#data-cleanup","title":"Data Cleanup","text":"<ul> <li>Remove Duplicate Data: Deduplicate data where possible</li> <li> <p>Regular Vacuum: Clean up stale files in Delta tables   <pre><code>-- Remove files no longer needed by the table\nVACUUM delta_table RETAIN 7 DAYS\n</code></pre></p> </li> <li> <p>Temporary Data Management: Remove temporary datasets after use</p> </li> </ul>"},{"location":"best-practices/cost-optimization/#pipeline-optimization","title":"Pipeline Optimization","text":""},{"location":"best-practices/cost-optimization/#integration-pipeline-costs","title":"Integration Pipeline Costs","text":""},{"location":"best-practices/cost-optimization/#activity-optimization","title":"Activity Optimization","text":"<ul> <li>Combine Activities: Reduce activity runs by combining related operations</li> <li>Use Appropriate Integration Runtime: Match the IR to the workload requirements</li> <li>Optimize Copy Activity: Configure appropriate compute size for data movement</li> </ul>"},{"location":"best-practices/cost-optimization/#monitoring-and-debugging","title":"Monitoring and Debugging","text":"<ul> <li>Limit Debug Runs: Use debug runs sparingly</li> <li>Optimize Logging: Implement appropriate logging levels</li> <li>Use Activity Constraints: Set appropriate timeouts and retry policies</li> </ul>"},{"location":"best-practices/cost-optimization/#orchestration-patterns","title":"Orchestration Patterns","text":""},{"location":"best-practices/cost-optimization/#trigger-optimization","title":"Trigger Optimization","text":"<ul> <li>Batch Related Activities: Trigger multiple related activities together</li> <li>Use Event-Based Triggers: Trigger only when needed, rather than on schedule</li> </ul>"},{"location":"best-practices/cost-optimization/#monitoring-and-analysis","title":"Monitoring and Analysis","text":""},{"location":"best-practices/cost-optimization/#cost-monitoring","title":"Cost Monitoring","text":""},{"location":"best-practices/cost-optimization/#azure-cost-management","title":"Azure Cost Management","text":"<ul> <li> <p>Budget Alerts: Set up alerts for cost thresholds   <pre><code># Create budget with alert\nNew-AzConsumptionBudget -Name \"SynapseMonthlyBudget\" `\n    -Amount 1000 `\n    -Category \"Cost\" `\n    -TimeGrain \"Monthly\" `\n    -StartDate (Get-Date) `\n    -EndDate (Get-Date).AddYears(1) `\n    -ContactEmail @(\"user@contoso.com\")\n</code></pre></p> </li> <li> <p>Cost Analysis: Regularly analyze costs by service, resource, and tag</p> </li> <li>Tag Resources: Implement consistent tagging for cost allocation   <pre><code>{\n  \"tags\": {\n    \"Environment\": \"Production\",\n    \"Department\": \"Finance\",\n    \"Project\": \"DataWarehouse\"\n  }\n}\n</code></pre></li> </ul>"},{"location":"best-practices/cost-optimization/#resource-utilization-analysis","title":"Resource Utilization Analysis","text":"<ul> <li>Monitor Usage Patterns: Track usage to identify optimization opportunities</li> <li>Identify Idle Resources: Find and address underutilized resources</li> <li>Workload Analysis: Understand peak vs. average requirements</li> </ul>"},{"location":"best-practices/cost-optimization/#cost-optimization-workflow","title":"Cost Optimization Workflow","text":""},{"location":"best-practices/cost-optimization/#regular-review-process","title":"Regular Review Process","text":"<ul> <li>Monthly Cost Review: Schedule regular cost review meetings</li> <li>Cost Optimization Backlog: Maintain a backlog of optimization opportunities</li> <li>ROI Analysis: Prioritize optimization efforts by potential savings</li> </ul>"},{"location":"best-practices/cost-optimization/#enterprise-strategies","title":"Enterprise Strategies","text":""},{"location":"best-practices/cost-optimization/#reserved-instances","title":"Reserved Instances","text":""},{"location":"best-practices/cost-optimization/#azure-reservations","title":"Azure Reservations","text":"<ul> <li>Reserved Capacity: Consider 1-year or 3-year reservations for stable workloads</li> <li>Reservation Scope: Choose appropriate scope (subscription or resource group)</li> <li>Mixed Approach: Use reserved instances for baseline and pay-as-you-go for variable workloads</li> </ul>"},{"location":"best-practices/cost-optimization/#enterprise-agreement-benefits","title":"Enterprise Agreement Benefits","text":""},{"location":"best-practices/cost-optimization/#ea-optimization","title":"EA Optimization","text":"<ul> <li>Leverage EA Pricing: Utilize enterprise agreement discounts</li> <li>Azure Hybrid Benefit: Apply for eligible workloads</li> <li>Enterprise Dev/Test Subscription: Use for non-production environments</li> </ul>"},{"location":"best-practices/cost-optimization/#conclusion","title":"Conclusion","text":"<p>Cost optimization in Azure Synapse Analytics requires a multi-faceted approach across compute, storage, and operational aspects. By implementing these best practices, organizations can achieve significant cost savings while maintaining performance and meeting business requirements.</p> <p>Remember that cost optimization is an ongoing process that should be integrated into your regular operational rhythms. Regular monitoring, analysis, and adjustment of your optimization strategies will ensure continued cost efficiency as your workloads evolve.</p>"},{"location":"best-practices/data-governance/","title":"Data Governance Best Practices for Azure Synapse Analytics","text":"<p>Home &gt; Best Practices &gt; Data Governance</p>"},{"location":"best-practices/data-governance/#data-governance-framework","title":"Data Governance Framework","text":""},{"location":"best-practices/data-governance/#core-components","title":"Core Components","text":""},{"location":"best-practices/data-governance/#data-catalog","title":"Data Catalog","text":"<ul> <li> <p>Metadata Management: Implement comprehensive metadata for all data assets</p> </li> <li> <p>Business Glossary: Maintain standardized definitions of business terms</p> </li> <li> <p>Data Dictionary: Document technical metadata including data types, constraints, and relationships</p> </li> </ul>"},{"location":"best-practices/data-governance/#data-quality-framework","title":"Data Quality Framework","text":"<ul> <li>Quality Rules: Define and implement data quality rules</li> </ul> <pre><code># Example quality rule implementation in PySpark\nfrom pyspark.sql.functions import col, when, count\n\n# Check for null values in critical columns\ndef check_null_values(df, column_name):\n    null_count = df.filter(col(column_name).isNull()).count()\n    total_count = df.count()\n    return {\n        \"column\": column_name,\n        \"null_count\": null_count,\n        \"total_count\": total_count,\n        \"null_percentage\": (null_count / total_count) * 100 if total_count &gt; 0 else 0\n    }\n</code></pre> <ul> <li> <p>Validation Frameworks: Implement automated data validation pipelines</p> </li> <li> <p>Quality Metrics: Track and report key quality metrics (completeness, accuracy, consistency)</p> </li> </ul>"},{"location":"best-practices/data-governance/#data-lineage","title":"Data Lineage","text":"<ul> <li> <p>End-to-End Tracking: Record data movement from source to consumption</p> </li> <li> <p>Impact Analysis: Enable analysis of upstream/downstream impacts of changes</p> </li> <li> <p>Audit Trail: Maintain history of data transformations and processing</p> </li> </ul>"},{"location":"best-practices/data-governance/#azure-purview-integration","title":"Azure Purview Integration","text":""},{"location":"best-practices/data-governance/#automated-discovery","title":"Automated Discovery","text":""},{"location":"best-practices/data-governance/#data-estate-scanning","title":"Data Estate Scanning","text":"<ul> <li>Automated Scanning: Schedule regular scans of your data estate</li> </ul> <pre><code>{\n  \"name\": \"Scan-Synapse\",\n  \"properties\": {\n    \"dataSourceName\": \"AzureSynapseDW\",\n    \"scanRulesetName\": \"System_DefaultScanRuleSet\",\n    \"scanRulesetType\": \"System\",\n    \"recurrenceInterval\": \"PT24H\"\n  }\n}\n</code></pre> <ul> <li> <p>Classification Rules: Configure custom classification rules for sensitive data</p> </li> <li> <p>Incremental Scanning: Optimize scan performance with incremental scans</p> </li> </ul>"},{"location":"best-practices/data-governance/#metadata-enrichment","title":"Metadata Enrichment","text":"<ul> <li> <p>Business Attributes: Enrich technical metadata with business context</p> </li> <li> <p>Ownership: Assign data owners and stewards</p> </li> <li> <p>Sensitivity Labels: Apply appropriate sensitivity labels</p> </li> </ul>"},{"location":"best-practices/data-governance/#catalog-and-search","title":"Catalog and Search","text":""},{"location":"best-practices/data-governance/#knowledge-center","title":"Knowledge Center","text":"<ul> <li> <p>Self-Service Discovery: Enable users to search and discover relevant data assets</p> </li> <li> <p>Asset Collections: Organize related data assets into collections</p> </li> <li> <p>Metadata Templates: Create standardized templates for consistent documentation</p> </li> </ul>"},{"location":"best-practices/data-governance/#insights","title":"Insights","text":"<ul> <li> <p>Usage Metrics: Track data asset usage patterns</p> </li> <li> <p>Popularity Metrics: Identify most valuable data assets</p> </li> <li> <p>Expert Identification: Connect users with data domain experts</p> </li> </ul>"},{"location":"best-practices/data-governance/#data-lifecycle-management","title":"Data Lifecycle Management","text":""},{"location":"best-practices/data-governance/#data-retention","title":"Data Retention","text":""},{"location":"best-practices/data-governance/#retention-policies","title":"Retention Policies","text":"<ul> <li>Policy Definition: Define retention requirements based on data type and regulations</li> </ul> <pre><code>-- Example of retention policy in Delta Lake\nALTER TABLE customer_data SET TBLPROPERTIES (\n  'delta.logRetentionDuration' = 'interval 7 years',\n  'delta.deletedFileRetentionDuration' = 'interval 30 days'\n)\n</code></pre> <ul> <li> <p>Automated Enforcement: Implement automated processes for policy enforcement</p> </li> <li> <p>Exceptions Handling: Define process for handling retention exceptions</p> </li> </ul>"},{"location":"best-practices/data-governance/#archiving-strategy","title":"Archiving Strategy","text":"<ul> <li> <p>Tiered Storage: Move data through appropriate storage tiers</p> </li> <li> <p>Preservation Format: Select appropriate formats for long-term preservation</p> </li> <li> <p>Retrieval Mechanisms: Define processes for retrieving archived data</p> </li> </ul>"},{"location":"best-practices/data-governance/#data-disposal","title":"Data Disposal","text":""},{"location":"best-practices/data-governance/#secure-deletion","title":"Secure Deletion","text":"<ul> <li> <p>Hard Delete Processes: Implement processes for complete data removal</p> </li> <li> <p>Verification: Verify successful deletion of data</p> </li> <li> <p>Deletion Certification: Document and certify deletion for compliance</p> </li> </ul>"},{"location":"best-practices/data-governance/#regulatory-compliance","title":"Regulatory Compliance","text":""},{"location":"best-practices/data-governance/#compliance-framework","title":"Compliance Framework","text":""},{"location":"best-practices/data-governance/#data-privacy","title":"Data Privacy","text":"<ul> <li> <p>GDPR Compliance: Implement mechanisms for data subject rights</p> </li> <li> <p>Right to access</p> </li> <li>Right to be forgotten</li> <li>Right to data portability</li> <li> <p>Right to correction</p> </li> <li> <p>PII Handling: Special protections for personally identifiable information</p> </li> <li> <p>Consent Management: Track and respect data usage consent</p> </li> </ul>"},{"location":"best-practices/data-governance/#industry-regulations","title":"Industry Regulations","text":"<ul> <li> <p>Financial Services: Implement controls for regulations like GLBA, SOX</p> </li> <li> <p>Healthcare: Support HIPAA compliance requirements</p> </li> <li> <p>Cross-Industry: Address requirements from regulations like CCPA, PIPEDA</p> </li> </ul>"},{"location":"best-practices/data-governance/#compliance-controls","title":"Compliance Controls","text":""},{"location":"best-practices/data-governance/#data-sovereignty","title":"Data Sovereignty","text":"<ul> <li>Geographic Restrictions: Enforce data residency requirements</li> </ul> <pre><code>{\n  \"location\": \"East US\",\n  \"tags\": {\n    \"DataResidency\": \"US\",\n    \"DataClassification\": \"Confidential\"\n  }\n}\n</code></pre> <ul> <li> <p>Cross-Border Transfers: Implement controls for international data transfers</p> </li> <li> <p>Regional Compliance: Adhere to local data protection laws</p> </li> </ul>"},{"location":"best-practices/data-governance/#audit-controls","title":"Audit Controls","text":"<ul> <li> <p>Comprehensive Logging: Maintain detailed logs of data access and processing</p> </li> <li> <p>Evidence Collection: Automate collection of compliance evidence</p> </li> <li> <p>Reporting: Generate compliance reports for regulators and auditors</p> </li> </ul>"},{"location":"best-practices/data-governance/#data-security-classifications","title":"Data Security Classifications","text":""},{"location":"best-practices/data-governance/#classification-framework","title":"Classification Framework","text":""},{"location":"best-practices/data-governance/#sensitivity-levels","title":"Sensitivity Levels","text":"<ul> <li> <p>Public: Information freely available to anyone</p> </li> <li> <p>Internal: Information for use within the organization only</p> </li> <li> <p>Confidential: Sensitive information with restricted access</p> </li> <li> <p>Restricted: Highly sensitive information with strictly controlled access</p> </li> </ul>"},{"location":"best-practices/data-governance/#implementation","title":"Implementation","text":"<ul> <li> <p>Automated Discovery: Use pattern matching and ML for initial classification</p> </li> <li> <p>Manual Review: Human verification of sensitive data classification</p> </li> <li> <p>Classification Maintenance: Regular review and update of classifications</p> </li> </ul>"},{"location":"best-practices/data-governance/#access-controls","title":"Access Controls","text":""},{"location":"best-practices/data-governance/#data-level-security","title":"Data-Level Security","text":"<ul> <li>Row-Level Security (RLS): Control data access at the row level</li> </ul> <pre><code>-- Create security predicate function\nCREATE FUNCTION dbo.fn_securitypredicate(@Region NVARCHAR(50))\n    RETURNS TABLE\nWITH SCHEMABINDING\nAS\n    RETURN SELECT 1 AS fn_securitypredicate_result\n    WHERE @Region IN (SELECT [Region] FROM dbo.UserRegions WHERE [User] = USER_NAME())\n    OR IS_MEMBER('db_owner') = 1;\n\n-- Create security policy\nCREATE SECURITY POLICY RegionalDataFilter\nADD FILTER PREDICATE dbo.fn_securitypredicate(Region)\nON dbo.SalesData\nWITH (STATE = ON);\n</code></pre> <ul> <li> <p>Column-Level Security: Restrict access to specific columns</p> </li> <li> <p>Dynamic Data Masking: Mask sensitive data for unauthorized users</p> </li> </ul> <pre><code>ALTER TABLE customers\nALTER COLUMN email ADD MASKED WITH (FUNCTION = 'email()');\n\nALTER TABLE customers\nALTER COLUMN phone ADD MASKED WITH (FUNCTION = 'partial(1,\"XXXXXXX\",4)');\n</code></pre>"},{"location":"best-practices/data-governance/#data-sharing-and-collaboration","title":"Data Sharing and Collaboration","text":""},{"location":"best-practices/data-governance/#secure-data-sharing","title":"Secure Data Sharing","text":""},{"location":"best-practices/data-governance/#sharing-mechanisms","title":"Sharing Mechanisms","text":"<ul> <li> <p>Shared Datasets: Define standardized datasets for sharing</p> </li> <li> <p>Views and Functions: Use to control exactly what data is exposed</p> </li> <li> <p>Data Sharing Agreements: Formalize data sharing arrangements</p> </li> </ul>"},{"location":"best-practices/data-governance/#access-governance","title":"Access Governance","text":"<ul> <li> <p>Approval Workflows: Implement formal approval processes</p> </li> <li> <p>Access Reviews: Conduct periodic reviews of shared data access</p> </li> <li> <p>Revocation: Implement mechanisms to revoke access when needed</p> </li> </ul>"},{"location":"best-practices/data-governance/#collaborative-governance","title":"Collaborative Governance","text":""},{"location":"best-practices/data-governance/#cross-functional-collaboration","title":"Cross-Functional Collaboration","text":"<ul> <li> <p>Data Stewardship: Assign domain-specific data stewards</p> </li> <li> <p>Governance Council: Establish cross-functional governance body</p> </li> <li> <p>Community of Practice: Foster data governance community</p> </li> </ul>"},{"location":"best-practices/data-governance/#feedback-mechanisms","title":"Feedback Mechanisms","text":"<ul> <li> <p>Issue Reporting: Create channels for data quality issues</p> </li> <li> <p>Continuous Improvement: Implement process for governance enhancement</p> </li> <li> <p>Knowledge Sharing: Facilitate sharing of best practices</p> </li> </ul>"},{"location":"best-practices/data-governance/#governance-operating-model","title":"Governance Operating Model","text":""},{"location":"best-practices/data-governance/#roles-and-responsibilities","title":"Roles and Responsibilities","text":""},{"location":"best-practices/data-governance/#key-roles","title":"Key Roles","text":"<ul> <li> <p>Chief Data Officer: Executive accountability for data governance</p> </li> <li> <p>Data Governance Lead: Day-to-day governance program management</p> </li> <li> <p>Data Stewards: Domain-specific governance implementation</p> </li> <li> <p>Data Custodians: Technical management of data assets</p> </li> <li> <p>Data Owners: Business accountability for specific data domains</p> </li> </ul>"},{"location":"best-practices/data-governance/#raci-matrix","title":"RACI Matrix","text":"<ul> <li>Define who is Responsible, Accountable, Consulted, and Informed for key governance activities</li> </ul>"},{"location":"best-practices/data-governance/#governance-processes","title":"Governance Processes","text":""},{"location":"best-practices/data-governance/#policy-management","title":"Policy Management","text":"<ul> <li> <p>Policy Development: Process for creating data policies</p> </li> <li> <p>Policy Communication: Mechanisms for communicating policies</p> </li> <li> <p>Policy Enforcement: Procedures for ensuring policy compliance</p> </li> </ul>"},{"location":"best-practices/data-governance/#issue-management","title":"Issue Management","text":"<ul> <li> <p>Issue Identification: Processes for identifying governance issues</p> </li> <li> <p>Remediation: Procedures for addressing identified issues</p> </li> <li> <p>Root Cause Analysis: Methods for preventing recurring issues</p> </li> </ul>"},{"location":"best-practices/data-governance/#measuring-governance-effectiveness","title":"Measuring Governance Effectiveness","text":""},{"location":"best-practices/data-governance/#key-performance-indicators","title":"Key Performance Indicators","text":""},{"location":"best-practices/data-governance/#quality-metrics","title":"Quality Metrics","text":"<ul> <li> <p>Data Quality Score: Composite measure of data quality dimensions</p> </li> <li> <p>Issue Resolution Time: Time to resolve data quality issues</p> </li> <li> <p>Data Coverage: Percentage of data assets under governance</p> </li> </ul>"},{"location":"best-practices/data-governance/#business-impact","title":"Business Impact","text":"<ul> <li> <p>Decision Confidence: Confidence in data-driven decisions</p> </li> <li> <p>Operational Efficiency: Reduced time spent on data preparation</p> </li> <li> <p>Regulatory Compliance: Reduction in compliance findings</p> </li> </ul>"},{"location":"best-practices/data-governance/#maturity-assessment","title":"Maturity Assessment","text":""},{"location":"best-practices/data-governance/#maturity-model","title":"Maturity Model","text":"<ul> <li> <p>Initial: Ad-hoc governance processes</p> </li> <li> <p>Repeatable: Documented governance processes</p> </li> <li> <p>Defined: Standardized governance across organization</p> </li> <li> <p>Managed: Quantitatively managed governance</p> </li> <li> <p>Optimizing: Continuous governance improvement</p> </li> </ul>"},{"location":"best-practices/data-governance/#assessment-process","title":"Assessment Process","text":"<ul> <li> <p>Self-Assessment: Regular internal evaluation of governance maturity</p> </li> <li> <p>Benchmarking: Comparison with industry standards</p> </li> <li> <p>Roadmap Development: Planning for maturity improvement</p> </li> </ul>"},{"location":"best-practices/data-governance/#conclusion","title":"Conclusion","text":"<p>Effective data governance in Azure Synapse Analytics requires a comprehensive approach spanning people, processes, and technology. By implementing these best practices, organizations can ensure their data assets are properly managed, protected, and utilized to deliver maximum business value while maintaining compliance with regulatory requirements.</p> <p>A well-designed data governance framework should evolve with the organization's needs and the changing regulatory landscape. Regular assessment and continuous improvement ensure that governance practices remain effective and aligned with business objectives.</p>"},{"location":"best-practices/performance-optimization/","title":"Performance optimization","text":"<p>Home &gt; Best Practices &gt; Performance Optimization</p>"},{"location":"best-practices/performance-optimization/#performance-optimization-best-practices","title":"Performance Optimization Best Practices","text":""},{"location":"best-practices/performance-optimization/#query-performance-optimization","title":"Query Performance Optimization","text":""},{"location":"best-practices/performance-optimization/#spark-pool-optimization","title":"Spark Pool Optimization","text":""},{"location":"best-practices/performance-optimization/#resource-configuration","title":"Resource Configuration","text":"<ul> <li> <p>Autoscale Configuration: Set appropriate min and max node counts based on workload patterns</p> </li> <li> <p>Node Size Selection: Choose the right memory-to-core ratio based on workload characteristics</p> </li> <li> <p>Dynamic Allocation: Enable dynamic executor allocation for variable workloads</p> </li> <li> <p>Spark Configurations:</p> </li> </ul> <pre><code>spark.sql.adaptive.enabled = true\nspark.sql.adaptive.coalescePartitions.enabled = true\nspark.sql.adaptive.skewJoin.enabled = true\n</code></pre>"},{"location":"best-practices/performance-optimization/#code-optimization","title":"Code Optimization","text":"<ul> <li>DataFrame Caching: Cache intermediate DataFrames for reuse in complex workflows</li> </ul> <pre><code>df = spark.read.format(\"delta\").load(\"/path/to/data\")\ndf.cache()  # Cache the DataFrame for repeated use\n</code></pre> <ul> <li>Partition Pruning: Ensure your queries can leverage partition pruning</li> </ul> <pre><code># Good - enables partition pruning\ndf.filter(df.date_column == \"2025-01-01\").show()\n\n# Bad - prevents partition pruning\ndf.filter(year(df.date_column) == 2025).show()\n</code></pre> <ul> <li>Broadcast Joins: Use broadcast joins for small-to-large table joins</li> </ul> <pre><code>from pyspark.sql.functions import broadcast\n\nlarge_df = spark.table(\"large_table\")\nsmall_df = spark.table(\"small_table\")\n\n# Broadcast the smaller table\nresult = large_df.join(broadcast(small_df), \"join_key\")\n</code></pre>"},{"location":"best-practices/performance-optimization/#serverless-sql-optimization","title":"Serverless SQL Optimization","text":""},{"location":"best-practices/performance-optimization/#query-structure","title":"Query Structure","text":"<ul> <li>Predicate Pushdown: Structure queries to enable predicate pushdown to the storage layer</li> </ul> <pre><code>-- Good: Enables pushdown\nSELECT * FROM external_table WHERE date_column = '2025-01-01'\n\n-- Avoid: Prevents pushdown\nSELECT * FROM external_table WHERE YEAR(date_column) = 2025\n</code></pre> <ul> <li>Column Pruning: Select only necessary columns</li> </ul> <pre><code>-- Good: Only reads required columns\nSELECT column1, column2 FROM large_table\n\n-- Avoid: Reads all columns\nSELECT * FROM large_table\n</code></pre>"},{"location":"best-practices/performance-optimization/#external-table-design","title":"External Table Design","text":"<ul> <li>Statistics: Create and maintain statistics on frequently queried columns</li> </ul> <pre><code>CREATE STATISTICS stats_column1 ON external_table (column1)\n</code></pre> <ul> <li> <p>File Format Selection: Prefer columnar formats (Parquet, Delta) over row-based formats (CSV, JSON)</p> </li> <li> <p>Partitioning Strategy: Align partitioning with common query filters</p> </li> </ul>"},{"location":"best-practices/performance-optimization/#data-storage-optimization","title":"Data Storage Optimization","text":""},{"location":"best-practices/performance-optimization/#file-format-optimization","title":"File Format Optimization","text":""},{"location":"best-practices/performance-optimization/#delta-lake-optimization","title":"Delta Lake Optimization","text":"<ul> <li>File Compaction: Regularly compact small files</li> </ul> <pre><code>OPTIMIZE tableName\n</code></pre> <ul> <li>Z-Ordering: Apply Z-ordering on commonly filtered columns</li> </ul> <pre><code>OPTIMIZE tableName ZORDER BY (column1, column2)\n</code></pre> <ul> <li>Bloom Filter Indexes: Create bloom filter indexes for selective string columns</li> </ul> <pre><code>CREATE BLOOMFILTER INDEX ON TABLE tableName FOR COLUMNS(category_column)\n</code></pre>"},{"location":"best-practices/performance-optimization/#parquet-optimization","title":"Parquet Optimization","text":"<ul> <li>Compression: Use appropriate compression codecs (Snappy for balance, Zstd for better compression)</li> </ul> <pre><code>df.write.option(\"compression\", \"snappy\").format(\"parquet\").save(\"/path/to/data\")\n</code></pre> <ul> <li>Row Group Size: Optimize row group size for your query patterns</li> </ul> <pre><code>df.write.option(\"parquet.block.size\", 134217728).format(\"parquet\").save(\"/path/to/data\")\n</code></pre>"},{"location":"best-practices/performance-optimization/#data-layout-optimization","title":"Data Layout Optimization","text":""},{"location":"best-practices/performance-optimization/#partitioning-strategies","title":"Partitioning Strategies","text":"<ul> <li>Date-Based Partitioning: Partition time series data by appropriate time granularity</li> </ul> <pre><code># Daily partitioning for frequently accessed recent data\ndf.write.partitionBy(\"year\", \"month\", \"day\").format(\"delta\").save(\"/path/to/data\")\n\n# Monthly partitioning for historical data\ndf.write.partitionBy(\"year\", \"month\").format(\"delta\").save(\"/path/to/historical_data\")\n</code></pre> <ul> <li>Categorical Partitioning: Partition by categorical columns with appropriate cardinality</li> </ul> <pre><code># Good: Low to medium cardinality column\ndf.write.partitionBy(\"region\").format(\"delta\").save(\"/path/to/data\")\n\n# Avoid: High cardinality columns like customer_id\n</code></pre> <ul> <li>Hybrid Partitioning: Combine date and categorical dimensions when appropriate</li> </ul> <pre><code>df.write.partitionBy(\"year\", \"month\", \"region\").format(\"delta\").save(\"/path/to/data\")\n</code></pre>"},{"location":"best-practices/performance-optimization/#memory-optimization","title":"Memory Optimization","text":""},{"location":"best-practices/performance-optimization/#spark-memory-management","title":"Spark Memory Management","text":""},{"location":"best-practices/performance-optimization/#memory-configuration","title":"Memory Configuration","text":"<ul> <li>Executor Memory: Allocate appropriate memory for executors based on data size</li> </ul> <pre><code>spark.executor.memory = 8g\n</code></pre> <ul> <li>Memory Fraction: Tune the fraction of heap used for execution and storage</li> </ul> <pre><code>spark.memory.fraction = 0.8\nspark.memory.storageFraction = 0.5\n</code></pre>"},{"location":"best-practices/performance-optimization/#data-skew-handling","title":"Data Skew Handling","text":"<ul> <li>Salting: Add a salt column for skewed keys</li> </ul> <pre><code>from pyspark.sql.functions import monotonically_increasing_id, lit\n\n# Add a salt column for skewed data\ndf = df.withColumn(\"salt\", (monotonically_increasing_id() % 10).cast(\"int\"))\n</code></pre> <ul> <li>Adaptive Query Execution: Enable adaptive query execution for automatic skew handling</li> </ul> <pre><code>spark.sql.adaptive.enabled = true\nspark.sql.adaptive.skewJoin.enabled = true\n</code></pre>"},{"location":"best-practices/performance-optimization/#monitoring-and-tuning","title":"Monitoring and Tuning","text":""},{"location":"best-practices/performance-optimization/#performance-monitoring","title":"Performance Monitoring","text":""},{"location":"best-practices/performance-optimization/#key-metrics-to-monitor","title":"Key Metrics to Monitor","text":"<ul> <li> <p>Spark UI Metrics: Regularly review stage duration, task skew, and shuffle data</p> </li> <li> <p>Query Execution Plans: Analyze physical and logical plans for optimization opportunities</p> </li> </ul> <pre><code>df.explain(True)  # Show the query execution plan\n</code></pre> <ul> <li>I/O Metrics: Monitor read/write throughput and latency</li> </ul>"},{"location":"best-practices/performance-optimization/#tuning-methodology","title":"Tuning Methodology","text":"<ul> <li> <p>Benchmarking: Establish baseline performance metrics</p> </li> <li> <p>Iterative Tuning: Make one change at a time and measure impact</p> </li> <li> <p>Workload Analysis: Tune based on dominant workload patterns</p> </li> </ul>"},{"location":"best-practices/performance-optimization/#cost-optimization","title":"Cost Optimization","text":""},{"location":"best-practices/performance-optimization/#resource-utilization","title":"Resource Utilization","text":""},{"location":"best-practices/performance-optimization/#auto-scaling","title":"Auto-Scaling","text":"<ul> <li> <p>Configure auto-scaling based on actual workload patterns</p> </li> <li> <p>Set appropriate idle timeout to reduce costs during inactive periods</p> </li> </ul>"},{"location":"best-practices/performance-optimization/#right-sizing","title":"Right-Sizing","text":"<ul> <li> <p>Start with smaller instances and scale up only when needed</p> </li> <li> <p>Monitor utilization metrics to identify over-provisioned resources</p> </li> </ul>"},{"location":"best-practices/performance-optimization/#storage-costs","title":"Storage Costs","text":""},{"location":"best-practices/performance-optimization/#data-lifecycle-management","title":"Data Lifecycle Management","text":"<ul> <li> <p>Archive infrequently accessed data to cooler storage tiers</p> </li> <li> <p>Implement retention policies for transient data</p> </li> <li> <p>Use the appropriate storage tier based on access patterns</p> </li> </ul>"},{"location":"best-practices/performance-optimization/#vacuum-operations","title":"Vacuum Operations","text":"<ul> <li>Regularly clean up old Delta files</li> </ul> <pre><code>VACUUM tableName RETAIN 7 DAYS\n</code></pre> <ul> <li>Monitor and manage transaction log growth</li> </ul>"},{"location":"best-practices/performance-optimization/#conclusion","title":"Conclusion","text":"<p>Optimizing performance in Azure Synapse Analytics requires a holistic approach covering storage organization, query design, resource configuration, and ongoing monitoring. By applying these best practices, you can achieve significant performance improvements and cost savings while meeting your analytical workload requirements.</p> <p>Remember that performance optimization is an iterative process that should be tailored to your specific workload characteristics and business requirements.</p>"},{"location":"best-practices/security/","title":"Security Best Practices for Azure Synapse Analytics","text":"<p>Home &gt; Best Practices &gt; Security</p>"},{"location":"best-practices/security/#identity-and-access-management","title":"Identity and Access Management","text":""},{"location":"best-practices/security/#azure-active-directory-integration","title":"Azure Active Directory Integration","text":""},{"location":"best-practices/security/#authentication","title":"Authentication","text":"<ul> <li>Enforce AAD Authentication: Use Azure Active Directory as the primary authentication method</li> <li>Managed Identities: Use managed identities for service-to-service authentication</li> </ul> <pre><code>{\n  \"type\": \"Microsoft.Synapse/workspaces\",\n  \"properties\": {\n    \"identity\": {\n      \"type\": \"SystemAssigned\"\n    }\n  }\n}\n</code></pre> <ul> <li>Multi-Factor Authentication: Require MFA for all users accessing Synapse workspaces</li> </ul>"},{"location":"best-practices/security/#authorization","title":"Authorization","text":"<ul> <li>Role-Based Access Control (RBAC): Implement least privilege principle with built-in and custom roles</li> </ul> <pre><code># Assign Synapse Contributor role to a user\nNew-AzRoleAssignment -SignInName user@contoso.com `\n    -RoleDefinitionName \"Synapse Contributor\" `\n    -Scope \"/subscriptions/&lt;subscription-id&gt;/resourceGroups/&lt;resource-group&gt;/providers/Microsoft.Synapse/workspaces/&lt;workspace-name&gt;\"\n</code></pre> <ul> <li>Custom Roles: Create custom roles for specialized access requirements</li> <li>Conditional Access Policies: Implement based on device compliance, location, and risk</li> </ul>"},{"location":"best-practices/security/#workspace-level-security","title":"Workspace-Level Security","text":"<ul> <li>IP Firewall Rules: Restrict workspace access to specific IP ranges</li> </ul> <pre><code>{\n  \"properties\": {\n    \"defaultDataLakeStorage\": {},\n    \"managedResourceGroupName\": \"workspaceManagedGroup\",\n    \"sqlAdministratorLogin\": \"sqladminuser\",\n    \"sqlAdministratorLoginPassword\": \"...\",\n    \"managedVirtualNetwork\": \"default\",\n    \"trustedServiceBypassEnabled\": true,\n    \"azureADOnlyAuthentication\": true,\n    \"firewallRules\": [\n      {\n        \"name\": \"AllowedIPs\",\n        \"properties\": {\n          \"startIpAddress\": \"10.0.0.0\",\n          \"endIpAddress\": \"10.0.0.255\"\n        }\n      }\n    ]\n  }\n}\n</code></pre> <ul> <li>Private Link: Use private endpoints to access Synapse from your VNet</li> <li>Managed Virtual Networks: Isolate Synapse resources within managed VNet</li> </ul>"},{"location":"best-practices/security/#data-security","title":"Data Security","text":""},{"location":"best-practices/security/#encryption-and-data-protection","title":"Encryption and Data Protection","text":""},{"location":"best-practices/security/#data-encryption","title":"Data Encryption","text":"<ul> <li>Encryption at Rest: Enable encryption for all data storage</li> <li>Use customer-managed keys when greater control is needed</li> </ul> <pre><code># Configure customer-managed key encryption\nSet-AzSynapseWorkspace -Name $workspaceName `\n    -ResourceGroupName $resourceGroupName `\n    -KeyVaultUrl $keyVaultUrl `\n    -KeyName $keyName `\n    -KeyVersion $keyVersion\n</code></pre> <ul> <li>Encryption in Transit: Ensure all data connections use TLS 1.2+</li> <li>Transparent Data Encryption (TDE): Enable for SQL pools</li> </ul>"},{"location":"best-practices/security/#sensitive-data-handling","title":"Sensitive Data Handling","text":"<ul> <li>Data Classification: Implement data discovery and classification</li> </ul> <pre><code>ADD SENSITIVITY CLASSIFICATION TO\n  schema.table.column\nWITH (\n  LABEL = 'Confidential',\n  INFORMATION_TYPE = 'Financial'\n)\n</code></pre> <ul> <li>Dynamic Data Masking: Apply to sensitive columns</li> </ul> <pre><code>ALTER TABLE customer ADD MASKED WITH (FUNCTION = 'partial(2,\"XXXXXXX\",0)') FOR COLUMN credit_card;\n</code></pre> <ul> <li>Data Anonymization: Use techniques like tokenization, perturbation, and generalization for analytics on sensitive data</li> </ul>"},{"location":"best-practices/security/#sql-security-features","title":"SQL Security Features","text":""},{"location":"best-practices/security/#sql-specific-controls","title":"SQL-Specific Controls","text":"<ul> <li>Row-Level Security (RLS): Implement for fine-grained access control</li> </ul> <pre><code>-- Create security predicate function\nCREATE FUNCTION dbo.fn_securitypredicate(@Region VARCHAR(50))  \n    RETURNS TABLE  \nWITH SCHEMABINDING  \nAS  \n    RETURN SELECT 1 AS fn_securitypredicate_result\n    WHERE @Region = 'North' AND USER_NAME() = 'northsalesuser'\n    OR @Region = 'South' AND USER_NAME() = 'southsalesuser'\n    OR IS_MEMBER('db_owner') = 1;\n\n-- Create security policy\nCREATE SECURITY POLICY SalesDataFilter  \nADD FILTER PREDICATE dbo.fn_securitypredicate(Region)\nON dbo.SalesData;\n</code></pre> <ul> <li>Column-Level Security: Restrict column access based on user roles</li> </ul> <pre><code>DENY SELECT ON dbo.employees(salary) TO analyst_role;\n</code></pre> <ul> <li>SQL Vulnerability Assessment: Enable regular automated security scans</li> </ul>"},{"location":"best-practices/security/#network-security","title":"Network Security","text":""},{"location":"best-practices/security/#network-isolation","title":"Network Isolation","text":""},{"location":"best-practices/security/#private-endpoints","title":"Private Endpoints","text":"<ul> <li>Private Link Service: Implement for secure connectivity from VNets</li> </ul> <pre><code>{\n  \"name\": \"private-endpoint\",\n  \"properties\": {\n    \"privateLinkServiceConnections\": [\n      {\n        \"name\": \"synapse-private-link\",\n        \"properties\": {\n          \"privateLinkServiceId\": \"/subscriptions/&lt;sub-id&gt;/resourceGroups/&lt;rg&gt;/providers/Microsoft.Synapse/workspaces/&lt;workspace&gt;\",\n          \"groupIds\": [ \"Sql\" ]\n        }\n      }\n    ],\n    \"subnet\": {\n      \"id\": \"/subscriptions/&lt;sub-id&gt;/resourceGroups/&lt;rg&gt;/providers/Microsoft.Network/virtualNetworks/&lt;vnet&gt;/subnets/&lt;subnet&gt;\"\n    }\n  }\n}\n</code></pre> <ul> <li>Service Endpoints: Enable for services that don't support Private Link</li> </ul>"},{"location":"best-practices/security/#network-security-groups","title":"Network Security Groups","text":"<ul> <li>NSG Rules: Implement restrictive inbound/outbound rules</li> <li>Application Security Groups: Group related resources for simplified management</li> </ul>"},{"location":"best-practices/security/#managed-virtual-network","title":"Managed Virtual Network","text":"<ul> <li>Data Exfiltration Protection: Enable to prevent data leakage</li> <li>Approved Private Endpoints: Restrict outbound connectivity to approved resources</li> </ul>"},{"location":"best-practices/security/#secret-management","title":"Secret Management","text":""},{"location":"best-practices/security/#azure-key-vault-integration","title":"Azure Key Vault Integration","text":""},{"location":"best-practices/security/#credential-storage","title":"Credential Storage","text":"<ul> <li>Key Vault References: Store and reference secrets securely</li> </ul> <pre><code># Using Key Vault reference in Spark\nconnectionString = dbutils.secrets.get(scope=\"key-vault-scope\", key=\"storage-connection-string\")\n</code></pre> <ul> <li>Key Rotation: Implement regular key rotation policies</li> <li>Access Policies: Restrict key vault access based on least privilege</li> </ul>"},{"location":"best-practices/security/#secure-parameter-passing","title":"Secure Parameter Passing","text":"<ul> <li>Azure Synapse Pipelines: Use pipeline parameters with secure strings</li> <li>Linked Services: Use Key Vault for credentials in linked services</li> </ul> <pre><code>{\n  \"name\": \"AzureStorageLinkedService\",\n  \"properties\": {\n    \"type\": \"AzureBlobStorage\",\n    \"typeProperties\": {\n      \"connectionString\": {\n        \"type\": \"AzureKeyVaultSecret\",\n        \"store\": {\n          \"referenceName\": \"AzureKeyVaultLinkedService\",\n          \"type\": \"LinkedServiceReference\"\n        },\n        \"secretName\": \"StorageConnectionString\"\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"best-practices/security/#auditing-and-monitoring","title":"Auditing and Monitoring","text":""},{"location":"best-practices/security/#comprehensive-logging","title":"Comprehensive Logging","text":""},{"location":"best-practices/security/#audit-configuration","title":"Audit Configuration","text":"<ul> <li>Enable Diagnostics Logging: Configure for all Synapse components</li> </ul> <pre><code>{\n  \"properties\": {\n    \"workspaceId\": \"/subscriptions/&lt;sub-id&gt;/resourceGroups/&lt;rg&gt;/providers/Microsoft.Synapse/workspaces/&lt;workspace&gt;\",\n    \"logs\": [\n      {\n        \"category\": \"SynapseRbacOperations\",\n        \"enabled\": true,\n        \"retentionPolicy\": {\n          \"days\": 90,\n          \"enabled\": true\n        }\n      }\n    ]\n  }\n}\n</code></pre> <ul> <li>SQL Auditing: Enable auditing for SQL pools</li> </ul> <pre><code>CREATE SERVER AUDIT [AuditName]\nTO BLOB_STORAGE (\n    STORAGE_ENDPOINT = 'https://storageaccount.blob.core.windows.net/';\n    STORAGE_ACCOUNT_ACCESS_KEY = '...';\n    RETENTION_DAYS = 90\n)\nWITH ( QUEUE_DELAY = 1000, ON_FAILURE = CONTINUE )\n</code></pre> <ul> <li>Advanced Threat Protection: Enable to detect anomalous activities</li> </ul>"},{"location":"best-practices/security/#security-monitoring","title":"Security Monitoring","text":"<ul> <li>Azure Security Center: Enable for vulnerability assessment</li> <li>Azure Sentinel: Integrate for advanced security monitoring and response</li> <li>Alert Configuration: Set up alerts for suspicious activities</li> </ul> <pre><code># Create a security alert\nNew-AzSecurityAlert -ResourceId \"/subscriptions/&lt;sub-id&gt;/resourceGroups/&lt;rg&gt;/providers/Microsoft.Synapse/workspaces/&lt;workspace&gt;\" `\n    -AlertDisplayName \"Suspicious authentication failure\" `\n    -AlertName \"SuspiciousAuthFailure\"\n</code></pre>"},{"location":"best-practices/security/#compliance-and-governance","title":"Compliance and Governance","text":""},{"location":"best-practices/security/#data-governance","title":"Data Governance","text":""},{"location":"best-practices/security/#data-lineage","title":"Data Lineage","text":"<ul> <li>Azure Purview Integration: Enable for automated data discovery and classification</li> <li>Metadata Management: Maintain accurate metadata with descriptions and ownership</li> </ul>"},{"location":"best-practices/security/#compliance-controls","title":"Compliance Controls","text":"<ul> <li>Data Residency: Ensure data remains in compliant regions</li> <li>Retention Policies: Implement appropriate data retention policies</li> </ul> <pre><code>-- Example retention policy in Spark SQL\nALTER TABLE orders SET TBLPROPERTIES (\n  'delta.logRetentionDuration' = 'interval 365 days',\n  'delta.deletedFileRetentionDuration' = 'interval 30 days'\n)\n</code></pre> <ul> <li>Regulatory Compliance: Implement controls required by GDPR, HIPAA, etc.</li> </ul>"},{"location":"best-practices/security/#security-devops","title":"Security DevOps","text":""},{"location":"best-practices/security/#security-in-cicd","title":"Security in CI/CD","text":""},{"location":"best-practices/security/#secure-deployment-practices","title":"Secure Deployment Practices","text":"<ul> <li>Infrastructure as Code: Use Azure Resource Manager or Bicep templates with security parameters</li> <li>Template Validation: Validate templates for security compliance</li> <li>Automated Testing: Include security testing in CI/CD pipelines</li> </ul>"},{"location":"best-practices/security/#security-posture-management","title":"Security Posture Management","text":"<ul> <li>Regular Assessment: Schedule regular security assessments</li> <li>Vulnerability Management: Track and remediate vulnerabilities</li> <li>Security Baselines: Establish and maintain security baselines</li> </ul>"},{"location":"best-practices/security/#conclusion","title":"Conclusion","text":"<p>Implementing a defense-in-depth approach to security in Azure Synapse Analytics requires attention to multiple layers including identity, data, network, and governance. By following these best practices, you can create a secure analytics environment that protects your data assets while enabling productive analytics workflows.</p> <p>For more information on security in Azure Synapse Analytics, refer to the official security documentation.</p>"},{"location":"code-examples/","title":"Azure Synapse Analytics Code Examples","text":"<p>Home &gt; Code Examples</p> <p>This section contains practical code examples for Azure Synapse Analytics, organized by feature area. Each example includes code snippets, explanations, and best practices to help you effectively implement Synapse Analytics solutions.</p>"},{"location":"code-examples/#categories","title":"Categories","text":""},{"location":"code-examples/#delta-lake","title":"Delta Lake","text":"<p>Delta Lake examples demonstrate how to work with Delta tables in Synapse Analytics using PySpark:</p> <ul> <li>Ingestion - Auto Loader examples for efficient data ingestion</li> <li>Change Data Capture (CDC) - CDC implementation patterns and streaming updates</li> <li>Table Optimization - Optimize, vacuum, and Z-ORDER Delta tables</li> </ul>"},{"location":"code-examples/#serverless-sql","title":"Serverless SQL","text":"<p>Serverless SQL examples show how to effectively query data lakes using SQL in Synapse:</p> <ul> <li>Query Optimization - Performance tuning techniques for serverless SQL</li> </ul>"},{"location":"code-examples/#integration","title":"Integration","text":"<p>Integration examples showcase how to connect Synapse with other Azure services:</p> <ul> <li>Coming soon: Azure ML integration</li> <li>Coming soon: Azure Purview integration</li> <li>Coming soon: Azure Data Factory integration</li> </ul>"},{"location":"code-examples/#troubleshooting","title":"Troubleshooting","text":"<p>Troubleshooting examples provide solutions to common issues:</p> <ul> <li>Coming soon: Spark memory management</li> <li>Coming soon: Serverless SQL connectivity issues</li> <li>Coming soon: Performance diagnostic techniques</li> </ul>"},{"location":"code-examples/#example-structure","title":"Example Structure","text":"<p>Each code example follows a consistent structure:</p> <ol> <li>Introduction - Brief overview of the feature and use case</li> <li>Prerequisites - Required resources and permissions</li> <li>Code Examples - Step-by-step implementation with code snippets</li> <li>Best Practices - Recommendations for optimal implementation</li> <li>Common Issues - Troubleshooting guidance for known issues</li> <li>Related Links - Additional resources for further reading</li> </ol>"},{"location":"code-examples/#contributing","title":"Contributing","text":"<p>To contribute new code examples, please follow these guidelines:</p> <ol> <li>Create a new markdown file in the appropriate category folder</li> <li>Follow the consistent example structure outlined above</li> <li>Include detailed comments in code snippets</li> <li>Ensure all examples are tested and validated</li> <li>Update the index files to include links to your new example</li> </ol>"},{"location":"code-examples/delta-lake/","title":"Delta Lake Examples for Azure Synapse Analytics","text":"<p>Home &gt; Code Examples &gt; Delta Lake</p> <p>This section provides examples and best practices for working with Delta Lake in Azure Synapse Analytics. Delta Lake is an open-source storage layer that brings reliability to data lakes by providing ACID transactions, scalable metadata handling, and unifying streaming and batch data processing.</p>"},{"location":"code-examples/delta-lake/#available-examples","title":"Available Examples","text":""},{"location":"code-examples/delta-lake/#data-ingestion","title":"Data Ingestion","text":"<ul> <li>Auto Loader - Efficiently ingest data from files into Delta tables</li> <li>Basic auto loading with schema inference</li> <li>Schema evolution handling</li> <li>Partition management</li> <li>Optimized configurations</li> </ul>"},{"location":"code-examples/delta-lake/#data-change-management","title":"Data Change Management","text":"<ul> <li>Change Data Capture (CDC) - Implement change data capture patterns with Delta Lake</li> <li>Delta Lake Change Data Feed (CDF)</li> <li>Time travel for table comparisons</li> <li>Streaming CDC processing</li> <li>SCD Type 2 implementation</li> <li>CDC from external sources</li> </ul>"},{"location":"code-examples/delta-lake/#performance-optimization","title":"Performance Optimization","text":"<ul> <li>Table Optimization - Optimize Delta tables for performance</li> <li>OPTIMIZE command usage</li> <li>VACUUM command usage</li> <li>Z-ORDER for data skipping</li> <li>Automated maintenance workflows</li> <li>Partition-aware optimization</li> <li>Monitoring and statistics</li> </ul>"},{"location":"code-examples/delta-lake/#why-delta-lake-in-azure-synapse","title":"Why Delta Lake in Azure Synapse?","text":"<p>Delta Lake provides several benefits for data lakes in Azure Synapse Analytics:</p> <ol> <li>ACID Transactions: Ensures data consistency with serializable isolation levels</li> <li>Schema Enforcement: Prevents data corruption by validating data against the schema</li> <li>Schema Evolution: Adapts to changing data schemas without breaking downstream applications</li> <li>Time Travel: Access and restore previous versions of data using snapshots</li> <li>Audit History: Track all changes made to tables with complete history</li> <li>Unified Batch and Streaming: Process both batch and streaming data in the same architecture</li> </ol>"},{"location":"code-examples/delta-lake/#delta-lake-architecture-in-azure-synapse","title":"Delta Lake Architecture in Azure Synapse","text":"<p>Delta Lake in Azure Synapse Analytics typically follows this architecture:</p> <ol> <li>Bronze Layer: Raw data ingested from various sources</li> <li>Silver Layer: Cleansed and conformed data with business keys</li> <li>Gold Layer: Aggregated, enriched data optimized for analytics and consumption</li> </ol>"},{"location":"code-examples/delta-lake/#related-resources","title":"Related Resources","text":"<ul> <li>Delta Lake Official Documentation</li> <li>Azure Synapse Analytics Documentation</li> <li>Lakehouse Architecture Patterns</li> </ul>"},{"location":"code-examples/delta-lake/cdc/change-data-capture/","title":"Change Data Capture (CDC) with Delta Lake in Azure Synapse Analytics","text":"<p>Home &gt; Code Examples &gt; Delta Lake &gt; Change Data Capture</p> <p>This guide provides detailed examples for implementing Change Data Capture (CDC) patterns with Delta Lake in Azure Synapse Analytics.</p>"},{"location":"code-examples/delta-lake/cdc/change-data-capture/#introduction-to-cdc-with-delta-lake","title":"Introduction to CDC with Delta Lake","text":"<p>Change Data Capture (CDC) is a pattern for efficiently tracking and processing changes to data. Delta Lake provides built-in features that make implementing CDC patterns straightforward and efficient in Azure Synapse Analytics.</p>"},{"location":"code-examples/delta-lake/cdc/change-data-capture/#prerequisites","title":"Prerequisites","text":"<ul> <li>Azure Synapse Analytics workspace</li> <li>Storage account with a container</li> <li>Appropriate permissions and access to Azure resources</li> </ul>"},{"location":"code-examples/delta-lake/cdc/change-data-capture/#cdc-implementation-methods","title":"CDC Implementation Methods","text":""},{"location":"code-examples/delta-lake/cdc/change-data-capture/#method-1-using-delta-lake-change-data-feed","title":"Method 1: Using Delta Lake Change Data Feed","text":"<p>Delta Lake's Change Data Feed captures row-level changes between versions of a Delta table. Here's how to enable and use it in Azure Synapse Analytics:</p> <pre><code># Import required libraries\nfrom pyspark.sql import SparkSession\nfrom delta.tables import DeltaTable\nimport pyspark.sql.functions as F\n\n# Create Spark session with Delta Lake support\nspark = SparkSession.builder \\\n    .appName(\"Delta Lake CDC Example\") \\\n    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n    .getOrCreate()\n\n# Define paths\ndelta_table_path = \"abfss://container@storage.dfs.core.windows.net/delta/customer_table/\"\n\n# Enable Change Data Feed (CDF) - For new tables\nspark.sql(f\"\"\"\nCREATE TABLE IF NOT EXISTS customer_table\nUSING DELTA\nLOCATION '{delta_table_path}'\nTBLPROPERTIES (delta.enableChangeDataFeed = true)\n\"\"\")\n\n# For existing tables, you can enable CDF using:\nspark.sql(f\"\"\"\nALTER TABLE delta.`{delta_table_path}`\nSET TBLPROPERTIES (delta.enableChangeDataFeed = true)\n\"\"\")\n\n# Make some changes to the table (insert, update, delete)\n# ...\n\n# Read the Change Data Feed to get all changes between versions\nchanges_df = spark.read.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .option(\"startingVersion\", 0) \\\n    .option(\"endingVersion\", 10) \\\n    .load(delta_table_path)\n\n# The changes dataframe includes these CDC-specific columns:\n# - _change_type: insert, update_preimage, update_postimage, delete\n# - _commit_version: Delta version for this change\n# - _commit_timestamp: Timestamp when this change was committed\n\n# Filter for specific change types\ninserts_df = changes_df.filter(\"_change_type = 'insert'\")\nupdates_df = changes_df.filter(\"_change_type = 'update_postimage'\")\ndeletes_df = changes_df.filter(\"_change_type = 'delete'\")\n\n# Display the changes\ninserts_df.show()\nupdates_df.show()\ndeletes_df.show()\n</code></pre>"},{"location":"code-examples/delta-lake/cdc/change-data-capture/#method-2-time-travel-and-table-comparison","title":"Method 2: Time Travel and Table Comparison","text":"<p>You can also implement CDC by comparing table versions using Delta Lake's time travel capability:</p> <pre><code># Import required libraries\nfrom pyspark.sql import SparkSession\nfrom delta.tables import DeltaTable\nimport pyspark.sql.functions as F\n\n# Create Spark session with Delta Lake support\nspark = SparkSession.builder \\\n    .appName(\"Delta Lake Time Travel CDC\") \\\n    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n    .getOrCreate()\n\n# Define paths\ndelta_table_path = \"abfss://container@storage.dfs.core.windows.net/delta/product_table/\"\n\n# Read current version\ncurrent_df = spark.read.format(\"delta\").load(delta_table_path)\n\n# Read previous version using time travel\nprevious_df = spark.read.format(\"delta\").option(\"versionAsOf\", 5).load(delta_table_path)\n\n# Register temporary views for SQL comparison\ncurrent_df.createOrReplaceTempView(\"current_version\")\nprevious_df.createOrReplaceTempView(\"previous_version\")\n\n# Identify inserted records (in current but not in previous)\ninserted_records = spark.sql(\"\"\"\nSELECT c.* \nFROM current_version c\nLEFT JOIN previous_version p ON c.id = p.id\nWHERE p.id IS NULL\n\"\"\")\n\n# Identify deleted records (in previous but not in current)\ndeleted_records = spark.sql(\"\"\"\nSELECT p.* \nFROM previous_version p\nLEFT JOIN current_version c ON p.id = c.id\nWHERE c.id IS NULL\n\"\"\")\n\n# Identify updated records (in both but with different values)\n# This assumes a 'last_modified' column exists to detect changes\nupdated_records = spark.sql(\"\"\"\nSELECT c.* \nFROM current_version c\nJOIN previous_version p ON c.id = p.id\nWHERE c.last_modified &gt; p.last_modified\n\"\"\")\n\n# Display the changes\ninserted_records.show()\ndeleted_records.show()\nupdated_records.show()\n</code></pre>"},{"location":"code-examples/delta-lake/cdc/change-data-capture/#advanced-cdc-patterns","title":"Advanced CDC Patterns","text":""},{"location":"code-examples/delta-lake/cdc/change-data-capture/#cdc-with-streaming-for-real-time-processing","title":"CDC with Streaming for Real-time Processing","text":"<pre><code># Import required libraries\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, current_timestamp\n\n# Create Spark session\nspark = SparkSession.builder \\\n    .appName(\"CDC Streaming with Delta\") \\\n    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n    .getOrCreate()\n\n# Define paths\ndelta_table_path = \"abfss://container@storage.dfs.core.windows.net/delta/orders_table/\"\ncheckpoint_path = \"abfss://container@storage.dfs.core.windows.net/checkpoints/orders_cdc/\"\n\n# Stream changes from the Change Data Feed\ncdc_stream = spark.readStream \\\n    .format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .option(\"startingVersion\", 0) \\\n    .load(delta_table_path)\n\n# Process changes based on change type\ndef process_cdc_batch(batch_df, batch_id):\n    # Split batch by operation type\n    inserts = batch_df.filter(\"_change_type = 'insert'\")\n    updates = batch_df.filter(\"_change_type = 'update_postimage'\")\n    deletes = batch_df.filter(\"_change_type = 'delete'\")\n\n    # Process each type differently (e.g., send to different destinations)\n    if not inserts.isEmpty():\n        inserts.drop(\"_change_type\", \"_commit_version\", \"_commit_timestamp\") \\\n            .write \\\n            .format(\"delta\") \\\n            .mode(\"append\") \\\n            .save(\"abfss://container@storage.dfs.core.windows.net/delta/orders_inserts/\")\n\n    if not updates.isEmpty():\n        updates.drop(\"_change_type\", \"_commit_version\", \"_commit_timestamp\") \\\n            .write \\\n            .format(\"delta\") \\\n            .mode(\"append\") \\\n            .save(\"abfss://container@storage.dfs.core.windows.net/delta/orders_updates/\")\n\n    if not deletes.isEmpty():\n        deletes.drop(\"_change_type\", \"_commit_version\", \"_commit_timestamp\") \\\n            .write \\\n            .format(\"delta\") \\\n            .mode(\"append\") \\\n            .save(\"abfss://container@storage.dfs.core.windows.net/delta/orders_deletes/\")\n\n# Start streaming process\nquery = cdc_stream.writeStream \\\n    .foreachBatch(process_cdc_batch) \\\n    .option(\"checkpointLocation\", checkpoint_path) \\\n    .trigger(processingTime=\"5 minutes\") \\\n    .start()\n\n# Wait for the query to terminate\nquery.awaitTermination()\n</code></pre>"},{"location":"code-examples/delta-lake/cdc/change-data-capture/#scd-type-2-implementation-with-delta-lake","title":"SCD Type 2 Implementation with Delta Lake","text":"<p>Slowly Changing Dimension Type 2 (SCD Type 2) preserves the history of data changes by creating new records for changed dimensions:</p> <pre><code># Import required libraries\nfrom pyspark.sql import SparkSession\nfrom delta.tables import DeltaTable\nimport pyspark.sql.functions as F\nfrom datetime import datetime\n\n# Create Spark session\nspark = SparkSession.builder \\\n    .appName(\"SCD Type 2 with Delta\") \\\n    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n    .getOrCreate()\n\n# Define paths\ndim_customer_path = \"abfss://container@storage.dfs.core.windows.net/delta/dim_customer/\"\ncustomer_updates_path = \"abfss://container@storage.dfs.core.windows.net/landing/customer_updates/\"\n\n# Load the current dimension table (if it exists)\nif DeltaTable.isDeltaTable(spark, dim_customer_path):\n    # If table exists, load as DeltaTable\n    customerDimTable = DeltaTable.forPath(spark, dim_customer_path)\n\n    # Read the new data that contains updates\n    newCustomerData = spark.read.format(\"parquet\").load(customer_updates_path)\n\n    # Current time for setting effective dates\n    current_time = datetime.now()\n\n    # Execute SCD Type 2 operation\n    (customerDimTable.alias(\"dim\")\n        .merge(\n            newCustomerData.alias(\"updates\"),\n            \"dim.customer_id = updates.customer_id AND dim.is_current = true\"\n        )\n        .whenMatchedAndExpressionsMatch(\n            [\n                \"dim.name &lt;&gt; updates.name OR \" + \n                \"dim.address &lt;&gt; updates.address OR \" +\n                \"dim.phone &lt;&gt; updates.phone\"\n            ]\n        )\n        .updateAll(\n            {\n                \"is_current\": \"false\",\n                \"end_date\": F.lit(current_time)\n            }\n        )\n        .whenMatchedAndExpressionsNotMatch(\n            [\n                \"dim.name &lt;&gt; updates.name OR \" + \n                \"dim.address &lt;&gt; updates.address OR \" +\n                \"dim.phone &lt;&gt; updates.phone\"\n            ]\n        )\n        .updateAll()  # No changes if attributes match\n        .whenNotMatchedInsertAll()\n        .execute())\n\n    # Insert new records for the updated customers\n    customerDimTable = DeltaTable.forPath(spark, dim_customer_path)\n\n    matched_updates = (customerDimTable.alias(\"dim\")\n        .merge(\n            newCustomerData.alias(\"updates\"),\n            \"dim.customer_id = updates.customer_id AND dim.is_current = false AND dim.end_date = '{}'\".format(current_time)\n        )\n        .whenMatchedUpdateAll()\n        .execute())\n\n    # Load dimension table as DataFrame\n    dimDF = spark.read.format(\"delta\").load(dim_customer_path)\n\n    # Get updated records that need a new current version\n    updatedCustomers = (dimDF\n        .filter(F.col(\"is_current\") == False)\n        .filter(F.col(\"end_date\") == current_time))\n\n    # Create new current records\n    newCurrentRecords = (updatedCustomers\n        .select(\n            \"customer_id\", \"name\", \"address\", \"phone\", \"email\", \"other_attributes\"\n        )\n        .withColumn(\"is_current\", F.lit(True))\n        .withColumn(\"start_date\", F.lit(current_time))\n        .withColumn(\"end_date\", F.lit(None)))\n\n    # Write new current records to the dimension table\n    newCurrentRecords.write \\\n        .format(\"delta\") \\\n        .mode(\"append\") \\\n        .save(dim_customer_path)\n\nelse:\n    # If table doesn't exist, create it with initial data\n    initial_data = spark.read.format(\"parquet\").load(customer_updates_path) \\\n        .withColumn(\"is_current\", F.lit(True)) \\\n        .withColumn(\"start_date\", F.lit(datetime.now())) \\\n        .withColumn(\"end_date\", F.lit(None))\n\n    initial_data.write \\\n        .format(\"delta\") \\\n        .save(dim_customer_path)\n</code></pre>"},{"location":"code-examples/delta-lake/cdc/change-data-capture/#implementing-cdc-from-external-source-systems","title":"Implementing CDC from External Source Systems","text":""},{"location":"code-examples/delta-lake/cdc/change-data-capture/#cdc-from-sql-server-using-debezium","title":"CDC from SQL Server Using Debezium","text":"<p>This example demonstrates how to capture changes from SQL Server using Debezium and process them with Delta Lake:</p> <pre><code># Import required libraries\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import from_json, col\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n\n# Create Spark session\nspark = SparkSession.builder \\\n    .appName(\"SQL Server CDC with Delta\") \\\n    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n    .getOrCreate()\n\n# Define Kafka parameters for Debezium CDC events\nkafka_bootstrap_servers = \"kafka:9092\"\nkafka_topic = \"sqlserver.dbo.customers\"\ncheckpoint_path = \"abfss://container@storage.dfs.core.windows.net/checkpoints/sqlserver_cdc/\"\ndelta_table_path = \"abfss://container@storage.dfs.core.windows.net/delta/customers_from_sqlserver/\"\n\n# Define schema for the customer data\ncustomer_schema = StructType([\n    StructField(\"id\", IntegerType(), True),\n    StructField(\"name\", StringType(), True),\n    StructField(\"email\", StringType(), True),\n    StructField(\"phone\", StringType(), True),\n    StructField(\"address\", StringType(), True),\n    StructField(\"created_at\", TimestampType(), True),\n    StructField(\"updated_at\", TimestampType(), True)\n])\n\n# Define schema for the CDC event\ncdc_schema = StructType([\n    StructField(\"before\", customer_schema, True),\n    StructField(\"after\", customer_schema, True),\n    StructField(\"source\", StructType([\n        StructField(\"version\", StringType(), True),\n        StructField(\"connector\", StringType(), True),\n        StructField(\"name\", StringType(), True),\n        StructField(\"ts_ms\", TimestampType(), True),\n        StructField(\"snapshot\", StringType(), True),\n        StructField(\"db\", StringType(), True),\n        StructField(\"table\", StringType(), True),\n        StructField(\"server_id\", StringType(), True),\n        StructField(\"file\", StringType(), True),\n        StructField(\"pos\", StringType(), True)\n    ]), True),\n    StructField(\"op\", StringType(), True),\n    StructField(\"ts_ms\", TimestampType(), True)\n])\n\n# Read Debezium CDC events from Kafka\ncdc_stream = spark.readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n    .option(\"subscribe\", kafka_topic) \\\n    .option(\"startingOffsets\", \"earliest\") \\\n    .load()\n\n# Parse the CDC events\nparsed_stream = cdc_stream \\\n    .selectExpr(\"CAST(value AS STRING)\") \\\n    .select(from_json(col(\"value\"), cdc_schema).alias(\"data\")) \\\n    .select(\"data.*\")\n\n# Process CDC operations\ndef process_cdc_batch(batch_df, batch_id):\n    if batch_df.isEmpty():\n        return\n\n    # Get the DeltaTable object\n    if DeltaTable.isDeltaTable(spark, delta_table_path):\n        delta_table = DeltaTable.forPath(spark, delta_table_path)\n    else:\n        # If the table doesn't exist, create it with an empty dataframe\n        empty_df = spark.createDataFrame([], customer_schema)\n        empty_df.write.format(\"delta\").save(delta_table_path)\n        delta_table = DeltaTable.forPath(spark, delta_table_path)\n\n    # Process inserts (op = 'c' for create)\n    inserts = batch_df.filter(\"op = 'c'\").select(\"after.*\")\n    if not inserts.isEmpty():\n        inserts.write.format(\"delta\").mode(\"append\").save(delta_table_path)\n\n    # Process updates (op = 'u' for update)\n    updates = batch_df.filter(\"op = 'u'\").select(\"after.*\")\n    if not updates.isEmpty():\n        for row in updates.collect():\n            customer_id = row.id\n            delta_table.update(\n                condition=f\"id = {customer_id}\",\n                set={\n                    \"name\": row.name,\n                    \"email\": row.email,\n                    \"phone\": row.phone,\n                    \"address\": row.address,\n                    \"updated_at\": row.updated_at\n                }\n            )\n\n    # Process deletes (op = 'd' for delete)\n    deletes = batch_df.filter(\"op = 'd'\").select(\"before.id\")\n    if not deletes.isEmpty():\n        for row in deletes.collect():\n            customer_id = row.id\n            delta_table.delete(f\"id = {customer_id}\")\n\n# Start the streaming query\nquery = parsed_stream.writeStream \\\n    .foreachBatch(process_cdc_batch) \\\n    .option(\"checkpointLocation\", checkpoint_path) \\\n    .start()\n\n# Wait for the query to terminate\nquery.awaitTermination()\n</code></pre>"},{"location":"code-examples/delta-lake/cdc/change-data-capture/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Enable Change Data Feed Proactively: Enable it on tables where you anticipate needing change tracking.</p> </li> <li> <p>Optimize for Write Performance: When implementing CDC patterns that involve frequent updates:    <pre><code>spark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\", \"true\")\nspark.conf.set(\"spark.databricks.delta.autoCompact.enabled\", \"true\")\n</code></pre></p> </li> <li> <p>Set Appropriate Retention Period: Configure the change data feed retention period:    <pre><code>spark.sql(f\"\"\"\nALTER TABLE delta.`{delta_table_path}`\nSET TBLPROPERTIES (delta.logRetentionDuration = '30 days')\n\"\"\")\n</code></pre></p> </li> <li> <p>Consider Partitioning: Partition your data appropriately to improve CDC query performance:    <pre><code>df.write \\\n  .format(\"delta\") \\\n  .partitionBy(\"year\", \"month\") \\\n  .save(delta_table_path)\n</code></pre></p> </li> <li> <p>Optimize After Large CDC Operations: Run OPTIMIZE after large CDC operations:    <pre><code>spark.sql(f\"OPTIMIZE delta.`{delta_table_path}`\")\n</code></pre></p> </li> </ol>"},{"location":"code-examples/delta-lake/cdc/change-data-capture/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"code-examples/delta-lake/cdc/change-data-capture/#issue-change-data-feed-is-not-capturing-changes","title":"Issue: Change Data Feed is not capturing changes","text":"<p>Solution: Verify that Change Data Feed is enabled and that you are reading with the correct version range.</p>"},{"location":"code-examples/delta-lake/cdc/change-data-capture/#issue-performance-degradation-with-large-change-volumes","title":"Issue: Performance degradation with large change volumes","text":"<p>Solution:  - Use appropriate partitioning - Implement incremental processing with smaller batch sizes - Consider compaction after large change operations</p>"},{"location":"code-examples/delta-lake/cdc/change-data-capture/#issue-duplicate-records-in-cdc-processing","title":"Issue: Duplicate records in CDC processing","text":"<p>Solution: Implement idempotent operations and use checkpoints to ensure exactly-once processing.</p>"},{"location":"code-examples/delta-lake/cdc/change-data-capture/#related-links","title":"Related Links","text":"<ul> <li>Azure Synapse Analytics documentation</li> <li>Delta Lake Change Data Feed documentation</li> <li>Debezium documentation</li> </ul>"},{"location":"code-examples/delta-lake/ingestion/auto-loader/","title":"Auto Loader for Delta Lake in Azure Synapse Analytics","text":"<p>Home &gt; Code Examples &gt; Delta Lake &gt; Auto Loader</p> <p>This guide provides detailed examples for using Auto Loader with Azure Synapse Analytics to efficiently ingest data into Delta Lake tables.</p>"},{"location":"code-examples/delta-lake/ingestion/auto-loader/#what-is-auto-loader","title":"What is Auto Loader?","text":"<p>Auto Loader provides an efficient way to incrementally process new files as they arrive in Azure Storage without having to list or reprocess the entire directory. It uses Azure Storage change feed notifications to efficiently identify new files.</p>"},{"location":"code-examples/delta-lake/ingestion/auto-loader/#prerequisites","title":"Prerequisites","text":"<ul> <li>Azure Synapse Analytics workspace</li> <li>Storage account with a container for data ingestion</li> <li>Appropriate permissions and access to Azure resources</li> </ul>"},{"location":"code-examples/delta-lake/ingestion/auto-loader/#basic-auto-loader-example","title":"Basic Auto Loader Example","text":"<pre><code># Import required libraries\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import current_timestamp\n\n# Create Spark session\nspark = SparkSession.builder \\\n    .appName(\"Auto Loader Example\") \\\n    .getOrCreate()\n\n# Source and destination paths\nsource_path = \"abfss://container@storage.dfs.core.windows.net/raw-data/\"\ncheckpoint_path = \"abfss://container@storage.dfs.core.windows.net/checkpoints/autoloader/\"\ndestination_path = \"abfss://container@storage.dfs.core.windows.net/delta/table/\"\n\n# Use Auto Loader to load data\ndf = spark.readStream \\\n    .format(\"cloudFiles\") \\\n    .option(\"cloudFiles.format\", \"csv\") \\\n    .option(\"cloudFiles.schemaLocation\", checkpoint_path) \\\n    .option(\"cloudFiles.inferColumnTypes\", \"true\") \\\n    .option(\"header\", \"true\") \\\n    .load(source_path)\n\n# Add ingestion timestamp\ndf = df.withColumn(\"ingestion_time\", current_timestamp())\n\n# Write to Delta table\nquery = df.writeStream \\\n    .format(\"delta\") \\\n    .option(\"checkpointLocation\", checkpoint_path) \\\n    .outputMode(\"append\") \\\n    .start(destination_path)\n\n# Wait for the query to terminate\nquery.awaitTermination()\n</code></pre>"},{"location":"code-examples/delta-lake/ingestion/auto-loader/#advanced-auto-loader-configuration","title":"Advanced Auto Loader Configuration","text":""},{"location":"code-examples/delta-lake/ingestion/auto-loader/#schema-evolution-with-auto-loader","title":"Schema Evolution with Auto Loader","text":"<pre><code># Import required libraries\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import current_timestamp, input_file_name\n\n# Create Spark session\nspark = SparkSession.builder \\\n    .appName(\"Auto Loader with Schema Evolution\") \\\n    .getOrCreate()\n\n# Source and destination paths\nsource_path = \"abfss://container@storage.dfs.core.windows.net/raw-data/\"\ncheckpoint_path = \"abfss://container@storage.dfs.core.windows.net/checkpoints/autoloader-schema-evolution/\"\ndestination_path = \"abfss://container@storage.dfs.core.windows.net/delta/evolved-table/\"\n\n# Use Auto Loader with schema evolution\ndf = spark.readStream \\\n    .format(\"cloudFiles\") \\\n    .option(\"cloudFiles.format\", \"json\") \\\n    .option(\"cloudFiles.schemaLocation\", checkpoint_path) \\\n    .option(\"cloudFiles.inferColumnTypes\", \"true\") \\\n    .option(\"cloudFiles.schemaEvolutionMode\", \"addNewColumns\") \\\n    .load(source_path)\n\n# Add metadata columns\ndf = df.withColumn(\"ingestion_time\", current_timestamp()) \\\n       .withColumn(\"source_file\", input_file_name())\n\n# Write to Delta table with schema evolution enabled\nquery = df.writeStream \\\n    .format(\"delta\") \\\n    .option(\"checkpointLocation\", checkpoint_path) \\\n    .option(\"mergeSchema\", \"true\") \\\n    .outputMode(\"append\") \\\n    .start(destination_path)\n\n# Wait for the query to terminate\nquery.awaitTermination()\n</code></pre>"},{"location":"code-examples/delta-lake/ingestion/auto-loader/#partition-management-with-auto-loader","title":"Partition Management with Auto Loader","text":"<pre><code># Import required libraries\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import year, month, dayofmonth, to_date, col\n\n# Create Spark session\nspark = SparkSession.builder \\\n    .appName(\"Auto Loader with Partitioning\") \\\n    .getOrCreate()\n\n# Source and destination paths\nsource_path = \"abfss://container@storage.dfs.core.windows.net/raw-data/\"\ncheckpoint_path = \"abfss://container@storage.dfs.core.windows.net/checkpoints/autoloader-partitioned/\"\ndestination_path = \"abfss://container@storage.dfs.core.windows.net/delta/partitioned-table/\"\n\n# Use Auto Loader to load data\ndf = spark.readStream \\\n    .format(\"cloudFiles\") \\\n    .option(\"cloudFiles.format\", \"parquet\") \\\n    .option(\"cloudFiles.schemaLocation\", checkpoint_path) \\\n    .load(source_path)\n\n# Assuming the data has a 'date' column, extract partitioning columns\ndf = df.withColumn(\"date\", to_date(col(\"date\"))) \\\n       .withColumn(\"year\", year(col(\"date\"))) \\\n       .withColumn(\"month\", month(col(\"date\"))) \\\n       .withColumn(\"day\", dayofmonth(col(\"date\")))\n\n# Write to Delta table with partitioning\nquery = df.writeStream \\\n    .format(\"delta\") \\\n    .option(\"checkpointLocation\", checkpoint_path) \\\n    .partitionBy(\"year\", \"month\", \"day\") \\\n    .outputMode(\"append\") \\\n    .start(destination_path)\n\n# Wait for the query to terminate\nquery.awaitTermination()\n</code></pre>"},{"location":"code-examples/delta-lake/ingestion/auto-loader/#optimization-strategies-for-auto-loader","title":"Optimization Strategies for Auto Loader","text":""},{"location":"code-examples/delta-lake/ingestion/auto-loader/#trigger-based-processing","title":"Trigger-Based Processing","text":"<pre><code># Import required libraries\nfrom pyspark.sql import SparkSession\n\n# Create Spark session\nspark = SparkSession.builder \\\n    .appName(\"Auto Loader with Trigger\") \\\n    .getOrCreate()\n\n# Source and destination paths\nsource_path = \"abfss://container@storage.dfs.core.windows.net/raw-data/\"\ncheckpoint_path = \"abfss://container@storage.dfs.core.windows.net/checkpoints/autoloader-trigger/\"\ndestination_path = \"abfss://container@storage.dfs.core.windows.net/delta/trigger-table/\"\n\n# Use Auto Loader to load data\ndf = spark.readStream \\\n    .format(\"cloudFiles\") \\\n    .option(\"cloudFiles.format\", \"csv\") \\\n    .option(\"cloudFiles.schemaLocation\", checkpoint_path) \\\n    .option(\"cloudFiles.maxFilesPerTrigger\", 1000) \\\n    .option(\"header\", \"true\") \\\n    .load(source_path)\n\n# Write to Delta table with trigger\nquery = df.writeStream \\\n    .format(\"delta\") \\\n    .option(\"checkpointLocation\", checkpoint_path) \\\n    .trigger(processingTime=\"5 minutes\") \\\n    .outputMode(\"append\") \\\n    .start(destination_path)\n\n# Wait for the query to terminate\nquery.awaitTermination()\n</code></pre>"},{"location":"code-examples/delta-lake/ingestion/auto-loader/#cost-optimized-auto-loader","title":"Cost-Optimized Auto Loader","text":"<pre><code># Import required libraries\nfrom pyspark.sql import SparkSession\n\n# Create Spark session\nspark = SparkSession.builder \\\n    .appName(\"Cost-Optimized Auto Loader\") \\\n    .getOrCreate()\n\n# Source and destination paths\nsource_path = \"abfss://container@storage.dfs.core.windows.net/raw-data/\"\ncheckpoint_path = \"abfss://container@storage.dfs.core.windows.net/checkpoints/autoloader-cost-optimized/\"\ndestination_path = \"abfss://container@storage.dfs.core.windows.net/delta/cost-optimized-table/\"\n\n# Use Auto Loader with optimized configuration\ndf = spark.readStream \\\n    .format(\"cloudFiles\") \\\n    .option(\"cloudFiles.format\", \"parquet\") \\\n    .option(\"cloudFiles.schemaLocation\", checkpoint_path) \\\n    .option(\"cloudFiles.maxFilesPerTrigger\", 100) \\\n    .option(\"cloudFiles.maxFileAge\", \"7d\") \\\n    .option(\"cloudFiles.useNotifications\", \"true\") \\\n    .load(source_path)\n\n# Write to Delta table with optimized configuration\nquery = df.writeStream \\\n    .format(\"delta\") \\\n    .option(\"checkpointLocation\", checkpoint_path) \\\n    .option(\"maxRecordsPerFile\", 1000000) \\\n    .option(\"optimizeWrite\", \"true\") \\\n    .trigger(processingTime=\"15 minutes\") \\\n    .outputMode(\"append\") \\\n    .start(destination_path)\n\n# Wait for the query to terminate\nquery.awaitTermination()\n</code></pre>"},{"location":"code-examples/delta-lake/ingestion/auto-loader/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Schema Inference: Use schema inference for development but consider providing an explicit schema in production for better control.</p> </li> <li> <p>Checkpoint Management: Always set a checkpoint location to keep track of which files have been processed.</p> </li> <li> <p>Error Handling: Add error handling options to handle corrupted files:    <pre><code>.option(\"cloudFiles.schemaLocation\", checkpoint_path)\n.option(\"cloudFiles.rescuedDataColumn\", \"_rescued_data\")\n</code></pre></p> </li> <li> <p>Resource Allocation: Adjust <code>maxFilesPerTrigger</code> based on your cluster's processing capacity.</p> </li> <li> <p>Notification Mode: Use notification mode when available for more efficient file discovery:    <pre><code>.option(\"cloudFiles.useNotifications\", \"true\")\n</code></pre></p> </li> </ol>"},{"location":"code-examples/delta-lake/ingestion/auto-loader/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"code-examples/delta-lake/ingestion/auto-loader/#issue-files-are-not-being-processed","title":"Issue: Files are not being processed","text":"<p>Solution: Check if the service principal has appropriate permissions on the storage account.</p>"},{"location":"code-examples/delta-lake/ingestion/auto-loader/#issue-schema-mismatch-errors","title":"Issue: Schema mismatch errors","text":"<p>Solution: Enable schema evolution with <code>mergeSchema</code> and <code>cloudFiles.schemaEvolutionMode</code>.</p>"},{"location":"code-examples/delta-lake/ingestion/auto-loader/#issue-performance-bottlenecks","title":"Issue: Performance bottlenecks","text":"<p>Solution:  - Increase parallelism with <code>spark.sql.shuffle.partitions</code> - Use efficient file formats like Parquet - Optimize file sizes (aim for 128MB to 1GB)</p>"},{"location":"code-examples/delta-lake/ingestion/auto-loader/#related-links","title":"Related Links","text":"<ul> <li>Azure Synapse Analytics documentation</li> <li>Delta Lake documentation</li> <li>Auto Loader performance tuning</li> </ul>"},{"location":"code-examples/delta-lake/optimization/table-optimization/","title":"Delta Table Optimization in Azure Synapse Analytics","text":"<p>Home &gt; Code Examples &gt; Delta Lake &gt; Table Optimization</p> <p>This guide provides detailed examples for optimizing Delta Lake tables in Azure Synapse Analytics to improve query performance and reduce costs.</p>"},{"location":"code-examples/delta-lake/optimization/table-optimization/#introduction-to-delta-table-optimization","title":"Introduction to Delta Table Optimization","text":"<p>Delta Lake tables can accumulate many small files over time, especially with streaming or incremental data loads. Optimization techniques help maintain performance by compacting small files and optimizing data layout.</p>"},{"location":"code-examples/delta-lake/optimization/table-optimization/#prerequisites","title":"Prerequisites","text":"<ul> <li>Azure Synapse Analytics workspace</li> <li>Storage account with a Delta Lake table</li> <li>Appropriate permissions to run Spark jobs</li> </ul>"},{"location":"code-examples/delta-lake/optimization/table-optimization/#core-optimization-commands","title":"Core Optimization Commands","text":""},{"location":"code-examples/delta-lake/optimization/table-optimization/#optimize-command","title":"OPTIMIZE Command","text":"<p>The <code>OPTIMIZE</code> command compacts small files into larger ones for better read performance:</p> <pre><code># Import required libraries\nfrom pyspark.sql import SparkSession\n\n# Create Spark session with Delta Lake support\nspark = SparkSession.builder \\\n    .appName(\"Delta Optimization Example\") \\\n    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n    .getOrCreate()\n\n# Define Delta table path\ndelta_table_path = \"abfss://container@storage.dfs.core.windows.net/delta/sales_table/\"\n\n# Run basic OPTIMIZE command\nspark.sql(f\"OPTIMIZE delta.`{delta_table_path}`\")\n\n# Run OPTIMIZE with Z-ORDER for better data clustering\nspark.sql(f\"OPTIMIZE delta.`{delta_table_path}` ZORDER BY (date, region, product_id)\")\n\n# Run OPTIMIZE with custom file size target\nspark.sql(f\"\"\"\nOPTIMIZE delta.`{delta_table_path}`\nWHERE date &gt;= '2023-01-01'\n\"\"\")\n</code></pre>"},{"location":"code-examples/delta-lake/optimization/table-optimization/#vacuum-command","title":"VACUUM Command","text":"<p>The <code>VACUUM</code> command removes files that are no longer needed by a Delta table:</p> <pre><code># Set retention period (default is 7 days)\nspark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")\nspark.conf.set(\"spark.databricks.delta.vacuum.logging.enabled\", \"true\")\n\n# List files that would be deleted (dry run)\nspark.sql(f\"VACUUM delta.`{delta_table_path}` RETAIN 168 HOURS DRY RUN\")\n\n# Actually remove files older than retention period\nspark.sql(f\"VACUUM delta.`{delta_table_path}` RETAIN 168 HOURS\")\n\n# Run VACUUM with shorter retention (use with caution)\nspark.sql(f\"VACUUM delta.`{delta_table_path}` RETAIN 24 HOURS\")\n</code></pre>"},{"location":"code-examples/delta-lake/optimization/table-optimization/#advanced-optimization-strategies","title":"Advanced Optimization Strategies","text":""},{"location":"code-examples/delta-lake/optimization/table-optimization/#scheduled-optimization-with-automated-workflows","title":"Scheduled Optimization with Automated Workflows","text":"<pre><code># Import required libraries\nfrom pyspark.sql import SparkSession\nfrom datetime import datetime\n\n# Create Spark session\nspark = SparkSession.builder \\\n    .appName(\"Automated Delta Optimization\") \\\n    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n    .getOrCreate()\n\n# Define logging function\ndef log_optimization(delta_path, optimization_type, start_time):\n    end_time = datetime.now()\n    duration = (end_time - start_time).total_seconds()\n\n    log_data = [(delta_path, optimization_type, start_time.isoformat(), \n                end_time.isoformat(), duration)]\n\n    schema = [\"table_path\", \"operation\", \"start_time\", \"end_time\", \"duration_seconds\"]\n    log_df = spark.createDataFrame(log_data, schema)\n\n    # Write to optimization log table\n    log_df.write \\\n        .format(\"delta\") \\\n        .mode(\"append\") \\\n        .save(\"abfss://container@storage.dfs.core.windows.net/logs/optimization_history/\")\n\n# Get list of tables to optimize\ntables_to_optimize = [\n    \"abfss://container@storage.dfs.core.windows.net/delta/sales_table/\",\n    \"abfss://container@storage.dfs.core.windows.net/delta/customer_table/\",\n    \"abfss://container@storage.dfs.core.windows.net/delta/product_table/\"\n]\n\n# Perform optimization for each table\nfor table_path in tables_to_optimize:\n    # Run OPTIMIZE\n    start_time = datetime.now()\n    spark.sql(f\"OPTIMIZE delta.`{table_path}`\")\n    log_optimization(table_path, \"OPTIMIZE\", start_time)\n\n    # Run VACUUM (if table is old enough)\n    start_time = datetime.now()\n    spark.sql(f\"VACUUM delta.`{table_path}` RETAIN 168 HOURS\")\n    log_optimization(table_path, \"VACUUM\", start_time)\n</code></pre>"},{"location":"code-examples/delta-lake/optimization/table-optimization/#partition-aware-optimization","title":"Partition-Aware Optimization","text":"<p>Optimize specific partitions for large tables:</p> <pre><code># Import required libraries\nfrom pyspark.sql import SparkSession\nfrom delta.tables import DeltaTable\nimport pyspark.sql.functions as F\n\n# Create Spark session\nspark = SparkSession.builder \\\n    .appName(\"Partition-Aware Optimization\") \\\n    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n    .getOrCreate()\n\n# Define Delta table path\ndelta_table_path = \"abfss://container@storage.dfs.core.windows.net/delta/large_partitioned_table/\"\n\n# Load Delta table\ndelta_table = DeltaTable.forPath(spark, delta_table_path)\n\n# Get list of partitions\npartitions_df = spark.sql(f\"SHOW PARTITIONS delta.`{delta_table_path}`\")\npartitions = [row.partition for row in partitions_df.collect()]\n\n# Get file counts for each partition\nfile_stats = []\nfor partition in partitions:\n    # Extract partition values (assuming year/month partitioning)\n    # Example partition format: \"year=2023/month=01\"\n    year = partition.split('/')[0].split('=')[1]\n    month = partition.split('/')[1].split('=')[1]\n\n    # Count files in the partition\n    files_df = spark.sql(f\"\"\"\n        SELECT COUNT(*) as file_count\n        FROM delta.`{delta_table_path}`\n        WHERE year = {year} AND month = {month}\n    \"\"\")\n\n    file_count = files_df.first()[0]\n    file_stats.append((partition, file_count, year, month))\n\n# Sort partitions by file count (optimize those with most files first)\nfile_stats.sort(key=lambda x: x[1], reverse=True)\n\n# Optimize partitions with more than 100 files\nfor partition, file_count, year, month in file_stats:\n    if file_count &gt; 100:\n        print(f\"Optimizing partition {partition} with {file_count} files\")\n        spark.sql(f\"\"\"\n            OPTIMIZE delta.`{delta_table_path}`\n            WHERE year = {year} AND month = {month}\n            ZORDER BY (customer_id, product_id)\n        \"\"\")\n</code></pre>"},{"location":"code-examples/delta-lake/optimization/table-optimization/#data-skipping-with-z-order","title":"Data Skipping with Z-ORDER","text":"<p>Optimize table for specific query patterns using Z-ORDER:</p> <pre><code># Import required libraries\nfrom pyspark.sql import SparkSession\nimport time\n\n# Create Spark session\nspark = SparkSession.builder \\\n    .appName(\"Z-ORDER Optimization\") \\\n    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n    .getOrCreate()\n\n# Define Delta table path\ndelta_table_path = \"abfss://container@storage.dfs.core.windows.net/delta/query_table/\"\n\n# Define test query\ntest_query = f\"\"\"\n    SELECT COUNT(*) \n    FROM delta.`{delta_table_path}`\n    WHERE region = 'Europe' AND transaction_date BETWEEN '2023-01-01' AND '2023-01-31'\n\"\"\"\n\n# Run query before optimization and measure time\nstart_time = time.time()\nspark.sql(test_query).show()\nbefore_time = time.time() - start_time\nprint(f\"Query time before optimization: {before_time:.2f} seconds\")\n\n# Run OPTIMIZE with Z-ORDER on query columns\nspark.sql(f\"\"\"\n    OPTIMIZE delta.`{delta_table_path}`\n    ZORDER BY (region, transaction_date)\n\"\"\")\n\n# Run the same query after optimization\nstart_time = time.time()\nspark.sql(test_query).show()\nafter_time = time.time() - start_time\nprint(f\"Query time after optimization: {after_time:.2f} seconds\")\nprint(f\"Performance improvement: {(before_time - after_time) / before_time * 100:.2f}%\")\n</code></pre>"},{"location":"code-examples/delta-lake/optimization/table-optimization/#delta-cache-optimization","title":"Delta Cache Optimization","text":"<p>Leverage Delta Lake caching for frequently accessed data:</p> <pre><code># Import required libraries\nfrom pyspark.sql import SparkSession\nimport time\n\n# Create Spark session with Delta Lake and cache support\nspark = SparkSession.builder \\\n    .appName(\"Delta Cache Optimization\") \\\n    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n    .config(\"spark.databricks.io.cache.enabled\", \"true\") \\\n    .getOrCreate()\n\n# Define Delta table path\ndelta_table_path = \"abfss://container@storage.dfs.core.windows.net/delta/frequently_accessed_table/\"\n\n# Query without cache priming\nstart_time = time.time()\nresult = spark.sql(f\"\"\"\n    SELECT region, product_category, SUM(sales_amount) AS total_sales\n    FROM delta.`{delta_table_path}`\n    GROUP BY region, product_category\n    ORDER BY total_sales DESC\n    LIMIT 10\n\"\"\").collect()\nfirst_query_time = time.time() - start_time\n\n# The second execution should use cache\nstart_time = time.time()\nresult = spark.sql(f\"\"\"\n    SELECT region, product_category, SUM(sales_amount) AS total_sales\n    FROM delta.`{delta_table_path}`\n    GROUP BY region, product_category\n    ORDER BY total_sales DESC\n    LIMIT 10\n\"\"\").collect()\nsecond_query_time = time.time() - start_time\n\nprint(f\"First query time: {first_query_time:.2f} seconds\")\nprint(f\"Second query time (cached): {second_query_time:.2f} seconds\")\nprint(f\"Cache speedup: {first_query_time / second_query_time:.2f}x\")\n\n# Cache specific columns for better memory utilization\nspark.sql(f\"\"\"\n    CACHE SELECT region, product_category, sales_amount\n    FROM delta.`{delta_table_path}`\n    WHERE transaction_date &gt;= '2023-01-01'\n\"\"\")\n</code></pre>"},{"location":"code-examples/delta-lake/optimization/table-optimization/#monitoring-and-maintaining-delta-tables","title":"Monitoring and Maintaining Delta Tables","text":""},{"location":"code-examples/delta-lake/optimization/table-optimization/#table-history-and-statistics","title":"Table History and Statistics","text":"<pre><code># Import required libraries\nfrom pyspark.sql import SparkSession\nfrom delta.tables import DeltaTable\n\n# Create Spark session\nspark = SparkSession.builder \\\n    .appName(\"Delta Table Monitoring\") \\\n    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n    .getOrCreate()\n\n# Define Delta table path\ndelta_table_path = \"abfss://container@storage.dfs.core.windows.net/delta/monitored_table/\"\n\n# Check table history\nhistory_df = spark.sql(f\"DESCRIBE HISTORY delta.`{delta_table_path}`\")\nhistory_df.show(10, truncate=False)\n\n# Get table details and statistics\ndetails_df = spark.sql(f\"DESCRIBE DETAIL delta.`{delta_table_path}`\")\ndetails_df.show(truncate=False)\n\n# Get file sizes and distribution\nfiles_df = spark.sql(f\"\"\"\n    DESCRIBE DETAIL delta.`{delta_table_path}`\n\"\"\").select(\"location\").first()\n\ntable_location = files_df[\"location\"]\n\n# List all files in the Delta table directory\nfiles = spark.sparkContext.wholeTextFiles(f\"{table_location}/[^_]*\").keys().collect()\n\n# Convert to DataFrame for analysis\nfiles_info = [(f.split(\"/\")[-1], f) for f in files if f.endswith(\".parquet\")]\nfile_df = spark.createDataFrame(files_info, [\"filename\", \"path\"])\n\n# Show file stats\nprint(f\"Total number of files: {file_df.count()}\")\n\n# Analyze file size distribution\nspark.sql(f\"\"\"\n    CREATE OR REPLACE TEMPORARY VIEW delta_files AS\n    SELECT \n        input_file_name() AS file_path,\n        COUNT(*) AS record_count\n    FROM delta.`{delta_table_path}`\n    GROUP BY input_file_name()\n\"\"\")\n\nspark.sql(\"\"\"\n    SELECT \n        percentile_approx(record_count, 0.5) AS median_records_per_file,\n        MIN(record_count) AS min_records,\n        MAX(record_count) AS max_records,\n        AVG(record_count) AS avg_records\n    FROM delta_files\n\"\"\").show()\n</code></pre>"},{"location":"code-examples/delta-lake/optimization/table-optimization/#auto-optimize-configuration","title":"Auto-Optimize Configuration","text":"<pre><code># Import required libraries\nfrom pyspark.sql import SparkSession\nfrom delta.tables import DeltaTable\n\n# Create Spark session with auto-optimize enabled\nspark = SparkSession.builder \\\n    .appName(\"Delta Auto Optimization\") \\\n    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n    .config(\"spark.databricks.delta.optimizeWrite.enabled\", \"true\") \\\n    .config(\"spark.databricks.delta.autoCompact.enabled\", \"true\") \\\n    .getOrCreate()\n\n# Define Delta table path\ndelta_table_path = \"abfss://container@storage.dfs.core.windows.net/delta/auto_optimized_table/\"\n\n# Create a new table with auto-optimize properties\nspark.sql(f\"\"\"\nCREATE TABLE IF NOT EXISTS auto_optimized_table\nUSING DELTA\nLOCATION '{delta_table_path}'\nTBLPROPERTIES (\n  delta.autoOptimize.optimizeWrite = true,\n  delta.autoOptimize.autoCompact = true\n)\n\"\"\")\n\n# For existing tables, you can set these properties:\nspark.sql(f\"\"\"\nALTER TABLE delta.`{delta_table_path}`\nSET TBLPROPERTIES (\n  delta.autoOptimize.optimizeWrite = true,\n  delta.autoOptimize.autoCompact = true\n)\n\"\"\")\n</code></pre>"},{"location":"code-examples/delta-lake/optimization/table-optimization/#performance-tuning-best-practices","title":"Performance Tuning Best Practices","text":""},{"location":"code-examples/delta-lake/optimization/table-optimization/#1-file-size-optimization","title":"1. File Size Optimization","text":"<p>Aim for file sizes between 128MB to 1GB:</p> <pre><code># Import required libraries\nfrom pyspark.sql import SparkSession\n\n# Create Spark session\nspark = SparkSession.builder \\\n    .appName(\"File Size Optimization\") \\\n    .config(\"spark.sql.files.maxPartitionBytes\", \"134217728\") # 128 MB\n    .config(\"spark.sql.shuffle.partitions\", \"200\")\n    .getOrCreate()\n\n# Configure write operation for optimal file sizes\ndf = spark.read.format(\"delta\").load(\"abfss://container@storage.dfs.core.windows.net/delta/input_table/\")\n\n# Write with target file size of ~128MB\ndf.repartition(200) \\\n    .write \\\n    .option(\"maxRecordsPerFile\", 500000) \\\n    .format(\"delta\") \\\n    .save(\"abfss://container@storage.dfs.core.windows.net/delta/optimized_table/\")\n</code></pre>"},{"location":"code-examples/delta-lake/optimization/table-optimization/#2-partitioning-strategy","title":"2. Partitioning Strategy","text":"<p>Create effective partitioning based on query patterns:</p> <pre><code># Import required libraries\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\n\n# Create Spark session\nspark = SparkSession.builder \\\n    .appName(\"Delta Partitioning Strategy\") \\\n    .getOrCreate()\n\n# Load data\ndf = spark.read.format(\"delta\").load(\"abfss://container@storage.dfs.core.windows.net/delta/source_table/\")\n\n# Add partitioning columns\ndf = df.withColumn(\"year\", F.year(\"transaction_date\")) \\\n       .withColumn(\"month\", F.month(\"transaction_date\"))\n\n# Write with partitioning\ndf.write \\\n    .format(\"delta\") \\\n    .partitionBy(\"year\", \"month\") \\\n    .save(\"abfss://container@storage.dfs.core.windows.net/delta/well_partitioned_table/\")\n\n# For time-series data with high cardinality, consider limiting partitions\ndf.write \\\n    .format(\"delta\") \\\n    .partitionBy(\"year\") \\\n    .save(\"abfss://container@storage.dfs.core.windows.net/delta/balanced_partitioned_table/\")\n</code></pre>"},{"location":"code-examples/delta-lake/optimization/table-optimization/#3-compact-metadata-with-delta-protocol-upgrades","title":"3. Compact Metadata with Delta Protocol Upgrades","text":"<pre><code># Import required libraries\nfrom pyspark.sql import SparkSession\n\n# Create Spark session\nspark = SparkSession.builder \\\n    .appName(\"Delta Protocol Upgrade\") \\\n    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n    .getOrCreate()\n\n# Define Delta table path\ndelta_table_path = \"abfss://container@storage.dfs.core.windows.net/delta/large_table/\"\n\n# Check current protocol version\nspark.sql(f\"\"\"\n    DESCRIBE DETAIL delta.`{delta_table_path}`\n\"\"\").select(\"protocol.*\").show()\n\n# Upgrade to latest protocol version for metadata improvements\nspark.sql(f\"\"\"\n    ALTER TABLE delta.`{delta_table_path}` \n    SET TBLPROPERTIES (delta.minReaderVersion = 2, delta.minWriterVersion = 5)\n\"\"\")\n\n# Verify upgrade\nspark.sql(f\"\"\"\n    DESCRIBE DETAIL delta.`{delta_table_path}`\n\"\"\").select(\"protocol.*\").show()\n</code></pre>"},{"location":"code-examples/delta-lake/optimization/table-optimization/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"code-examples/delta-lake/optimization/table-optimization/#issue-slow-query-performance-despite-optimization","title":"Issue: Slow query performance despite optimization","text":"<p>Solution: - Check data skew in partitions - Verify Z-ORDER columns match query predicates - Consider adjusting file sizes for your specific workload</p>"},{"location":"code-examples/delta-lake/optimization/table-optimization/#issue-vacuum-removing-files-that-are-still-needed","title":"Issue: VACUUM removing files that are still needed","text":"<p>Solution: - Use longer retention periods (7 days minimum recommended) - Always run with DRY RUN first - Ensure no long-running queries or operations are using old versions</p>"},{"location":"code-examples/delta-lake/optimization/table-optimization/#issue-out-of-memory-errors-during-optimize","title":"Issue: Out of memory errors during OPTIMIZE","text":"<p>Solution: - Optimize smaller partitions individually - Increase executor memory - Use bin-packing optimization instead of Z-ORDER for very large tables</p>"},{"location":"code-examples/delta-lake/optimization/table-optimization/#related-links","title":"Related Links","text":"<ul> <li>Azure Synapse Analytics documentation</li> <li>Delta Lake optimization documentation</li> <li>Performance tuning guide for Spark in Azure Synapse</li> </ul>"},{"location":"code-examples/serverless-sql/","title":"Serverless SQL Examples for Azure Synapse Analytics","text":"<p>Home &gt; Code Examples &gt; Serverless SQL</p> <p>This section provides examples and best practices for working with Serverless SQL pools in Azure Synapse Analytics. Serverless SQL pools allow you to query data directly from your data lake storage without the need for data movement or pre-loading.</p>"},{"location":"code-examples/serverless-sql/#available-examples","title":"Available Examples","text":""},{"location":"code-examples/serverless-sql/#performance-optimization","title":"Performance Optimization","text":"<ul> <li>Query Optimization - Techniques to optimize serverless SQL queries</li> <li>File format selection</li> <li>Column pruning</li> <li>Predicate pushdown</li> <li>Partition elimination</li> <li>External tables and statistics</li> <li>Resource management</li> </ul>"},{"location":"code-examples/serverless-sql/#coming-soon","title":"Coming Soon","text":"<ul> <li>External Tables Management - Best practices for creating and maintaining external tables</li> <li>Complex Query Patterns - Solutions for common analytical query scenarios</li> <li>Security and Access Control - Row-level security and column-level access</li> <li>Data Virtualization - Creating logical data warehouses using views and stored procedures</li> </ul>"},{"location":"code-examples/serverless-sql/#why-serverless-sql-in-azure-synapse","title":"Why Serverless SQL in Azure Synapse?","text":"<p>Serverless SQL pools in Azure Synapse Analytics offer several benefits:</p> <ol> <li>Pay-per-Query: Only pay for the data processed during query execution</li> <li>No Infrastructure Management: Eliminates the need to provision or scale resources</li> <li>Built-in Security: Seamless integration with Azure AD and role-based access control</li> <li>Data Exploration: Efficiently query and analyze data in various formats</li> <li>Integration with BI Tools: Connect with PowerBI and other visualization tools</li> </ol>"},{"location":"code-examples/serverless-sql/#serverless-sql-architecture-patterns","title":"Serverless SQL Architecture Patterns","text":"<p>Serverless SQL in Azure Synapse Analytics supports several architecture patterns:</p> <ol> <li>Data Lake Query Engine: Direct querying of files in storage</li> <li>Data Virtualization Layer: Creating views and stored procedures over external data</li> <li>Hybrid Architecture: Combining serverless SQL with dedicated SQL pools</li> <li>Logical Data Warehouse: Federated queries across multiple data sources</li> </ol>"},{"location":"code-examples/serverless-sql/#related-resources","title":"Related Resources","text":"<ul> <li>Azure Synapse Analytics Documentation</li> <li>Serverless SQL Pool Best Practices</li> <li>T-SQL Reference for Serverless SQL Pools</li> </ul>"},{"location":"code-examples/serverless-sql/query-optimization/","title":"Serverless SQL Query Optimization in Azure Synapse Analytics","text":"<p>Home &gt; Code Examples &gt; Serverless SQL &gt; Query Optimization</p> <p>This guide provides detailed examples for optimizing SQL queries in Azure Synapse Serverless SQL pools to improve performance and reduce costs.</p>"},{"location":"code-examples/serverless-sql/query-optimization/#introduction-to-serverless-sql-optimization","title":"Introduction to Serverless SQL Optimization","text":"<p>Azure Synapse Serverless SQL pools provide on-demand query processing for data in data lakes. Optimizing these queries is essential for reducing costs and improving query performance.</p>"},{"location":"code-examples/serverless-sql/query-optimization/#prerequisites","title":"Prerequisites","text":"<ul> <li>Azure Synapse Analytics workspace</li> <li>Storage account with data files (Parquet, CSV, JSON, etc.)</li> <li>Appropriate permissions to execute SQL queries</li> </ul>"},{"location":"code-examples/serverless-sql/query-optimization/#query-optimization-techniques","title":"Query Optimization Techniques","text":""},{"location":"code-examples/serverless-sql/query-optimization/#1-file-format-selection","title":"1. File Format Selection","text":"<p>One of the most important optimization factors is choosing the right file format:</p> <pre><code>-- Query against Parquet (recommended) - most efficient\nSELECT TOP 100 *\nFROM OPENROWSET(\n    BULK 'https://synapseexampledata.blob.core.windows.net/data/parquet/sales_data/*.parquet',\n    FORMAT = 'PARQUET'\n) AS [sales];\n\n-- Query against CSV - less efficient\nSELECT TOP 100 *\nFROM OPENROWSET(\n    BULK 'https://synapseexampledata.blob.core.windows.net/data/csv/sales_data/*.csv',\n    FORMAT = 'CSV',\n    PARSER_VERSION = '2.0',\n    HEADER_ROW = TRUE\n) AS [sales];\n\n-- Query against JSON - least efficient for large datasets\nSELECT TOP 100 *\nFROM OPENROWSET(\n    BULK 'https://synapseexampledata.blob.core.windows.net/data/json/sales_data/*.json',\n    FORMAT = 'CSV',\n    FIELDTERMINATOR = '0x0b',\n    FIELDQUOTE = '0x0b',\n    ROWTERMINATOR = '0x0b'\n) WITH (jsonDoc NVARCHAR(MAX)) AS [sales]\nCROSS APPLY OPENJSON(jsonDoc)\nWITH (\n    order_id INT,\n    customer_id INT,\n    product_id INT,\n    quantity INT,\n    price DECIMAL(10,2),\n    order_date DATE\n);\n</code></pre>"},{"location":"code-examples/serverless-sql/query-optimization/#2-column-pruning","title":"2. Column Pruning","text":"<p>Only select the columns you need to reduce data scanning:</p> <pre><code>-- Inefficient - scans all columns\nSELECT *\nFROM OPENROWSET(\n    BULK 'https://synapseexampledata.blob.core.windows.net/data/parquet/sales_data/*.parquet',\n    FORMAT = 'PARQUET'\n) AS [sales];\n\n-- Optimized - only scans necessary columns\nSELECT customer_id, SUM(price * quantity) AS total_spent\nFROM OPENROWSET(\n    BULK 'https://synapseexampledata.blob.core.windows.net/data/parquet/sales_data/*.parquet',\n    FORMAT = 'PARQUET'\n) AS [sales]\nGROUP BY customer_id\nORDER BY total_spent DESC;\n</code></pre>"},{"location":"code-examples/serverless-sql/query-optimization/#3-predicate-pushdown","title":"3. Predicate Pushdown","text":"<p>Utilize filter conditions that can be pushed down to storage:</p> <pre><code>-- Inefficient - filters after loading all data\nSELECT *\nFROM OPENROWSET(\n    BULK 'https://synapseexampledata.blob.core.windows.net/data/parquet/sales_data/*.parquet',\n    FORMAT = 'PARQUET'\n) AS [sales]\nWHERE YEAR(order_date) = 2023 AND MONTH(order_date) = 6;\n\n-- Optimized - uses predicate pushdown\nSELECT *\nFROM OPENROWSET(\n    BULK 'https://synapseexampledata.blob.core.windows.net/data/parquet/sales_data/*.parquet',\n    FORMAT = 'PARQUET'\n) AS [sales]\nWHERE order_date BETWEEN '2023-06-01' AND '2023-06-30';\n</code></pre>"},{"location":"code-examples/serverless-sql/query-optimization/#4-partition-elimination","title":"4. Partition Elimination","text":"<p>Leverage partitioned data for efficient queries:</p> <pre><code>-- Query against partitioned data\n-- Data is stored in a folder structure like: /year=2023/month=06/day=15/data.parquet\nSELECT *\nFROM OPENROWSET(\n    BULK 'https://synapseexampledata.blob.core.windows.net/data/parquet/sales_data/year=*/month=*/day=*/*.parquet',\n    FORMAT = 'PARQUET'\n) WITH (\n    order_id INT,\n    customer_id INT,\n    product_id INT,\n    quantity INT,\n    price DECIMAL(10,2),\n    order_date DATE,\n    year INT,\n    month INT,\n    day INT\n) AS [sales]\nWHERE year = 2023 AND month = 6;\n</code></pre>"},{"location":"code-examples/serverless-sql/query-optimization/#5-external-tables-for-better-performance","title":"5. External Tables for Better Performance","text":"<p>Create external tables with optimized statistics:</p> <pre><code>-- Create database for external tables\nCREATE DATABASE SalesData;\nGO\nUSE SalesData;\nGO\n\n-- Create external data source\nCREATE EXTERNAL DATA SOURCE ExampleDataSource\nWITH (\n    LOCATION = 'https://synapseexampledata.blob.core.windows.net/data/'\n);\nGO\n\n-- Create file format\nCREATE EXTERNAL FILE FORMAT ParquetFormat\nWITH (\n    FORMAT_TYPE = PARQUET,\n    DATA_COMPRESSION = 'org.apache.hadoop.io.compress.SnappyCodec'\n);\nGO\n\n-- Create external table\nCREATE EXTERNAL TABLE SalesTable (\n    order_id INT,\n    customer_id INT,\n    product_id INT,\n    quantity INT,\n    price DECIMAL(10,2),\n    order_date DATE,\n    year INT,\n    month INT,\n    day INT\n)\nWITH (\n    LOCATION = '/parquet/sales_data/',\n    DATA_SOURCE = ExampleDataSource,\n    FILE_FORMAT = ParquetFormat\n);\nGO\n\n-- Create statistics on the external table\nCREATE STATISTICS stat_customer_id ON SalesTable(customer_id);\nCREATE STATISTICS stat_order_date ON SalesTable(order_date);\nCREATE STATISTICS stat_product_id ON SalesTable(product_id);\nGO\n\n-- Query the external table with statistics\nSELECT \n    year,\n    month,\n    SUM(quantity * price) AS total_sales\nFROM SalesTable\nWHERE order_date BETWEEN '2023-01-01' AND '2023-12-31'\nGROUP BY year, month\nORDER BY year, month;\n</code></pre>"},{"location":"code-examples/serverless-sql/query-optimization/#advanced-optimization-techniques","title":"Advanced Optimization Techniques","text":""},{"location":"code-examples/serverless-sql/query-optimization/#1-query-plan-analysis","title":"1. Query Plan Analysis","text":"<p>Use the EXPLAIN command to analyze query plans:</p> <pre><code>-- View the query execution plan\nEXPLAIN\nSELECT \n    customer_id,\n    SUM(price * quantity) AS total_spent\nFROM OPENROWSET(\n    BULK 'https://synapseexampledata.blob.core.windows.net/data/parquet/sales_data/*.parquet',\n    FORMAT = 'PARQUET'\n) AS [sales]\nGROUP BY customer_id\nORDER BY total_spent DESC;\n</code></pre>"},{"location":"code-examples/serverless-sql/query-optimization/#2-optimizing-joins","title":"2. Optimizing Joins","text":"<p>Optimize joins by using the proper join type and join order:</p> <pre><code>-- Create customer external table\nCREATE EXTERNAL TABLE CustomerTable (\n    customer_id INT,\n    customer_name NVARCHAR(100),\n    customer_segment NVARCHAR(50),\n    customer_region NVARCHAR(50)\n)\nWITH (\n    LOCATION = '/parquet/customer_data/',\n    DATA_SOURCE = ExampleDataSource,\n    FILE_FORMAT = ParquetFormat\n);\n\n-- Create statistics on join columns\nCREATE STATISTICS stat_sales_customer_id ON SalesTable(customer_id);\nCREATE STATISTICS stat_customer_customer_id ON CustomerTable(customer_id);\n\n-- Inefficient join - larger table on left side\nSELECT \n    c.customer_name,\n    SUM(s.price * s.quantity) AS total_spent\nFROM SalesTable s\nLEFT JOIN CustomerTable c ON s.customer_id = c.customer_id\nGROUP BY c.customer_name\nORDER BY total_spent DESC;\n\n-- Optimized join - smaller table on left side\nSELECT \n    c.customer_name,\n    SUM(s.price * s.quantity) AS total_spent\nFROM CustomerTable c\nINNER JOIN SalesTable s ON c.customer_id = s.customer_id\nWHERE s.year = 2023\nGROUP BY c.customer_name\nORDER BY total_spent DESC;\n</code></pre>"},{"location":"code-examples/serverless-sql/query-optimization/#3-data-skew-handling","title":"3. Data Skew Handling","text":"<p>Address data skew with more granular partitioning or CETAS (Create External Table As Select):</p> <pre><code>-- Identify data skew\nSELECT \n    product_id,\n    COUNT(*) as row_count\nFROM SalesTable\nGROUP BY product_id\nORDER BY row_count DESC;\n\n-- Handle skew using CETAS for high-volume products\nCREATE EXTERNAL TABLE HighVolumeProducts\nWITH (\n    LOCATION = '/optimized/high_volume_products/',\n    DATA_SOURCE = ExampleDataSource,\n    FILE_FORMAT = ParquetFormat\n)\nAS\nSELECT *\nFROM SalesTable\nWHERE product_id IN (101, 202, 303); -- High volume product IDs\n\n-- Handle skew using CETAS for normal-volume products\nCREATE EXTERNAL TABLE NormalVolumeProducts\nWITH (\n    LOCATION = '/optimized/normal_volume_products/',\n    DATA_SOURCE = ExampleDataSource,\n    FILE_FORMAT = ParquetFormat\n)\nAS\nSELECT *\nFROM SalesTable\nWHERE product_id NOT IN (101, 202, 303); -- Exclude high volume product IDs\n\n-- Union the results when querying\nSELECT * FROM HighVolumeProducts\nUNION ALL\nSELECT * FROM NormalVolumeProducts;\n</code></pre>"},{"location":"code-examples/serverless-sql/query-optimization/#4-caching-with-materialized-views","title":"4. Caching with Materialized Views","text":"<p>Use materialized views for frequently accessed aggregated data:</p> <pre><code>-- Create materialized view\nCREATE EXTERNAL TABLE MonthlySalesSummary\nWITH (\n    LOCATION = '/optimized/monthly_sales_summary/',\n    DATA_SOURCE = ExampleDataSource,\n    FILE_FORMAT = ParquetFormat\n)\nAS\nSELECT \n    year,\n    month,\n    product_id,\n    SUM(quantity) AS total_quantity,\n    SUM(price * quantity) AS total_sales,\n    COUNT(DISTINCT order_id) AS order_count\nFROM SalesTable\nGROUP BY year, month, product_id;\n\n-- Query the materialized view\nSELECT \n    year,\n    month,\n    SUM(total_sales) AS monthly_revenue\nFROM MonthlySalesSummary\nWHERE year = 2023\nGROUP BY year, month\nORDER BY year, month;\n</code></pre>"},{"location":"code-examples/serverless-sql/query-optimization/#5-result-set-caching","title":"5. Result Set Caching","text":"<p>Enable result set caching for repeated queries:</p> <pre><code>-- Enable result set caching\nALTER DATABASE SalesData\nSET RESULT_SET_CACHING ON;\n\n-- Run a query that will be cached\nSELECT TOP 100 *\nFROM SalesTable\nWHERE year = 2023 AND month = 6;\n\n-- Run the same query again - will use the cached results\nSELECT TOP 100 *\nFROM SalesTable\nWHERE year = 2023 AND month = 6;\n</code></pre>"},{"location":"code-examples/serverless-sql/query-optimization/#working-with-different-file-types","title":"Working with Different File Types","text":""},{"location":"code-examples/serverless-sql/query-optimization/#1-csv-file-optimization","title":"1. CSV File Optimization","text":"<pre><code>-- Create external table for CSV with optimal settings\nCREATE EXTERNAL TABLE SalesCSV (\n    order_id INT,\n    customer_id INT,\n    product_id INT,\n    quantity INT,\n    price DECIMAL(10,2),\n    order_date DATE\n)\nWITH (\n    LOCATION = '/csv/sales_data/',\n    DATA_SOURCE = ExampleDataSource,\n    FILE_FORMAT = DELIMITED TEXT WITH (\n        FIELD_TERMINATOR = ',',\n        USE_TYPE_DEFAULT = TRUE,\n        STRING_DELIMITER = '\"',\n        DATE_FORMAT = 'yyyy-MM-dd',\n        PARSER_VERSION = '2.0',\n        FIRST_ROW = 2 -- Skip header row\n    )\n);\n\n-- Query with optimal file handling\nSELECT \n    YEAR(order_date) AS year,\n    MONTH(order_date) AS month,\n    SUM(price * quantity) AS total_sales\nFROM SalesCSV\nGROUP BY YEAR(order_date), MONTH(order_date)\nORDER BY year, month;\n</code></pre>"},{"location":"code-examples/serverless-sql/query-optimization/#2-json-file-optimization","title":"2. JSON File Optimization","text":"<pre><code>-- Create external table for JSON with optimal settings\nCREATE EXTERNAL TABLE SalesJSON\nWITH (\n    LOCATION = '/json/sales_data/',\n    DATA_SOURCE = ExampleDataSource,\n    FILE_FORMAT = JSON\n)\nAS\nSELECT \n    JSON_VALUE(jsonDoc, '$.order_id') AS order_id,\n    JSON_VALUE(jsonDoc, '$.customer_id') AS customer_id,\n    JSON_VALUE(jsonDoc, '$.product_id') AS product_id,\n    JSON_VALUE(jsonDoc, '$.quantity') AS quantity,\n    JSON_VALUE(jsonDoc, '$.price') AS price,\n    CONVERT(DATE, JSON_VALUE(jsonDoc, '$.order_date')) AS order_date\nFROM OPENROWSET(\n    BULK 'https://synapseexampledata.blob.core.windows.net/data/json/sales_data/*.json',\n    FORMAT = 'CSV',\n    FIELDTERMINATOR = '0x0b',\n    FIELDQUOTE = '0x0b',\n    ROWTERMINATOR = '0x0b'\n) WITH (jsonDoc NVARCHAR(MAX)) AS [sales];\n\n-- Query the optimized JSON table\nSELECT \n    YEAR(order_date) AS year,\n    MONTH(order_date) AS month,\n    SUM(CAST(quantity AS INT) * CAST(price AS DECIMAL(10,2))) AS total_sales\nFROM SalesJSON\nGROUP BY YEAR(order_date), MONTH(order_date)\nORDER BY year, month;\n</code></pre>"},{"location":"code-examples/serverless-sql/query-optimization/#resource-management-and-concurrency","title":"Resource Management and Concurrency","text":""},{"location":"code-examples/serverless-sql/query-optimization/#1-setting-appropriate-dwu","title":"1. Setting Appropriate DWU","text":"<pre><code>-- Check current resource utilization\nSELECT * FROM sys.dm_exec_requests;\n\n-- Check query resource consumption\nSELECT\n    r.request_id,\n    r.total_elapsed_time,\n    r.cpu_time,\n    r.reads,\n    r.writes,\n    r.logical_reads,\n    t.text\nFROM sys.dm_exec_requests r\nCROSS APPLY sys.dm_exec_sql_text(r.sql_handle) t\nWHERE r.session_id &gt; 50 -- Filter out system sessions\nORDER BY r.total_elapsed_time DESC;\n</code></pre>"},{"location":"code-examples/serverless-sql/query-optimization/#2-optimizing-for-concurrency","title":"2. Optimizing for Concurrency","text":"<p>Use query hints for better concurrency:</p> <pre><code>-- Add resource allocation hints\nSELECT \n    year,\n    month,\n    SUM(quantity * price) AS total_sales\nFROM SalesTable\nWHERE order_date BETWEEN '2023-01-01' AND '2023-12-31'\nGROUP BY year, month\nORDER BY year, month\nOPTION (LABEL = 'Monthly Sales Report', MAXDOP 4);\n</code></pre>"},{"location":"code-examples/serverless-sql/query-optimization/#cost-optimization-strategies","title":"Cost Optimization Strategies","text":""},{"location":"code-examples/serverless-sql/query-optimization/#1-reduce-data-scanning","title":"1. Reduce Data Scanning","text":"<pre><code>-- Use partitioning and file filtering\nSELECT *\nFROM OPENROWSET(\n    BULK 'https://synapseexampledata.blob.core.windows.net/data/parquet/sales_data/year=2023/month=06/day=*/*.parquet',\n    FORMAT = 'PARQUET'\n) AS [sales];\n</code></pre>"},{"location":"code-examples/serverless-sql/query-optimization/#2-query-monitoring-for-cost-control","title":"2. Query Monitoring for Cost Control","text":"<pre><code>-- Monitor data processed by queries\nSELECT\n    r.session_id,\n    r.request_id,\n    r.start_time,\n    r.end_time,\n    r.total_elapsed_time,\n    s.bytes_processed,\n    s.files_processed,\n    t.text\nFROM sys.dm_exec_requests r\nJOIN sys.dm_external_work_stats s ON r.request_id = s.request_id\nCROSS APPLY sys.dm_exec_sql_text(r.sql_handle) t\nORDER BY s.bytes_processed DESC;\n</code></pre>"},{"location":"code-examples/serverless-sql/query-optimization/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Use Parquet Format: Parquet provides the best performance for both storage and query efficiency.</p> </li> <li> <p>Apply Column Pruning: Always select only the columns you need instead of using SELECT *.</p> </li> <li> <p>Leverage Partitioning: Use partitioned data and partition elimination in your queries.</p> </li> <li> <p>Create Statistics: Create statistics on external tables for better query optimization.</p> </li> <li> <p>Use CETAS: Create External Table As Select (CETAS) to materialize intermediate results and optimize complex queries.</p> </li> <li> <p>Regular Monitoring: Monitor query performance and data processed to identify optimization opportunities.</p> </li> <li> <p>Proper File Sizes: Aim for file sizes between 100MB and 1GB for optimal performance.</p> </li> <li> <p>Minimize File Count: Reduce the number of small files by using CETAS to combine them.</p> </li> <li> <p>Enable Result Set Caching: For frequently executed identical queries.</p> </li> <li> <p>Use WITH Clause: Simplify complex queries with common table expressions.</p> </li> </ol>"},{"location":"code-examples/serverless-sql/query-optimization/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"code-examples/serverless-sql/query-optimization/#issue-slow-query-performance-on-csv-files","title":"Issue: Slow query performance on CSV files","text":"<p>Solution: Convert CSV to Parquet using CETAS for better performance.</p>"},{"location":"code-examples/serverless-sql/query-optimization/#issue-out-of-memory-errors","title":"Issue: Out of memory errors","text":"<p>Solution:</p> <ul> <li>Reduce the amount of data processed in a single query</li> <li>Implement proper partitioning</li> <li>Use CETAS for large intermediate results</li> </ul>"},{"location":"code-examples/serverless-sql/query-optimization/#issue-high-costs-due-to-excessive-data-scanning","title":"Issue: High costs due to excessive data scanning","text":"<p>Solution:</p> <ul> <li>Implement column pruning</li> <li>Use partitioning and partition elimination</li> <li>Convert to Parquet format</li> <li>Create smaller, focused external tables</li> </ul>"},{"location":"code-examples/serverless-sql/query-optimization/#related-links","title":"Related Links","text":"<ul> <li>Azure Synapse Analytics documentation</li> <li>Serverless SQL pool best practices</li> <li>Query optimization techniques for serverless SQL pools</li> </ul>"},{"location":"diagrams/","title":"Azure Synapse Analytics Architecture Diagrams","text":"<p>This section contains architecture diagrams for Azure Synapse Analytics components and workflows, focusing on Delta Lakehouse and Serverless SQL capabilities.</p>"},{"location":"diagrams/#delta-lakehouse-architecture","title":"Delta Lakehouse Architecture","text":"<p>Note: The diagram above shows the logical architecture of a Delta Lakehouse implementation in Azure Synapse Analytics.</p>"},{"location":"diagrams/#components-description","title":"Components Description","text":"<ol> <li>Azure Data Lake Storage Gen2 - Provides the foundation for storing Delta tables</li> <li>Azure Synapse Spark Pools - Executes Spark jobs for data processing</li> <li>Delta Lake - Provides ACID transactions, time travel, and schema enforcement</li> <li>Azure Synapse Pipeline - Orchestrates data movement and transformation</li> <li>Azure Synapse Serverless SQL - Provides SQL query capabilities over the Delta Lake</li> </ol>"},{"location":"diagrams/#serverless-sql-architecture","title":"Serverless SQL Architecture","text":"<p>Note: The diagram above illustrates the serverless SQL query architecture in Azure Synapse Analytics.</p>"},{"location":"diagrams/#components-description_1","title":"Components Description","text":"<ol> <li>Azure Synapse Serverless SQL Pool - On-demand SQL query service</li> <li>Storage Accounts - ADLS Gen2, Blob Storage, etc.</li> <li>File Formats - Support for Parquet, Delta, CSV, JSON</li> <li>Query Service - Distributed query processing engine</li> <li>Results - Query results available via JDBC/ODBC or direct export</li> </ol>"},{"location":"diagrams/#shared-metadata-architecture","title":"Shared Metadata Architecture","text":"<p>Note: The diagram above shows how metadata can be shared across different compute engines in Azure Synapse Analytics.</p>"},{"location":"diagrams/#components-description_2","title":"Components Description","text":"<ol> <li>Azure Synapse Workspace - Central workspace for all analytics services</li> <li>Metadata Services - Shared metadata layer</li> <li>Spark Metastore - Hive metastore for Spark</li> <li>SQL Metadata - SQL catalog and metadata</li> <li>Integration Runtime - Shared integration services</li> </ol>"},{"location":"diagrams/#data-flow-diagrams","title":"Data Flow Diagrams","text":""},{"location":"diagrams/#delta-lake-write-flow","title":"Delta Lake Write Flow","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Raw Data  \u2502\u2500\u2500\u2500\u2500\u25b6\u2502 Spark Pool \u2502\u2500\u2500\u2500\u2500\u25b6\u2502 Processing \u2502\u2500\u2500\u2500\u2500\u25b6\u2502 Delta Lake \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                               \u2502\n                                                               \u25bc\n                                                        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                                        \u2502  Metadata  \u2502\n                                                        \u2502   Update   \u2502\n                                                        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"diagrams/#serverless-sql-query-flow","title":"Serverless SQL Query Flow","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502    User    \u2502\u2500\u2500\u2500\u2500\u25b6\u2502  SQL Query \u2502\u2500\u2500\u2500\u2500\u25b6\u2502 Query Plan \u2502\u2500\u2500\u2500\u2500\u25b6\u2502   Query    \u2502\n\u2502   Query    \u2502     \u2502   Parser   \u2502     \u2502 Generation \u2502     \u2502 Execution  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                               \u2502\n                                                               \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Results   \u2502\u25c0\u2500\u2500\u2500\u2500\u2502   Result   \u2502\u25c0\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502 Data Source\u2502\n\u2502            \u2502     \u2502 Processing \u2502                       \u2502   Access   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"diagrams/#creating-architecture-diagrams","title":"Creating Architecture Diagrams","text":"<p>For production documentation, actual diagram images should be created using tools like:</p> <ol> <li>Microsoft Visio - Professional diagramming tool</li> <li>Draw.io - Free online diagramming tool</li> <li>Lucidchart - Collaborative diagramming</li> <li>Mermaid - Markdown-based diagramming</li> </ol>"},{"location":"diagrams/#diagram-standards","title":"Diagram Standards","text":"<p>When creating architecture diagrams for this documentation:</p> <ol> <li>Use Azure official icons for Azure services</li> <li>Maintain consistent color schemes</li> <li>Include clear labels for all components</li> <li>Provide a legend if multiple icon types are used</li> <li>Ensure high resolution for all exported images</li> <li>Use PNG format with transparent backgrounds</li> <li>Include both logical and physical architecture views when appropriate</li> </ol>"},{"location":"diagrams/#placeholder-notice","title":"Placeholder Notice","text":"<p>The diagrams referenced in this document need to be created and placed in this directory. The text diagrams are placeholders for actual visual diagrams that should follow the standards outlined above.</p>"},{"location":"reference/security-checklist/","title":"Azure Synapse Analytics Security Checklist","text":"<p>Home &gt; Reference &gt; Security Checklist</p> <p>This checklist provides a comprehensive set of security measures and best practices for securing your Azure Synapse Analytics environment.</p>"},{"location":"reference/security-checklist/#network-security","title":"Network Security","text":"<ul> <li> <p>[ ] Implement private endpoints for all Synapse workspace connections</p> </li> <li> <p>[ ] Configure managed virtual network for the Synapse workspace</p> </li> <li> <p>[ ] Set up IP firewall rules to restrict access</p> </li> <li> <p>[ ] Enable service endpoints for additional security</p> </li> <li> <p>[ ] Configure DNS settings for private endpoints</p> </li> <li> <p>[ ] Review and limit outbound network connectivity</p> </li> <li> <p>[ ] Implement network security groups (NSGs) where applicable</p> </li> </ul>"},{"location":"reference/security-checklist/#authentication-and-authorization","title":"Authentication and Authorization","text":"<ul> <li> <p>[ ] Enable Microsoft Entra ID (Azure AD) authentication for all services</p> </li> <li> <p>[ ] Configure multi-factor authentication for all admin accounts</p> </li> <li> <p>[ ] Set up managed identities for Synapse workspace resources</p> </li> <li> <p>[ ] Create custom RBAC roles with least privilege permissions</p> </li> <li> <p>[ ] Regularly review and audit role assignments</p> </li> <li> <p>[ ] Implement Privileged Identity Management for just-in-time access</p> </li> <li> <p>[ ] Use service principals with limited scope for automated processes</p> </li> <li> <p>[ ] Implement conditional access policies for sensitive workloads</p> </li> </ul>"},{"location":"reference/security-checklist/#data-protection","title":"Data Protection","text":"<ul> <li> <p>[ ] Enable encryption at rest for all storage accounts</p> </li> <li> <p>[ ] Use TLS 1.2+ for all data in transit</p> </li> <li> <p>[ ] Implement customer-managed keys for encryption</p> </li> <li> <p>[ ] Configure double encryption where available</p> </li> <li> <p>[ ] Implement column-level security for sensitive data</p> </li> <li> <p>[ ] Set up row-level security for multi-tenant scenarios</p> </li> <li> <p>[ ] Enable dynamic data masking for PII data</p> </li> <li> <p>[ ] Configure Azure Purview integration for data governance</p> </li> <li> <p>[ ] Implement data classification and labeling</p> </li> <li> <p>[ ] Set up data exfiltration protection</p> </li> </ul>"},{"location":"reference/security-checklist/#key-management","title":"Key Management","text":"<ul> <li> <p>[ ] Store all secrets in Azure Key Vault</p> </li> <li> <p>[ ] Rotate keys and secrets on a regular schedule</p> </li> <li> <p>[ ] Enable soft delete and purge protection for Key Vault</p> </li> <li> <p>[ ] Implement access policies with least privilege</p> </li> <li> <p>[ ] Set up Key Vault diagnostics logging</p> </li> <li> <p>[ ] Configure managed identities for Key Vault access</p> </li> <li> <p>[ ] Use separate key vaults for different environments</p> </li> </ul>"},{"location":"reference/security-checklist/#monitoring-and-logging","title":"Monitoring and Logging","text":"<ul> <li> <p>[ ] Enable diagnostic settings for all Synapse components</p> </li> <li> <p>[ ] Configure workspace diagnostic logs to be sent to Log Analytics</p> </li> <li> <p>[ ] Set up SQL audit logs for all SQL pools</p> </li> <li> <p>[ ] Configure Apache Spark application logs</p> </li> <li> <p>[ ] Create alert rules for security events</p> </li> <li> <p>[ ] Implement Azure Defender for SQL</p> </li> <li> <p>[ ] Set up Microsoft Sentinel integration for advanced threat protection</p> </li> <li> <p>[ ] Configure automated security responses for critical alerts</p> </li> <li> <p>[ ] Implement regular security assessments</p> </li> <li> <p>[ ] Review logs for unauthorized access attempts</p> </li> </ul>"},{"location":"reference/security-checklist/#compliance","title":"Compliance","text":"<ul> <li> <p>[ ] Document compliance requirements for your organization</p> </li> <li> <p>[ ] Configure appropriate compliance settings in Microsoft Purview</p> </li> <li> <p>[ ] Implement regular compliance audits</p> </li> <li> <p>[ ] Set up data residency requirements</p> </li> <li> <p>[ ] Configure retention policies for all data</p> </li> <li> <p>[ ] Implement privacy controls for personal data</p> </li> <li> <p>[ ] Set up regular compliance reporting</p> </li> <li> <p>[ ] Configure audit trails for regulatory requirements</p> </li> <li> <p>[ ] Document all security measures for compliance evidence</p> </li> </ul>"},{"location":"reference/security-checklist/#development-and-cicd-security","title":"Development and CI/CD Security","text":"<ul> <li> <p>[ ] Implement secure development lifecycle practices</p> </li> <li> <p>[ ] Set up code scanning for vulnerabilities</p> </li> <li> <p>[ ] Configure secret scanning in code repositories</p> </li> <li> <p>[ ] Implement secure CI/CD pipelines</p> </li> <li> <p>[ ] Set up separate environments for development, testing, and production</p> </li> <li> <p>[ ] Implement approval gates for production deployments</p> </li> <li> <p>[ ] Configure automated security testing in pipelines</p> </li> <li> <p>[ ] Use Infrastructure as Code with security best practices</p> </li> <li> <p>[ ] Implement regular security training for developers</p> </li> </ul>"},{"location":"reference/security-checklist/#serverless-sql-security","title":"Serverless SQL Security","text":"<ul> <li> <p>[ ] Configure appropriate access controls on storage accounts</p> </li> <li> <p>[ ] Set up managed identities for storage access</p> </li> <li> <p>[ ] Implement column-level security for external tables</p> </li> <li> <p>[ ] Configure row-level security policies</p> </li> <li> <p>[ ] Limit query concurrency and resource usage</p> </li> <li> <p>[ ] Implement proper database-scoped credentials</p> </li> <li> <p>[ ] Set up secure external data sources</p> </li> <li> <p>[ ] Review and limit data exfiltration risks</p> </li> </ul>"},{"location":"reference/security-checklist/#spark-pool-security","title":"Spark Pool Security","text":"<ul> <li> <p>[ ] Implement proper access control for notebook access</p> </li> <li> <p>[ ] Configure secret scopes for sensitive information</p> </li> <li> <p>[ ] Set up package security for third-party libraries</p> </li> <li> <p>[ ] Isolate development, test, and production environments</p> </li> <li> <p>[ ] Configure proper IAM roles for Spark pools</p> </li> <li> <p>[ ] Implement node initialization scripts with security hardening</p> </li> <li> <p>[ ] Configure proper network isolation for Spark pools</p> </li> <li> <p>[ ] Audit all package installations and dependencies</p> </li> </ul>"},{"location":"reference/security-checklist/#regular-maintenance","title":"Regular Maintenance","text":"<ul> <li> <p>[ ] Schedule regular security reviews</p> </li> <li> <p>[ ] Implement automated vulnerability scanning</p> </li> <li> <p>[ ] Set up regular penetration testing</p> </li> <li> <p>[ ] Schedule key and secret rotation</p> </li> <li> <p>[ ] Perform regular access reviews</p> </li> <li> <p>[ ] Update policies as security requirements change</p> </li> <li> <p>[ ] Conduct regular security training for all users</p> </li> <li> <p>[ ] Test disaster recovery procedures with security focus</p> </li> </ul>"},{"location":"reference/security-checklist/#additional-resources","title":"Additional Resources","text":"<ul> <li> <p>Azure Synapse Analytics Security White Paper</p> </li> <li> <p>Microsoft Security Best Practices</p> </li> <li> <p>Synapse Analytics Security Best Practices</p> </li> </ul>"},{"location":"reference/security/","title":"Azure Synapse Analytics Security Reference","text":"<p>Home &gt; Reference &gt; Security Guide</p>"},{"location":"reference/security/#overview","title":"Overview","text":"<p>This document provides comprehensive security guidance for Azure Synapse Analytics, covering key security aspects across various compute engines and data layers.</p>"},{"location":"reference/security/#network-security","title":"Network Security","text":""},{"location":"reference/security/#network-isolation","title":"Network Isolation","text":"<ul> <li> <p>Use private endpoints to ensure data flows through Azure backbone network</p> </li> <li> <p>Configure managed virtual networks for Synapse workspaces</p> </li> <li> <p>Use IP firewall rules to restrict access</p> </li> <li> <p>Enable service endpoints for added protection</p> </li> </ul>"},{"location":"reference/security/#connectivity","title":"Connectivity","text":"<pre><code>graph TD\n    Client[Client] -- Private Link --&gt; PE[Private Endpoint]\n    PE --&gt; Synapse[Synapse Workspace]\n    Synapse -- Private Link --&gt; Storage[Azure Storage]\n    Synapse -- Private Link --&gt; KeyVault[Azure Key Vault]\n</code></pre>"},{"location":"reference/security/#authentication-and-authorization","title":"Authentication and Authorization","text":""},{"location":"reference/security/#authentication-methods","title":"Authentication Methods","text":"<ul> <li> <p>Microsoft Entra ID (formerly Azure AD) integration</p> </li> <li> <p>Multi-factor authentication</p> </li> <li> <p>Managed identities for Azure resources</p> </li> <li> <p>Service principals with limited scopes</p> </li> </ul>"},{"location":"reference/security/#role-based-access-control-rbac","title":"Role-Based Access Control (RBAC)","text":"<ul> <li> <p>Synapse RBAC roles:</p> </li> <li> <p>Synapse Administrator</p> </li> <li>Synapse Contributor</li> <li>Synapse Compute Operator</li> <li>Synapse Artifact Publisher</li> <li> <p>Synapse Artifact User</p> </li> <li> <p>Azure RBAC roles integration</p> </li> <li> <p>Custom role definitions</p> </li> </ul>"},{"location":"reference/security/#data-protection","title":"Data Protection","text":""},{"location":"reference/security/#encryption","title":"Encryption","text":"<ul> <li> <p>Encryption at rest (storage level)</p> </li> <li> <p>Encryption in transit (TLS 1.2+)</p> </li> <li> <p>Customer-managed keys integration</p> </li> <li> <p>Double encryption support</p> </li> </ul>"},{"location":"reference/security/#data-access-controls","title":"Data Access Controls","text":"<ul> <li> <p>Column-level security</p> </li> <li> <p>Row-level security</p> </li> <li> <p>Dynamic data masking</p> </li> <li> <p>Azure Purview integration for data governance</p> </li> </ul>"},{"location":"reference/security/#monitoring-and-auditing","title":"Monitoring and Auditing","text":""},{"location":"reference/security/#audit-logging","title":"Audit Logging","text":"<ul> <li> <p>Integrate with Azure Monitor</p> </li> <li> <p>Workspace diagnostic logging</p> </li> <li> <p>SQL audit logging</p> </li> <li> <p>Apache Spark application logs</p> </li> </ul>"},{"location":"reference/security/#security-alerts","title":"Security Alerts","text":"<ul> <li> <p>Azure Defender for SQL</p> </li> <li> <p>Microsoft Sentinel integration</p> </li> <li> <p>Anomaly detection</p> </li> <li> <p>Threat protection</p> </li> </ul>"},{"location":"reference/security/#best-practices","title":"Best Practices","text":""},{"location":"reference/security/#serverless-sql-pool-security","title":"Serverless SQL Pool Security","text":"<ul> <li> <p>Implement proper access controls on underlying storage</p> </li> <li> <p>Use managed identities for storage access</p> </li> <li> <p>Apply appropriate RBAC permissions</p> </li> <li> <p>Enable diagnostic logging</p> </li> </ul>"},{"location":"reference/security/#spark-pool-security","title":"Spark Pool Security","text":"<ul> <li> <p>Configure secure access to notebooks</p> </li> <li> <p>Use secret scopes for sensitive information</p> </li> <li> <p>Isolate development, test, and production workspaces</p> </li> <li> <p>Implement proper package management</p> </li> </ul>"},{"location":"reference/security/#shared-metadata-security","title":"Shared Metadata Security","text":"<ul> <li> <p>Control database and table permissions</p> </li> <li> <p>Implement column-level security for sensitive data</p> </li> <li> <p>Use row-level security for multi-tenant scenarios</p> </li> <li> <p>Regularly audit security permissions</p> </li> </ul>"},{"location":"reference/security/#code-examples","title":"Code Examples","text":""},{"location":"reference/security/#configuring-column-level-security","title":"Configuring Column-Level Security","text":"<pre><code>-- Create users\nCREATE USER DataAnalyst WITHOUT LOGIN;\nCREATE USER DataScientist WITHOUT LOGIN;\n\n-- Grant access to the table\nGRANT SELECT ON SalesData TO DataAnalyst, DataScientist;\n\n-- Deny access to sensitive columns for DataAnalyst\nDENY SELECT ON SalesData(CustomerEmail, CreditCardNumber) TO DataAnalyst;\n</code></pre>"},{"location":"reference/security/#implementing-row-level-security","title":"Implementing Row-Level Security","text":"<pre><code>-- Create security predicate function\nCREATE FUNCTION dbo.fn_securitypredicate(@Region AS VARCHAR(100))\nRETURNS TABLE\nWITH SCHEMABINDING\nAS\nRETURN SELECT 1 AS fn_result \n       WHERE @Region = 'North America' \n       OR USER_NAME() = 'dbo'\n       OR USER_NAME() = 'GlobalAnalyst';\n\n-- Create security policy\nCREATE SECURITY POLICY RegionalDataFilter\nADD FILTER PREDICATE dbo.fn_securitypredicate(Region) \nON dbo.SalesData;\n</code></pre>"},{"location":"reference/security/#setting-up-dynamic-data-masking","title":"Setting Up Dynamic Data Masking","text":"<pre><code>-- Apply masking to sensitive columns\nALTER TABLE dbo.Customers\nALTER COLUMN Email ADD MASKED WITH (FUNCTION = 'email()');\n\nALTER TABLE dbo.Customers\nALTER COLUMN PhoneNumber ADD MASKED WITH (FUNCTION = 'partial(0,\"XXX-XXX-\",4)');\n\nALTER TABLE dbo.CreditCards\nALTER COLUMN CardNumber ADD MASKED WITH (FUNCTION = 'partial(0,\"XXXX-XXXX-XXXX-\",4)');\n</code></pre>"},{"location":"reference/security/#next-steps","title":"Next Steps","text":"<ol> <li>Azure Synapse Analytics Best Practices</li> <li>Shared Metadata Security</li> <li>Complete Security Checklist</li> </ol>"},{"location":"serverless-sql/","title":"Serverless SQL Architecture","text":""},{"location":"serverless-sql/#overview","title":"Overview","text":"<p>Serverless SQL is a key component of Azure Synapse Analytics that provides on-demand, scalable SQL query capabilities over data stored in Azure Data Lake Storage. This architecture pattern enables organizations to implement a cost-effective analytics solution without provisioning or managing infrastructure.</p>"},{"location":"serverless-sql/#architecture-components","title":"Architecture Components","text":""},{"location":"serverless-sql/#core-components","title":"Core Components","text":"<ol> <li>Azure Data Lake Storage Gen2</li> <li>Primary storage for data in various formats (Parquet, CSV, JSON)</li> <li>Hierarchical namespace for efficient organization</li> <li> <p>Integration with Azure AD for security</p> </li> <li> <p>Azure Synapse Serverless SQL Pool</p> </li> <li>On-demand query service with pay-per-query billing</li> <li>T-SQL interface for data exploration and analysis</li> <li>No infrastructure to provision or manage</li> <li> <p>Automatic scaling based on query complexity</p> </li> <li> <p>Data Virtualization Layer</p> </li> <li>External tables and views for logical data organization</li> <li>Schema-on-read capabilities</li> <li> <p>Support for various file formats and compression types</p> </li> <li> <p>Integration Components</p> </li> <li>Power BI for reporting and visualization</li> <li>Azure Synapse Pipelines for orchestration</li> <li>Azure Purview for data governance</li> </ol>"},{"location":"serverless-sql/#implementation-patterns","title":"Implementation Patterns","text":""},{"location":"serverless-sql/#data-lake-query-optimization","title":"Data Lake Query Optimization","text":"<p>Serverless SQL pools perform best with optimized data formats and organization:</p>"},{"location":"serverless-sql/#file-format-hierarchy-best-to-worst","title":"File Format Hierarchy (Best to Worst)","text":"<ol> <li>Parquet</li> <li>Columnar format with compression</li> <li>Support for predicate pushdown</li> <li> <p>Partition elimination capabilities</p> </li> <li> <p>ORC</p> </li> <li>Similar benefits to Parquet</li> <li> <p>Good compression ratio</p> </li> <li> <p>CSV/TSV with Header</p> </li> <li>Row-based format</li> <li>Moderate performance</li> <li> <p>Good for small datasets</p> </li> <li> <p>JSON</p> </li> <li>Flexible schema</li> <li>Lower performance</li> <li>Higher compute costs</li> </ol>"},{"location":"serverless-sql/#partitioning-strategies","title":"Partitioning Strategies","text":"<pre><code>-- Example of querying a partitioned dataset efficiently\nSELECT *\nFROM OPENROWSET(\n    BULK 'https://mydatalake.blob.core.windows.net/data/sales/year=2023/month=08/*.parquet',\n    FORMAT = 'PARQUET'\n) AS [sales]\nWHERE [region] = 'West';\n</code></pre>"},{"location":"serverless-sql/#schema-inference-and-management","title":"Schema Inference and Management","text":""},{"location":"serverless-sql/#automatic-schema-inference","title":"Automatic Schema Inference","text":"<pre><code>SELECT TOP 100 *\nFROM OPENROWSET(\n    BULK 'https://mydatalake.blob.core.windows.net/data/products/*.parquet',\n    FORMAT = 'PARQUET'\n) WITH (\n    AUTODETECT = TRUE\n) AS [products];\n</code></pre>"},{"location":"serverless-sql/#explicit-schema-definition","title":"Explicit Schema Definition","text":"<pre><code>CREATE EXTERNAL TABLE [dbo].[Sales] (\n    [OrderId] INT,\n    [CustomerId] INT,\n    [ProductId] INT,\n    [Quantity] INT,\n    [Price] DECIMAL(10,2),\n    [OrderDate] DATETIME2\n)\nWITH (\n    LOCATION = '/sales/',\n    DATA_SOURCE = [MyDataLake],\n    FILE_FORMAT = [ParquetFormat]\n);\n</code></pre>"},{"location":"serverless-sql/#performance-best-practices","title":"Performance Best Practices","text":"<ol> <li>Use Parquet Format</li> <li>Columnar storage for efficient reads</li> <li>Compression to reduce data size</li> <li> <p>Statistics for query optimization</p> </li> <li> <p>Implement Effective Partitioning</p> </li> <li>Partition by frequently filtered columns</li> <li>Balance partition size (100MB-1GB ideal)</li> <li> <p>Avoid over-partitioning</p> </li> <li> <p>Optimize File Sizes</p> </li> <li>Target file sizes between 100MB-1GB</li> <li>Avoid small files (&lt;100MB)</li> <li> <p>Implement file compaction as needed</p> </li> <li> <p>Use Query Optimization Techniques</p> </li> <li>Leverage predicate pushdown</li> <li>Apply column pruning</li> <li>Utilize statistics for better execution plans</li> </ol>"},{"location":"serverless-sql/#cost-management","title":"Cost Management","text":"<p>Serverless SQL pools use a consumption-based pricing model:</p> <ol> <li>Query Costs</li> <li>Billed per TB of data processed</li> <li>No charges for failed queries</li> <li> <p>Metadata operations are free</p> </li> <li> <p>Cost Optimization Strategies</p> </li> <li>Limit data scanned with partitioning</li> <li>Use columnar formats to reduce I/O</li> <li>Apply query filters early</li> <li>Set query result caching where appropriate</li> </ol>"},{"location":"serverless-sql/#security-implementation","title":"Security Implementation","text":"<ol> <li>Authentication</li> <li>Azure Active Directory integration</li> <li> <p>Managed identities for service-to-service authentication</p> </li> <li> <p>Authorization</p> </li> <li>Row-Level Security for data filtering</li> <li>Column-Level Security for sensitive data</li> <li> <p>Dynamic data masking for PII</p> </li> <li> <p>Data Protection</p> </li> <li>In-transit encryption with TLS</li> <li>At-rest encryption with Azure Storage encryption</li> </ol>"},{"location":"serverless-sql/#integration-scenarios","title":"Integration Scenarios","text":""},{"location":"serverless-sql/#business-intelligence-integration","title":"Business Intelligence Integration","text":"<p>Serverless SQL pools integrate seamlessly with Power BI for analytics:</p> <ol> <li>DirectQuery Mode</li> <li>Real-time querying of data lake</li> <li>No need to import data</li> <li> <p>Pushdown query processing</p> </li> <li> <p>Import Mode</p> </li> <li>Scheduled data refresh</li> <li>In-memory analytics</li> <li>Disconnected reporting</li> </ol>"},{"location":"serverless-sql/#data-virtualization","title":"Data Virtualization","text":"<p>Create logical data warehouse views over your data lake:</p> <pre><code>CREATE VIEW [dbo].[CustomerSalesAnalysis] AS\nSELECT \n    c.[CustomerId],\n    c.[CustomerName],\n    c.[Region],\n    s.[OrderId],\n    s.[ProductId],\n    s.[Quantity],\n    s.[Price],\n    s.[OrderDate]\nFROM [dbo].[Customers] c\nJOIN [dbo].[Sales] s ON c.[CustomerId] = s.[CustomerId];\n</code></pre>"},{"location":"serverless-sql/#deployment-and-devops","title":"Deployment and DevOps","text":"<ol> <li>Infrastructure as Code</li> <li>ARM templates for workspace configuration</li> <li> <p>Terraform for resource provisioning</p> </li> <li> <p>CI/CD for Database Objects</p> </li> <li>Source control for SQL scripts</li> <li>Automated testing for views and procedures</li> <li>Deployment pipelines for schema changes</li> </ol>"},{"location":"serverless-sql/#monitoring-and-management","title":"Monitoring and Management","text":"<ol> <li>Query Monitoring</li> <li>Dynamic Management Views (DMVs) for query insights</li> <li>Azure Monitor integration</li> <li> <p>Query Store for performance tracking</p> </li> <li> <p>Resource Governance</p> </li> <li>Query timeout configuration</li> <li>Workload management through classifications</li> <li>Request importance settings</li> </ol>"},{"location":"serverless-sql/#common-use-cases","title":"Common Use Cases","text":"<ol> <li>Data Lake Exploration</li> <li>Ad-hoc querying of raw and refined data</li> <li> <p>Schema discovery and profiling</p> </li> <li> <p>Self-Service Analytics</p> </li> <li>Business analyst access to data lake</li> <li> <p>SQL-based data exploration</p> </li> <li> <p>Data Science Support</p> </li> <li>Feature engineering with SQL</li> <li>Training data preparation</li> <li> <p>Model inference data processing</p> </li> <li> <p>Log Analytics</p> </li> <li>Query across application logs</li> <li>Security and compliance monitoring</li> <li>Operational analytics</li> </ol>"},{"location":"shared-metadata/","title":"Shared Metadata Architecture","text":""},{"location":"shared-metadata/#overview","title":"Overview","text":"<p>The Shared Metadata Architecture in Azure Synapse Analytics enables a unified semantic layer across different compute engines, allowing consistent data access, governance, and business logic implementation regardless of the query engine used. This approach reduces redundancy, improves maintainability, and provides a consistent view of enterprise data.</p>"},{"location":"shared-metadata/#architecture-components","title":"Architecture Components","text":""},{"location":"shared-metadata/#core-components","title":"Core Components","text":"<ol> <li>Azure Synapse Analytics Workspace</li> <li>Central hub for all analytics activities</li> <li>Integration point for different compute engines</li> <li> <p>Management of shared metadata artifacts</p> </li> <li> <p>Synapse SQL Pools (Dedicated and Serverless)</p> </li> <li>T-SQL interface for data access</li> <li>Support for external tables over data lake</li> <li> <p>View definitions for logical data modeling</p> </li> <li> <p>Synapse Spark Pools</p> </li> <li>Apache Spark processing engine</li> <li>Support for Delta, Parquet, and other formats</li> <li> <p>Integration with SQL through SparkSQL</p> </li> <li> <p>Azure Data Lake Storage Gen2</p> </li> <li>Common storage layer for all data</li> <li>Support for POSIX-compliant ACLs</li> <li> <p>Hierarchical namespace for organization</p> </li> <li> <p>Metadata Services</p> </li> <li>Synapse Workspace Metadata</li> <li>Azure Purview for cataloging and lineage</li> <li>Git integration for metadata version control</li> </ol>"},{"location":"shared-metadata/#implementation-patterns","title":"Implementation Patterns","text":""},{"location":"shared-metadata/#cross-engine-table-definitions","title":"Cross-Engine Table Definitions","text":""},{"location":"shared-metadata/#sql-external-tables","title":"SQL External Tables","text":"<pre><code>-- Create a database scoped credential for accessing ADLS\nCREATE DATABASE SCOPED CREDENTIAL [ADLSCredential]\nWITH\n    IDENTITY = 'Managed Service Identity';\n\n-- Create an external data source\nCREATE EXTERNAL DATA SOURCE [DataLake]\nWITH (\n    LOCATION = 'abfss://data@youraccount.dfs.core.windows.net',\n    CREDENTIAL = [ADLSCredential]\n);\n\n-- Create an external file format\nCREATE EXTERNAL FILE FORMAT [ParquetFormat]\nWITH (\n    FORMAT_TYPE = PARQUET,\n    DATA_COMPRESSION = 'org.apache.hadoop.io.compress.SnappyCodec'\n);\n\n-- Create an external table\nCREATE EXTERNAL TABLE [dbo].[Customer] (\n    [CustomerId] INT,\n    [Name] NVARCHAR(100),\n    [Email] NVARCHAR(100),\n    [RegistrationDate] DATETIME2\n)\nWITH (\n    LOCATION = '/curated/customers/',\n    DATA_SOURCE = [DataLake],\n    FILE_FORMAT = [ParquetFormat]\n);\n</code></pre>"},{"location":"shared-metadata/#spark-dataframe-access","title":"Spark DataFrame Access","text":"<pre><code># Access the same table from Spark\ndf = spark.read.format(\"delta\").load(\"abfss://data@youraccount.dfs.core.windows.net/curated/customers/\")\n\n# Register as a temp view for SparkSQL access\ndf.createOrReplaceTempView(\"Customer\")\n\n# Query using SparkSQL\nsparkDF = spark.sql(\"SELECT CustomerId, Name, Email FROM Customer WHERE RegistrationDate &gt; '2023-01-01'\")\n</code></pre>"},{"location":"shared-metadata/#unified-semantic-layer","title":"Unified Semantic Layer","text":""},{"location":"shared-metadata/#sql-views-for-business-logic","title":"SQL Views for Business Logic","text":"<pre><code>-- Create a business view that can be accessed from multiple engines\nCREATE VIEW [dbo].[CustomerSummary] AS\nSELECT\n    c.[CustomerId],\n    c.[Name],\n    c.[Email],\n    c.[RegistrationDate],\n    COUNT(o.[OrderId]) AS [TotalOrders],\n    SUM(o.[OrderAmount]) AS [TotalSpend],\n    DATEDIFF(day, c.[RegistrationDate], GETDATE()) AS [CustomerAgeInDays]\nFROM [dbo].[Customer] c\nLEFT JOIN [dbo].[Order] o ON c.[CustomerId] = o.[CustomerId]\nGROUP BY c.[CustomerId], c.[Name], c.[Email], c.[RegistrationDate];\n</code></pre>"},{"location":"shared-metadata/#spark-to-sql-view-access","title":"Spark to SQL View Access","text":"<pre><code># Access SQL views from Spark using JDBC connector\nserver_name = \"mysynapseworkspace-ondemand.sql.azuresynapse.net\"\ndatabase_name = \"MetadataDB\"\n\ncustomer_summary = spark.read \\\n    .format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n    .option(\"url\", f\"jdbc:sqlserver://{server_name}:1433;database={database_name}\") \\\n    .option(\"query\", \"SELECT * FROM [dbo].[CustomerSummary]\") \\\n    .option(\"authentication\", \"ActiveDirectoryMSI\") \\\n    .option(\"encrypt\", \"true\") \\\n    .option(\"trustServerCertificate\", \"false\") \\\n    .load()\n</code></pre>"},{"location":"shared-metadata/#metadata-synchronization-patterns","title":"Metadata Synchronization Patterns","text":""},{"location":"shared-metadata/#schema-propagation","title":"Schema Propagation","text":"<ol> <li>Source of Truth Approach</li> <li>Designate one system (typically SQL) as the schema authority</li> <li>Automate schema propagation to other engines</li> <li> <p>Use tools like Azure Data Factory or Synapse Pipelines for orchestration</p> </li> <li> <p>Schema Evolution Handling</p> </li> <li>Implement version control for schema changes</li> <li>Use schema compatibility modes in Delta Lake</li> <li>Automate testing of schema compatibility</li> </ol>"},{"location":"shared-metadata/#metadata-management","title":"Metadata Management","text":"<ol> <li>Azure Purview Integration</li> <li>Central catalog for data assets</li> <li>Automated scanning and classification</li> <li>Lineage tracking across engines</li> <li> <p>Business glossary integration</p> </li> <li> <p>Custom Metadata Registry</p> </li> <li>Create a metadata registry database</li> <li>Track schema versions and changes</li> <li>Store engine-specific optimizations</li> </ol>"},{"location":"shared-metadata/#security-implementation","title":"Security Implementation","text":""},{"location":"shared-metadata/#unified-security-model","title":"Unified Security Model","text":"<pre><code>-- Implement Row-Level Security\nCREATE SECURITY POLICY [CustomerPolicy]\nADD FILTER PREDICATE [dbo].[fn_securitypredicate]([TenantId])\nON [dbo].[Customer];\n\n-- Column-Level Security\nGRANT SELECT ON [dbo].[Customer]([CustomerId], [Name]) TO [Analysts];\nGRANT SELECT ON [dbo].[Customer]([Email]) TO [MarketingTeam];\n</code></pre>"},{"location":"shared-metadata/#synapse-workspace-permissions","title":"Synapse Workspace Permissions","text":"<ul> <li>Workspace-level roles (Admin, Contributor, User)</li> <li>SQL permissions for database objects</li> <li>Spark pool permissions for notebooks and jobs</li> <li>Integration runtime permissions for pipelines</li> </ul>"},{"location":"shared-metadata/#performance-optimization","title":"Performance Optimization","text":""},{"location":"shared-metadata/#cross-engine-query-optimization","title":"Cross-Engine Query Optimization","text":"<ol> <li>Dedicated SQL Pool Optimizations</li> <li>Distribution keys aligned with join columns</li> <li>Partition aligned with filtering patterns</li> <li> <p>Statistics maintenance</p> </li> <li> <p>Serverless SQL Optimizations</p> </li> <li>Optimal file formats (Parquet/Delta)</li> <li>Partition elimination strategies</li> <li> <p>File size optimization</p> </li> <li> <p>Spark Optimizations</p> </li> <li>Spark configuration tuning</li> <li>Broadcast joins for dimension tables</li> <li>Partition pruning through predicate pushdown</li> </ol>"},{"location":"shared-metadata/#common-use-cases","title":"Common Use Cases","text":""},{"location":"shared-metadata/#enterprise-data-warehouse-modernization","title":"Enterprise Data Warehouse Modernization","text":"<ol> <li>Hybrid Approach</li> <li>Keep core EDW workloads in Dedicated SQL Pool</li> <li>Use Spark for data preparation and ML</li> <li>Use Serverless SQL for ad-hoc exploration</li> <li> <p>Maintain consistent business definitions across all engines</p> </li> <li> <p>Phased Migration</p> </li> <li>Start with shared metadata layer</li> <li>Gradually migrate workloads to appropriate engines</li> <li>Maintain backward compatibility</li> </ol>"},{"location":"shared-metadata/#advanced-analytics-integration","title":"Advanced Analytics Integration","text":"<ol> <li>Machine Learning Pipeline</li> <li>Feature engineering in SQL or Spark</li> <li>Model training in Spark</li> <li>Model scoring in SQL or Spark</li> <li> <p>Consistent data access across pipeline stages</p> </li> <li> <p>Real-time Analytics</p> </li> <li>Stream processing in Spark</li> <li>Serving layer in SQL</li> <li>Shared schema definitions</li> </ol>"},{"location":"shared-metadata/#devops-and-governance","title":"DevOps and Governance","text":"<ol> <li>CI/CD for Metadata</li> <li>Source control for all metadata definitions</li> <li>Automated testing for cross-engine compatibility</li> <li> <p>Deployment pipelines for metadata changes</p> </li> <li> <p>Monitoring and Observability</p> </li> <li>Track query performance across engines</li> <li>Monitor metadata usage patterns</li> <li>Audit access to sensitive data</li> </ol>"},{"location":"shared-metadata/#best-practices","title":"Best Practices","text":"<ol> <li>Design for Compatibility</li> <li>Use data types supported across engines</li> <li>Avoid engine-specific SQL extensions where possible</li> <li> <p>Document engine-specific behaviors</p> </li> <li> <p>Implement Data Governance Early</p> </li> <li>Define data ownership and stewardship</li> <li>Establish metadata management practices</li> <li> <p>Automate compliance and quality checks</p> </li> <li> <p>Balance Flexibility and Control</p> </li> <li>Allow specialized optimizations per engine</li> <li>Maintain core business logic consistency</li> <li> <p>Enable self-service while ensuring governance</p> </li> <li> <p>Optimize for Performance</p> </li> <li>Profile workloads across engines</li> <li>Apply engine-specific optimizations</li> <li>Use appropriate compute for each workload type</li> </ol>"}]}