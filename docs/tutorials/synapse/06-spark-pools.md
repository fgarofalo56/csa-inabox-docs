# ðŸ”¥ Tutorial 6: Spark Pool Configuration

> __ðŸ  [Home](../../../README.md)__ | __ðŸ“– [Documentation](../../README.md)__ | __ðŸŽ“ Tutorials__ | __ðŸ—ï¸ [Synapse Series](README.md)__ | __ðŸ”¥ Spark Pools__

![Tutorial](https://img.shields.io/badge/Tutorial-06_Spark_Pools-blue)
![Duration](https://img.shields.io/badge/Duration-30_minutes-green)
![Level](https://img.shields.io/badge/Level-Intermediate-yellow)

__Configure and optimize Apache Spark pools for performance and cost-efficiency. Learn pool sizing, auto-scaling, package management, and performance tuning.__

## ðŸŽ¯ Learning Objectives

- âœ… __Create Spark pools__ with optimal configurations
- âœ… __Implement auto-scaling__ and auto-pause policies
- âœ… __Manage Python/Scala packages__ and library dependencies
- âœ… __Tune Spark performance__ parameters
- âœ… __Monitor resource utilization__ and optimize costs

## â±ï¸ Time Estimate: 30 minutes

## ðŸ“‹ Prerequisites

- [x] [Tutorial 1: Environment Setup](01-environment-setup.md)
- [x] [Tutorial 2: Workspace Basics](02-workspace-basics.md)

## ðŸ—ï¸ Step 1: Create Production Spark Pool

```powershell
$config = Get-Content "workspace-config.json" | ConvertFrom-Json

# Create medium-sized Spark pool for production workloads
az synapse spark pool create `
  --name "sparkmedium" `
  --workspace-name $config.WorkspaceName `
  --resource-group $config.ResourceGroup `
  --spark-version "3.4" `
  --node-count 5 `
  --node-size Medium `
  --node-size-family MemoryOptimized `
  --enable-auto-scale true `
  --min-node-count 3 `
  --max-node-count 10 `
  --enable-auto-pause true `
  --delay 15 `
  --enable-dynamic-executor-allocation true `
  --tags Environment=Production Workload=Analytics

Write-Host "âœ… Production Spark pool created" -ForegroundColor Green
```

## âš™ï¸ Step 2: Configure Spark Settings

### __2.1 Spark Configuration File__

```python
# spark-defaults.conf
spark.dynamicAllocation.enabled true
spark.dynamicAllocation.minExecutors 2
spark.dynamicAllocation.maxExecutors 10
spark.sql.shuffle.partitions 200
spark.sql.adaptive.enabled true
spark.sql.adaptive.coalescePartitions.enabled true
spark.sql.files.maxPartitionBytes 134217728
spark.speculation true
spark.sql.parquet.compression.codec snappy
spark.eventLog.enabled true
```

### __2.2 Apply Configuration__

```powershell
# Upload Spark configuration
az storage blob upload `
  --account-name $config.StorageAccount `
  --container-name "synapsefs" `
  --name "spark-config/spark-defaults.conf" `
  --file "spark-defaults.conf" `
  --auth-mode login

Write-Host "âœ… Spark configuration uploaded" -ForegroundColor Green
```

## ðŸ“¦ Step 3: Package Management

### __3.1 Create Requirements File__

```text
# requirements.txt
pandas==2.1.0
numpy==1.25.0
delta-spark==3.0.0
azure-storage-blob==12.19.0
pyarrow==13.0.0
matplotlib==3.8.0
scikit-learn==1.3.0
```

### __3.2 Install Packages__

```powershell
# Upload requirements.txt
az storage blob upload `
  --account-name $config.StorageAccount `
  --container-name "synapsefs" `
  --name "spark-config/requirements.txt" `
  --file "requirements.txt" `
  --auth-mode login

# Update Spark pool to use requirements
az synapse spark pool update `
  --name "sparkmedium" `
  --workspace-name $config.WorkspaceName `
  --resource-group $config.ResourceGroup `
  --library-requirements "requirements.txt"

Write-Host "âœ… Package requirements configured" -ForegroundColor Green
```

## ðŸŽ¯ Step 4: Performance Tuning

### __4.1 Optimize for Different Workloads__

**ETL Workloads**:

```python
spark.conf.set("spark.sql.shuffle.partitions", "200")
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")
```

**Machine Learning**:

```python
spark.conf.set("spark.executor.memory", "8g")
spark.conf.set("spark.executor.cores", "4")
spark.conf.set("spark.python.worker.memory", "2g")
```

**Streaming**:

```python
spark.conf.set("spark.streaming.backpressure.enabled", "true")
spark.conf.set("spark.streaming.receiver.maxRate", "10000")
```

### __4.2 Memory Management__

```python
# Optimal memory configuration
spark.conf.set("spark.executor.memoryOverhead", "2g")
spark.conf.set("spark.memory.fraction", "0.8")
spark.conf.set("spark.memory.storageFraction", "0.3")
```

## ðŸ“Š Step 5: Monitor Performance

```sql
-- Query Spark application metrics
SELECT
    application_id,
    application_name,
    start_time,
    end_time,
    DATEDIFF(second, start_time, end_time) as duration_seconds,
    executor_count,
    executor_cores_total,
    executor_memory_total_gb
FROM monitoring.spark_applications
WHERE start_time >= DATEADD(day, -7, GETDATE())
ORDER BY start_time DESC;
```

## âœ… Validation

```powershell
# Verify Spark pool configuration
az synapse spark pool show `
  --name "sparkmedium" `
  --workspace-name $config.WorkspaceName `
  --resource-group $config.ResourceGroup `
  --query "{Name:name, NodeSize:nodeSize, MinNodes:autoScale.minNodeCount, MaxNodes:autoScale.maxNodeCount, AutoPause:autoPause.enabled}" `
  --output table

Write-Host "âœ… Spark pool validated" -ForegroundColor Green
```

## ðŸ’¡ Best Practices

- âœ… Enable auto-pause with 15-minute delay
- âœ… Use auto-scaling for variable workloads
- âœ… Pin library versions in requirements.txt
- âœ… Enable adaptive query execution
- âœ… Monitor and right-size node count/size

## ðŸš€ What's Next?

**Continue to Tutorial 7**: [PySpark Data Processing](07-pyspark-processing.md)

---

__Tutorial Progress__: 6 of 14 completed
__Next__: [07. PySpark Processing â†’](07-pyspark-processing.md)
