name: Documentation Tests

on:
  push:
    branches: [ master, main, develop ]
  pull_request:
    branches: [ master, main ]
  schedule:
    # Run tests weekly on Sundays at 6 AM UTC
    - cron: '0 6 * * 0'

env:
  PYTHON_VERSION: '3.11'
  NODE_VERSION: '18'

jobs:
  test-documentation:
    name: Test Documentation Quality
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    strategy:
      fail-fast: false
      matrix:
        test-type: [unit, integration]
        
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
      
      - name: Cache Python dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-
      
      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            libmagic1 \
            libffi-dev \
            libssl-dev
      
      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip setuptools wheel
          pip install -r requirements.txt
          pip install -r requirements-test.txt
      
      - name: Install Node.js dependencies for markdownlint
        run: |
          npm install -g markdownlint-cli
      
      - name: Verify installation
        run: |
          python --version
          pip --version
          markdownlint --version
          mkdocs --version
      
      - name: Run unit tests
        if: matrix.test-type == 'unit'
        run: |
          pytest tests/unit/ \
            --cov=src/csa_docs_tools \
            --cov-branch \
            --cov-report=xml:coverage-unit.xml \
            --cov-report=term-missing \
            --junitxml=junit-unit.xml \
            -v
      
      - name: Run integration tests
        if: matrix.test-type == 'integration'
        run: |
          pytest tests/integration/ \
            --cov=src/csa_docs_tools \
            --cov-branch \
            --cov-report=xml:coverage-integration.xml \
            --cov-report=term-missing \
            --junitxml=junit-integration.xml \
            -v \
            -m "not network"
      
      - name: Upload coverage to Codecov
        if: always()
        uses: codecov/codecov-action@v3
        with:
          file: coverage-${{ matrix.test-type }}.xml
          flags: ${{ matrix.test-type }}
          name: codecov-${{ matrix.test-type }}
          fail_ci_if_error: false
      
      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: test-results-${{ matrix.test-type }}
          path: |
            junit-${{ matrix.test-type }}.xml
            htmlcov/
            coverage-${{ matrix.test-type }}.xml
  
  lint-and-format:
    name: Lint and Format Check
    runs-on: ubuntu-latest
    timeout-minutes: 15
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install ruff mypy black isort
          pip install -r requirements.txt
          pip install -r requirements-test.txt
      
      - name: Run ruff linting
        run: |
          ruff check src/ tests/
      
      - name: Run ruff formatting check
        run: |
          ruff format --check src/ tests/
      
      - name: Run mypy type checking
        run: |
          mypy src/csa_docs_tools/ --ignore-missing-imports --no-strict-optional
      
      - name: Check import sorting
        run: |
          isort --check-only --diff src/ tests/

  test-documentation-build:
    name: Test MkDocs Build
    runs-on: ubuntu-latest
    timeout-minutes: 20
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install MkDocs dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Test MkDocs configuration
        run: |
          mkdocs build --strict --verbose
      
      - name: Upload built site
        if: success()
        uses: actions/upload-artifact@v3
        with:
          name: mkdocs-site
          path: site/
          retention-days: 7

  test-link-validation:
    name: Link Validation
    runs-on: ubuntu-latest
    timeout-minutes: 25
    if: github.event_name != 'schedule'  # Skip on scheduled runs to avoid rate limiting
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -r requirements-test.txt
      
      - name: Run internal link validation
        run: |
          python -c "
          import asyncio
          from src.csa_docs_tools import LinkValidator
          from pathlib import Path
          
          async def validate():
              async with LinkValidator(Path('.')) as validator:
                  results = await validator.validate_all_links(check_external=False)
                  report = validator.generate_report(results)
                  print(f'Internal links: {report[\"valid_links\"]}/{report[\"total_links\"]} valid')
                  if report['broken_links'] > 0:
                      print('Broken internal links found!')
                      for broken in report['broken_link_details'][:5]:
                          print(f'  - {broken[\"url\"]} in {broken[\"file\"]}')
                      exit(1)
          
          asyncio.run(validate())
          "
      
      - name: Run external link validation (sample)
        if: github.event_name == 'schedule'
        continue-on-error: true  # External links may be temporarily unavailable
        run: |
          python -c "
          import asyncio
          from src.csa_docs_tools import LinkValidator
          from pathlib import Path
          
          async def validate():
              async with LinkValidator(Path('.')) as validator:
                  results = await validator.validate_all_links(check_external=True)
                  report = validator.generate_report(results)
                  print(f'All links: {report[\"valid_links\"]}/{report[\"total_links\"]} valid')
                  print(f'Success rate: {report[\"success_rate\"]:.1f}%')
          
          asyncio.run(validate())
          "

  markdown-quality:
    name: Markdown Quality Check
    runs-on: ubuntu-latest
    timeout-minutes: 15
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -r requirements-test.txt
          npm install -g markdownlint-cli
      
      - name: Run markdownlint
        run: |
          markdownlint docs/**/*.md --config .markdownlint.json
      
      - name: Run Python markdown quality checks
        run: |
          python -c "
          from src.csa_docs_tools import MarkdownQualityChecker
          from pathlib import Path
          
          checker = MarkdownQualityChecker(Path('.'))
          results = checker.check_all_files()
          report = checker.generate_quality_report(results)
          
          print(f'Markdown Quality Report:')
          print(f'  Files processed: {report[\"total_files\"]}')
          print(f'  Total issues: {report[\"total_issues\"]}')
          print(f'  Quality score: {report[\"quality_score\"]:.1f}/100')
          
          if report['quality_score'] < 70:
              print('Quality score too low!')
              exit(1)
          "

  image-validation:
    name: Image Validation
    runs-on: ubuntu-latest
    timeout-minutes: 15
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -r requirements-test.txt
      
      - name: Run image validation
        run: |
          python -c "
          from src.csa_docs_tools import ImageReferenceValidator
          from pathlib import Path
          
          validator = ImageReferenceValidator(Path('.'))
          results = validator.validate_all_images()
          report = validator.generate_image_report(results)
          
          print(f'Image Validation Report:')
          print(f'  Total images: {report[\"total_images\"]}')
          print(f'  Image issues: {report[\"total_image_issues\"]}')
          print(f'  Unused images: {report[\"unused_images\"]}')
          print(f'  Health score: {report[\"image_health_score\"]:.1f}/100')
          
          # Report unused images but don't fail
          if report['unused_images'] > 0:
              print(f'Found {report[\"unused_images\"]} unused images')
          "

  security-scan:
    name: Security Scan
    runs-on: ubuntu-latest
    timeout-minutes: 10
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install safety bandit
          pip install -r requirements.txt
          pip install -r requirements-test.txt
      
      - name: Run safety check
        run: |
          safety check --json || true
      
      - name: Run bandit security linter
        run: |
          bandit -r src/ -f json || true

  generate-report:
    name: Generate Test Report
    runs-on: ubuntu-latest
    needs: [test-documentation, test-documentation-build, markdown-quality, image-validation]
    if: always()
    timeout-minutes: 10
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Download all artifacts
        uses: actions/download-artifact@v3
      
      - name: Generate combined report
        run: |
          echo "# Documentation Test Report" > test-report.md
          echo "" >> test-report.md
          echo "Generated on: $(date)" >> test-report.md
          echo "Commit: ${{ github.sha }}" >> test-report.md
          echo "" >> test-report.md
          
          echo "## Job Status" >> test-report.md
          echo "- Unit Tests: ${{ needs.test-documentation.result }}" >> test-report.md
          echo "- MkDocs Build: ${{ needs.test-documentation-build.result }}" >> test-report.md
          echo "- Markdown Quality: ${{ needs.markdown-quality.result }}" >> test-report.md
          echo "- Image Validation: ${{ needs.image-validation.result }}" >> test-report.md
          echo "" >> test-report.md
          
          if [ -f "test-results-unit/junit-unit.xml" ]; then
              echo "## Unit Test Results Available" >> test-report.md
          fi
          
          if [ -f "test-results-integration/junit-integration.xml" ]; then
              echo "## Integration Test Results Available" >> test-report.md
          fi
      
      - name: Upload combined report
        uses: actions/upload-artifact@v3
        with:
          name: documentation-test-report
          path: test-report.md
          retention-days: 30

  notify-on-failure:
    name: Notify on Failure
    runs-on: ubuntu-latest
    needs: [test-documentation, test-documentation-build, markdown-quality, image-validation]
    if: failure() && github.event_name == 'push'
    timeout-minutes: 5
    
    steps:
      - name: Notify failure
        run: |
          echo "Documentation tests failed on push to ${{ github.ref }}"
          echo "Please check the workflow logs and fix any issues."
          # Add notification logic here (Slack, email, etc.) if needed